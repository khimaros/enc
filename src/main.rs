// automatically generated by https://github.com/khimaros/enc
// invocation: ./enc-release src/enc.en -o src/main.rs --context-files ./.enc.env.example:./Cargo.toml

use anyhow::{anyhow, Context, Result};
use clap::Parser;
use lazy_static::lazy_static;
use regex::Regex;
use serde::{Deserialize, Serialize};
use serde_json::{json, Value};
use std::collections::{BTreeMap, HashMap};
use std::fs::{self, File};
use std::io::{BufRead, BufReader, Write};
use std::path::Path;
use std::process::Command;
use std::time::{Duration, Instant};

// enc uses a layered configuration. settings are sourced with the following precedence
#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    // input file is the first positional argument
    #[arg(required_unless_present = "show_config")]
    input_file: Option<String>,

    // output file is specified via the -o or --output flag
    #[arg(short, long, required_unless_present = "show_config")]
    output: Option<String>,

    #[arg(long)]
    show_config: bool,

    #[arg(long)]
    provider: Option<String>,
    #[arg(long)]
    model: Option<String>,
    #[arg(long)]
    max_tokens: Option<u64>,
    #[arg(long)]
    thinking_budget: Option<u64>,
    #[arg(long)]
    seed: Option<i64>,
    #[arg(long)]
    hacking_conventions: Option<String>,
    #[arg(long)]
    timeout: Option<u64>,
    #[arg(long)]
    context_files: Option<String>,
    #[arg(long)]
    openai_api_base: Option<String>,
    #[arg(long)]
    logs_path: Option<String>,
    #[arg(long)]
    resources_path: Option<String>,
    #[arg(long)]
    test_command: Option<String>,
    #[arg(long)]
    test_iterations: Option<u32>,

    // api keys usually via env, but support flags
    #[arg(long)]
    gemini_api_key: Option<String>,
    #[arg(long)]
    anthropic_api_key: Option<String>,
    #[arg(long)]
    openai_api_key: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Config {
    provider: String,
    model: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    thinking_budget: Option<u64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    seed: Option<i64>,
    hacking_conventions: String,
    timeout: u64,
    context_files: String,
    openai_api_base: String,
    logs_path: String,
    resources_path: String,
    test_command: String,
    test_iterations: u32,
    // api keys stored separately or redacted in serialize
    #[serde(skip)]
    gemini_api_key: String,
    #[serde(skip)]
    anthropic_api_key: String,
    #[serde(skip)]
    openai_api_key: String,
}

// default models
const DEFAULT_GOOGLE_MODEL: &str = "gemini-2.5-pro";
const DEFAULT_ANTHROPIC_MODEL: &str = "claude-sonnet-4-20250514";
const DEFAULT_OPENAI_MODEL: &str = "gpt-5-2025-08-07";

lazy_static! {
    static ref RE_ENV_VAR: Regex = Regex::new(r"^\s*([A-Z_]+)=(.*)$").unwrap();
}

// the "consolidated configuration" is the FINAL merged result
fn load_config(args: &Args) -> Result<Config> {
    // 5. hardcoded fallbacks
    let mut c = Config {
        provider: "google".to_string(),
        model: "".to_string(), // determine based on provider later if empty
        max_tokens: None,
        thinking_budget: None,
        seed: None,
        hacking_conventions: "./HACKING.md".to_string(),
        timeout: 1800,
        context_files: "".to_string(),
        openai_api_base: "".to_string(),
        logs_path: "./log/".to_string(),
        resources_path: "./res/:${XDG_DATA_HOME}/enc/res/".to_string(),
        test_command: "".to_string(),
        test_iterations: 3,
        gemini_api_key: "".to_string(),
        anthropic_api_key: "".to_string(),
        openai_api_key: "".to_string(),
    };

    let mut env_vars = HashMap::new();

    // 4. home config: ~/.enc.env
    if let Some(home) = dirs::home_dir() {
        load_env_file(&home.join(".enc.env"), &mut env_vars);
    }

    // 3. working dir config: ./.enc.env
    load_env_file(Path::new("./.enc.env"), &mut env_vars);

    // 2. environment variables (actual env vars override .env files)
    for (k, v) in std::env::vars() {
        env_vars.insert(k, v);
    }

    // apply env vars to config
    apply_env_to_config(&mut c, &env_vars);

    // 1. command line flags (override everything)
    if let Some(v) = &args.provider { c.provider = v.clone(); }
    if let Some(v) = &args.model { c.model = v.clone(); }
    if args.max_tokens.is_some() { c.max_tokens = args.max_tokens; }
    if args.thinking_budget.is_some() { c.thinking_budget = args.thinking_budget; }
    if args.seed.is_some() { c.seed = args.seed; }
    if let Some(v) = &args.hacking_conventions { c.hacking_conventions = v.clone(); }
    if let Some(v) = args.timeout { c.timeout = v; }
    if let Some(v) = &args.context_files { c.context_files = v.clone(); }
    if let Some(v) = &args.openai_api_base { c.openai_api_base = v.clone(); }
    if let Some(v) = &args.logs_path { c.logs_path = v.clone(); }
    if let Some(v) = &args.resources_path { c.resources_path = v.clone(); }
    if let Some(v) = &args.test_command { c.test_command = v.clone(); }
    if let Some(v) = args.test_iterations { c.test_iterations = v; }
    if let Some(v) = &args.gemini_api_key { c.gemini_api_key = v.clone(); }
    if let Some(v) = &args.anthropic_api_key { c.anthropic_api_key = v.clone(); }
    if let Some(v) = &args.openai_api_key { c.openai_api_key = v.clone(); }

    // defaults for model if not set
    if c.model.is_empty() {
        c.model = match c.provider.as_str() {
            "google" => DEFAULT_GOOGLE_MODEL,
            "anthropic" => DEFAULT_ANTHROPIC_MODEL,
            "openai" => DEFAULT_OPENAI_MODEL,
            _ => DEFAULT_GOOGLE_MODEL,
        }.to_string();
    }

    Ok(c)
}

// .env files are read manually to extract config values WITHOUT modifying the process environment
fn load_env_file(path: &Path, vars: &mut HashMap<String, String>) {
    if let Ok(file) = File::open(path) {
        for line in BufReader::new(file).lines().flatten() {
            let line = line.trim();
            if line.starts_with('#') || line.is_empty() { continue; }
            if let Some(caps) = RE_ENV_VAR.captures(line) {
                let key = caps.get(1).unwrap().as_str().to_string();
                let val = caps.get(2).unwrap().as_str().trim_matches('"').to_string();
                vars.insert(key, val);
            }
        }
    }
}

fn apply_env_to_config(c: &mut Config, vars: &HashMap<String, String>) {
    if let Some(v) = vars.get("PROVIDER") { c.provider = v.clone(); }
    if let Some(v) = vars.get("MODEL") { c.model = v.clone(); }
    if let Some(v) = vars.get("MAX_TOKENS") { if !v.is_empty() { c.max_tokens = v.parse().ok(); } }
    if let Some(v) = vars.get("THINKING_BUDGET") { if !v.is_empty() { c.thinking_budget = v.parse().ok(); } }
    if let Some(v) = vars.get("SEED") { if !v.is_empty() { c.seed = v.parse().ok(); } }
    if let Some(v) = vars.get("HACKING_CONVENTIONS") { c.hacking_conventions = v.clone(); }
    if let Some(v) = vars.get("TIMEOUT") { if !v.is_empty() { c.timeout = v.parse().unwrap_or(1800); } }
    if let Some(v) = vars.get("CONTEXT_FILES") { c.context_files = v.clone(); }
    if let Some(v) = vars.get("OPENAI_API_BASE") { c.openai_api_base = v.clone(); }
    if let Some(v) = vars.get("LOGS_PATH") { c.logs_path = v.clone(); }
    if let Some(v) = vars.get("RESOURCES_PATH") { c.resources_path = v.clone(); }
    if let Some(v) = vars.get("TEST_COMMAND") { c.test_command = v.clone(); }
    if let Some(v) = vars.get("TEST_ITERATIONS") { if !v.is_empty() { c.test_iterations = v.parse().unwrap_or(3); } }
    if let Some(v) = vars.get("GEMINI_API_KEY") { c.gemini_api_key = v.clone(); }
    if let Some(v) = vars.get("ANTHROPIC_API_KEY") { c.anthropic_api_key = v.clone(); }
    if let Some(v) = vars.get("OPENAI_API_KEY") { c.openai_api_key = v.clone(); }
}

// resources workarounds
fn get_resource(name: &str, config: &Config) -> Result<String> {
    let xdg_data = std::env::var("XDG_DATA_HOME").unwrap_or_else(|_| {
        let home = std::env::var("HOME").unwrap_or_else(|_| ".".to_string());
        format!("{}/.local/share", home)
    });
    
    let paths = config.resources_path.replace("${XDG_DATA_HOME}", &xdg_data);
    for path in paths.split(':') {
        let p = Path::new(path).join(name);
        if p.exists() {
            return fs::read_to_string(p).context("failed to read resource");
        }
    }

    // compile-time macros fallback
    match name {
        "languages.json" => Ok(include_str!("../res/languages.json").to_string()),
        "pricing.json" => Ok(include_str!("../res/pricing.json").to_string()),
        "prompt.tmpl" => Ok(include_str!("../res/prompt.tmpl").to_string()),
        _ => Err(anyhow!("resource not found: {}", name)),
    }
}

// show-config logic
fn print_config(c: &Config) {
    let mut map = serde_json::to_value(c).unwrap();
    let obj = map.as_object_mut().unwrap();

    // Redact API keys manually (even though they are skip_serializing in struct, we must include them redacted)
    obj.insert("gemini_api_key".to_string(), json!("[REDACTED]"));
    obj.insert("anthropic_api_key".to_string(), json!("[REDACTED]"));
    obj.insert("openai_api_key".to_string(), json!("[REDACTED]"));

    // Filter empty strings. Numeric Options are already filtered by skip_serializing_if
    let keys: Vec<String> = obj.keys().cloned().collect();
    for k in keys {
        if let Some(s) = obj.get(&k).and_then(|v| v.as_str()) {
            if s.is_empty() && !k.ends_with("_api_key") {
                obj.remove(&k);
            }
        }
    }

    // Lexical sort happens automatically in serde_json's default formatter for maps usually, 
    // but let's ensure it by collecting to BTreeMap
    let sorted: BTreeMap<String, Value> = serde_json::from_value(Value::Object(obj.clone())).unwrap();
    println!("{}", serde_json::to_string_pretty(&sorted).unwrap());
}

#[derive(Debug, Deserialize)]
struct Pricing {
    #[serde(default)]
    input_cost_per_token: f64,
    #[serde(default)]
    output_cost_per_token: f64,
    #[serde(default)]
    #[allow(dead_code)]
    max_tokens: u64,
}

#[derive(Default)]
struct ApiStats {
    input_tokens: u64,
    output_tokens: u64,
    thinking_tokens: u64,
    input_cost: f64,
    output_cost: f64,
    thinking_cost: f64,
    elapsed_secs: f64,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    let config = load_config(&args)?;

    if args.show_config {
        print_config(&config);
        return Ok(());
    }

    let input_file = args.input_file.ok_or_else(|| anyhow!("input file required"))?;
    let output_file = args.output.ok_or_else(|| anyhow!("output file required"))?;

    // logging setup
    let timestamp = chrono::Local::now().format("%Y%m%d_%H%M%S");
    fs::create_dir_all(&config.logs_path)?;
    let log_file_path = Path::new(&config.logs_path).join(format!("{}.log", timestamp));
    println!("debug log path: {}", log_file_path.display());
    
    // simple logger
    let log_file = std::sync::Arc::new(std::sync::Mutex::new(File::create(&log_file_path)?));
    let logger = move |msg: &str| {
        let mut f = log_file.lock().unwrap();
        writeln!(f, "{}", msg).unwrap();
    };

    // Log config
    let mut config_val = serde_json::to_value(&config)?;
    config_val.as_object_mut().unwrap().insert("gemini_api_key".to_string(), json!("[REDACTED]"));
    config_val.as_object_mut().unwrap().insert("anthropic_api_key".to_string(), json!("[REDACTED]"));
    config_val.as_object_mut().unwrap().insert("openai_api_key".to_string(), json!("[REDACTED]"));
    logger(&format!("configuration: {}", serde_json::to_string_pretty(&config_val)?));

    // Load resources
    let languages_json = get_resource("languages.json", &config)?;
    let languages: HashMap<String, String> = serde_json::from_str(&languages_json)?;
    let pricing_json = get_resource("pricing.json", &config)?;
    let pricing_map: HashMap<String, Pricing> = serde_json::from_str(&pricing_json).unwrap_or_default();
    let prompt_tmpl = get_resource("prompt.tmpl", &config)?;

    // Determine target language
    let output_path = Path::new(&output_file);
    let ext = output_path.extension().and_then(|s| s.to_str()).unwrap_or("");
    let basename = output_path.file_name().and_then(|s| s.to_str()).unwrap_or("");
    
    let lookup_key = if !ext.is_empty() {
        if ext.starts_with('.') { ext.to_string() } else { format!(".{}", ext) }
    } else {
        basename.to_string()
    };
    
    let target_language = languages.get(&lookup_key).cloned().unwrap_or_else(|| lookup_key.clone());

    println!("transpiling '{}' to '{}' ({})", input_file, output_file, target_language);

    // Read input
    let english_content = fs::read_to_string(&input_file).context("failed to read input file")?;

    // Prepare prompt context
    let hacking = if !config.hacking_conventions.is_empty() {
        fs::read_to_string(&config.hacking_conventions).unwrap_or_default()
    } else {
        String::new()
    };

    let mut context_content = String::new();
    if !config.context_files.is_empty() {
        for fpath in config.context_files.split(':') {
            if fpath.is_empty() { continue; }
            let content = fs::read_to_string(fpath).context(format!("failed to read context file {}", fpath))?;
            context_content.push_str(&format!("\n### {}\n\n```\n{}\n```\n", fpath, content));
        }
    }

    // Expand prompt
    let mut generation_command = std::env::args().collect::<Vec<_>>().join(" ");
    // strip double hyphens for svg/xml derivative target formats
    if target_language.contains("xml") || target_language.contains("svg") || output_file.ends_with(".svg") || output_file.ends_with(".xml") {
        generation_command = generation_command.replace("--", "");
    }

    let prompt = prompt_tmpl
        .replace("{{generation_command}}", &generation_command)
        .replace("{{generation_config}}", &serde_json::to_string_pretty(&config_val)?)
        .replace("{{target_language}}", &target_language)
        .replace("{{output_path}}", &output_file)
        .replace("{{english_content}}", &english_content)
        .replace("{{hacking_conventions}}", &hacking)
        .replace("{{context_files}}", &context_content);

    logger(&format!("prompt:\n{}", prompt));

    // Main loop
    let mut current_prompt = prompt.clone();
    let mut total_stats = ApiStats::default();
    let mut iterations = 0;
    let max_iterations = if config.test_command.is_empty() { 1 } else { config.test_iterations };

    // Resolve max tokens once
    let pricing_key = format!("{}/{}", config.provider, config.model);
    let pricing = pricing_map.get(&pricing_key);
    let model_max_tokens = pricing.map(|p| p.max_tokens).unwrap_or(8192);
    
    // Effective max tokens: use config override if present, else use model max
    let effective_max_tokens = config.max_tokens.unwrap_or(model_max_tokens);
    
    // Prepare run config with resolved max_tokens
    let mut run_config = config.clone();
    run_config.max_tokens = Some(effective_max_tokens);

    loop {
        iterations += 1;
        println!("calling api (provider: {}, model: {})...", config.provider, config.model);
        let start_time = Instant::now();

        // API Call
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(config.timeout))
            .build()?;

        let api_result = call_api(&client, &run_config, &current_prompt, &logger).await;
        
        let elapsed = start_time.elapsed();
        let seconds = elapsed.as_secs_f64();

        match api_result {
            Ok((code, usage, status_code)) => {
                println!("api call completed with response code {} after {:.2}s", status_code, seconds);

                // stats update
                let input_tokens = usage.0;
                let output_tokens = usage.1;
                let thinking_tokens = usage.2;

                // Cost calculation
                if let Some(p) = pricing {
                    let in_cost = input_tokens as f64 * p.input_cost_per_token;
                    let out_cost = output_tokens as f64 * p.output_cost_per_token;
                    // thinking tokens priced as output if not known
                    let think_cost = thinking_tokens as f64 * p.output_cost_per_token; 

                    total_stats.input_tokens += input_tokens;
                    total_stats.output_tokens += output_tokens;
                    total_stats.thinking_tokens += thinking_tokens;
                    total_stats.input_cost += in_cost;
                    total_stats.output_cost += out_cost;
                    total_stats.thinking_cost += think_cost;
                } else {
                    eprintln!("warning: pricing data not found for {}", pricing_key);
                    total_stats.input_tokens += input_tokens;
                    total_stats.output_tokens += output_tokens;
                    total_stats.thinking_tokens += thinking_tokens;
                }
                total_stats.elapsed_secs += seconds;

                // Validation: strip fences
                let mut lines: Vec<&str> = code.lines().collect();
                if !lines.is_empty() {
                    if lines[0].trim().starts_with("```") { lines.remove(0); }
                }
                if !lines.is_empty() {
                    if lines[lines.len()-1].trim().starts_with("```") { lines.pop(); }
                }
                let clean_code = lines.join("\n");

                if clean_code.trim().is_empty() {
                    eprintln!("error: received empty response from llm");
                    print_summary(&total_stats);
                    std::process::exit(1);
                }

                // Write output
                fs::write(&output_file, &clean_code)?;
                logger(&format!("generated code:\n{}", clean_code));

                // Test command
                if !config.test_command.is_empty() {
                    println!("executing test command (`{}`), attempt {}/{}", config.test_command, iterations, max_iterations);
                    let parts: Vec<&str> = config.test_command.split_whitespace().collect();
                    if parts.is_empty() { break; } 
                    
                    let mut cmd = Command::new(parts[0]);
                    cmd.args(&parts[1..]);
                    cmd.env_clear();
                    
                    // whitelist envs
                    for key in ["PATH", "RUSTUP_HOME", "CARGO_HOME"] {
                        if let Ok(val) = std::env::var(key) {
                            cmd.env(key, val);
                        }
                    }

                    let test_out = cmd.output();
                    match test_out {
                        Ok(out) => {
                            if out.status.success() {
                                println!("test command succeeded!");
                                println!("successfully transpiled '{}'", output_file);
                                break;
                            } else {
                                println!("test command failed, see log for details");
                                let stdout = String::from_utf8_lossy(&out.stdout);
                                let stderr = String::from_utf8_lossy(&out.stderr);
                                let log_msg = format!("test failed (code {:?}):\nSTDOUT:\n{}\nSTDERR:\n{}", out.status.code(), stdout, stderr);
                                logger(&log_msg);

                                if iterations >= max_iterations {
                                    print_summary(&total_stats);
                                    std::process::exit(1);
                                }

                                // Prepare retry prompt
                                current_prompt = format!("{}\n\nPREVIOUS GENERATION:\n```\n{}\n```\n\nTEST COMMAND: {}\nEXIT CODE: {:?}\n\nSTDOUT:\n{}\n\nSTDERR:\n{}\n\nPlease fix the code based on the test failure.", prompt, clean_code, config.test_command, out.status.code(), stdout, stderr);
                            }
                        }
                        Err(e) => {
                            eprintln!("failed to execute test command: {}", e);
                            logger(&format!("test execution error: {}", e));
                            break;
                        }
                    }
                } else {
                    println!("successfully transpiled '{}'", output_file);
                    break;
                }
            }
            Err(e) => {
                eprintln!("error: {}", e);
                logger(&format!("api error: {}", e));
                print_summary(&total_stats);
                std::process::exit(1);
            }
        }
    }

    print_summary(&total_stats);
    Ok(())
}

fn print_summary(stats: &ApiStats) {
    println!("\n--- api request summary ---");
    if stats.thinking_tokens > 0 {
        println!("tokens: {} input, {} output, {} thinking", stats.input_tokens, stats.output_tokens, stats.thinking_tokens);
    } else {
        println!("tokens: {} input, {} output", stats.input_tokens, stats.output_tokens);
    }
    
    if stats.input_cost > 0.0 || stats.output_cost > 0.0 {
        println!("estimated cost:");
        println!("  - input: ${:.6}", stats.input_cost);
        println!("  - output: ${:.6}", stats.output_cost);
        if stats.thinking_cost > 0.0 && stats.thinking_tokens > 0 {
            println!("  - thinking: ${:.6}", stats.thinking_cost);
        }
        println!("total: ${:.6}", stats.input_cost + stats.output_cost + stats.thinking_cost);
    }
    println!("elapsed time: {:.2}s", stats.elapsed_secs);
}

// (input, output, thinking)
async fn call_api(client: &reqwest::Client, config: &Config, prompt: &str, logger: &impl Fn(&str)) -> Result<(String, (u64, u64, u64), u16)> {
    match config.provider.as_str() {
        "google" => call_google(client, config, prompt, logger).await,
        "anthropic" => call_anthropic(client, config, prompt, logger).await,
        _ => call_openai(client, config, prompt, logger).await, // default/openai
    }
}

async fn call_google(client: &reqwest::Client, config: &Config, prompt: &str, logger: &impl Fn(&str)) -> Result<(String, (u64, u64, u64), u16)> {
    let url = format!("https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}", config.model, config.gemini_api_key);
    
    let mut body = json!({
        "contents": [{ "parts": [{ "text": prompt }] }],
        "generationConfig": {}
    });

    // Seed ISN'T supported. instead, temperature is set to 0 if seed explicitly provided
    if config.seed.is_some() {
        body["generationConfig"].as_object_mut().unwrap().insert("temperature".to_string(), json!(0.0));
    }

    if let Some(mt) = config.max_tokens {
         body["generationConfig"].as_object_mut().unwrap().insert("maxOutputTokens".to_string(), json!(mt));
    }

    let resp = client.post(&url).json(&body).send().await?;
    let status = resp.status();
    let text = resp.text().await?;
    logger(&format!("response: {}", text));

    if !status.is_success() {
        return Err(anyhow!("google api error: {} - {}", status, text));
    }

    let json: Value = serde_json::from_str(&text)?;
    
    // Concat parts
    let mut content = String::new();
    if let Some(candidates) = json["candidates"].as_array() {
        if let Some(first) = candidates.get(0) {
            if let Some(parts) = first["content"]["parts"].as_array() {
                for part in parts {
                    if let Some(t) = part["text"].as_str() {
                        content.push_str(t);
                    }
                }
            }
        }
    }

    let input_tok = json["usageMetadata"]["promptTokenCount"].as_u64().unwrap_or(0);
    let total_tok = json["usageMetadata"]["totalTokenCount"].as_u64().unwrap_or(0);
    let output_tok = json["usageMetadata"]["candidatesTokenCount"].as_u64().unwrap_or(0);
    
    // Thinking tokens calculation
    // "if total_tokens is provided but thinking_tokens is not, calculate ... total - input - output"
    let thinking_tok = if total_tok > (input_tok + output_tok) {
        total_tok - input_tok - output_tok
    } else { 0 };

    Ok((content, (input_tok, output_tok, thinking_tok), status.as_u16()))
}

async fn call_anthropic(client: &reqwest::Client, config: &Config, prompt: &str, logger: &impl Fn(&str)) -> Result<(String, (u64, u64, u64), u16)> {
    let url = "https://api.anthropic.com/v1/messages";
    
    let mut body = json!({
        "model": config.model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": config.max_tokens.unwrap_or(8192),
        "stream": true
    });

    // Seed workaround
    if config.seed.is_some() {
         body.as_object_mut().unwrap().insert("temperature".to_string(), json!(0.0));
    }

    // Thinking
    let mut thinking_enabled = false;
    if let Some(budget) = config.thinking_budget {
        if budget > 0 {
            thinking_enabled = true;
            body.as_object_mut().unwrap().insert("thinking".to_string(), json!({
                "type": "enabled",
                "budget_tokens": budget
            }));
            // "temperature may only be set to 1 when thinking is enabled"
            body.as_object_mut().unwrap().insert("temperature".to_string(), json!(1.0));
        }
    }

    let resp = client.post(url)
        .header("x-api-key", &config.anthropic_api_key)
        .header("anthropic-version", "2023-06-01")
        .json(&body)
        .send()
        .await?;

    let status = resp.status();
    if !status.is_success() {
        let text = resp.text().await?;
        return Err(anyhow!("anthropic api error: {} - {}", status, text));
    }

    // Streaming parser
    let mut stream = resp.bytes_stream();
    use futures::StreamExt;
    
    let mut full_content = String::new();
    let mut thinking_content = String::new();
    let mut input_tokens = 0;
    let mut output_tokens = 0;
    
    let mut buffer = String::new();

    while let Some(item) = stream.next().await {
        let chunk = item?;
        let s = String::from_utf8_lossy(&chunk);
        buffer.push_str(&s);

        while let Some(idx) = buffer.find('\n') {
            let line = buffer[..idx].to_string();
            buffer = buffer[idx+1..].to_string();
            
            let line = line.trim();
            if line.is_empty() || line.starts_with(':') { continue; }
            
            if let Some(data) = line.strip_prefix("data: ") {
                if data == "[DONE]" { break; }
                
                if let Ok(event) = serde_json::from_str::<Value>(data) {
                    let type_ = event["type"].as_str().unwrap_or("");
                    match type_ {
                        "message_start" => {
                            if let Some(usage) = event["message"]["usage"].as_object() {
                                input_tokens = usage.get("input_tokens").and_then(|v| v.as_u64()).unwrap_or(0);
                            }
                        },
                        "content_block_delta" => {
                            if let Some(delta) = event["delta"].as_object() {
                                let dtype = delta.get("type").and_then(|s| s.as_str()).unwrap_or("");
                                match dtype {
                                    "text_delta" => {
                                        if let Some(t) = delta.get("text").and_then(|s| s.as_str()) {
                                            full_content.push_str(t);
                                            logger(&format!("delta: {}", t));
                                        }
                                    },
                                    "thinking_delta" => {
                                        if let Some(t) = delta.get("thinking").and_then(|s| s.as_str()) {
                                            thinking_content.push_str(t);
                                            logger(&format!("thinking: {}", t));
                                        }
                                    },
                                    "signature_delta" => {
                                         logger("signature delta received");
                                    },
                                    _ => {}
                                }
                            }
                        },
                        "message_delta" => {
                             if let Some(usage) = event["usage"].as_object() {
                                output_tokens = usage.get("output_tokens").and_then(|v| v.as_u64()).unwrap_or(0);
                            }
                        },
                        "error" => {
                            return Err(anyhow!("stream error: {:?}", event));
                        },
                        _ => {}
                    }
                }
            }
        }
    }
    
    logger(&format!("final response: {}", full_content));

    // Estimate thinking tokens
    let thinking_tokens = if thinking_enabled && !thinking_content.is_empty() {
        (thinking_content.len() / 4) as u64
    } else { 0 };

    Ok((full_content, (input_tokens, output_tokens, thinking_tokens), status.as_u16()))
}

async fn call_openai(client: &reqwest::Client, config: &Config, prompt: &str, logger: &impl Fn(&str)) -> Result<(String, (u64, u64, u64), u16)> {
    let base = if config.openai_api_base.is_empty() { "https://api.openai.com/v1" } else { &config.openai_api_base };
    let url = format!("{}/chat/completions", base.trim_end_matches('/'));

    let mut body = json!({
        "model": config.model,
        "messages": [{"role": "user", "content": prompt}],
    });

    // Token param
    let token_param = if base.contains("api.openai.com") { "max_completion_tokens" } else { "max_tokens" };
    if let Some(mt) = config.max_tokens {
         body.as_object_mut().unwrap().insert(token_param.to_string(), json!(mt));
    }

    if let Some(seed) = config.seed {
         body.as_object_mut().unwrap().insert("seed".to_string(), json!(seed));
    }
    
    // Random seed if supported and not provided
    if config.seed.is_none() {
        let rnd: i64 = rand::random();
        body.as_object_mut().unwrap().insert("seed".to_string(), json!(rnd));
    }

    let resp = client.post(&url)
        .header("Authorization", format!("Bearer {}", config.openai_api_key))
        .json(&body)
        .send()
        .await?;

    let status = resp.status();
    let text = resp.text().await?;
    logger(&format!("response: {}", text));

    if !status.is_success() {
        return Err(anyhow!("openai api error: {} - {}", status, text));
    }

    let json: Value = serde_json::from_str(&text)?;
    let content = json["choices"][0]["message"]["content"].as_str().unwrap_or("").to_string();

    let usage = &json["usage"];
    let input_tok = usage["prompt_tokens"].as_u64().unwrap_or(0);
    let total_tok = usage["total_tokens"].as_u64().unwrap_or(0);
    let mut completion_tok = usage["completion_tokens"].as_u64().unwrap_or(0);
    
    // Thinking tokens extraction
    let mut thinking_tok = 0;
    if let Some(t) = usage["completion_tokens_details"]["reasoning_tokens"].as_u64() {
        thinking_tok = t;
    } else if let Some(t) = usage["reasoning_tokens"].as_u64() {
        thinking_tok = t;
    } else if let Some(t) = usage["thinking_tokens"].as_u64() {
        thinking_tok = t;
    } else if total_tok > 0 {
        thinking_tok = total_tok.saturating_sub(input_tok).saturating_sub(completion_tok);
    }

    // If completion_tokens is 0 but we have total, infer completion
    if completion_tok == 0 && total_tok > 0 {
        completion_tok = total_tok - input_tok;
    }

    Ok((content, (input_tok, completion_tok, thinking_tok), status.as_u16()))
}