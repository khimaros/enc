// NOTICE: this file was automatically generated by https://github.com/khimaros/enc using the following invocation: ./enc src/enc.en -o src/main.rs --context-files ./.enc.env.example:./res/pricing.json:./Cargo.toml
use anyhow::{anyhow, Context, Result};
use chrono::Local;
use clap::Parser;
use lazy_static::lazy_static;
use regex::Regex;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::env;
use std::fs::{self, File, OpenOptions};
use std::io::Write;
use std::path::{Path, PathBuf};
use std::sync::Mutex;
use std::time::Duration;

lazy_static! {
    // each invocation of enc creates a new log file using a timestamp as the file name.
    // detailed logging goes ONLY to the log file, not to the console.
    static ref LOGGER: Mutex<Option<File>> = Mutex::new(None);
    // if the language model returns markdown fencing around the code, it will be removed before being written to the output file.
    static ref MARKDOWN_FENCE_RE: Regex = Regex::new(r"(?s)^```[^\n]*\n(.*?)\n```$").unwrap();
}

// detailed logging goes ONLY to the log file, not to the console.
fn log_message(msg: &str) {
    if let Ok(mut logger_guard) = LOGGER.lock() {
        if let Some(file) = logger_guard.as_mut() {
            let timestamp = Local::now().format("%Y-%m-%d %H:%M:%S");
            writeln!(file, "[{}] {}", timestamp, msg).ok();
        }
    }
}

fn main() -> Result<()> {
    // enc uses a layered configuration. settings are loaded in the following order:
    // 1. home config: ~/.enc.env
    if let Some(home_dir) = dirs::home_dir() {
        let home_config_path = home_dir.join(".enc.env");
        // when using an external dotenv library, all env file paths are explicitly defined to ensure the correct path is used.
        if home_config_path.exists() {
            dotenvy::from_path(&home_config_path).ok();
        }
    }
    // 2. working dir config: ./.enc.env
    let work_dir_config_path = PathBuf::from("./.enc.env");
    if work_dir_config_path.exists() {
        dotenvy::from_path(&work_dir_config_path).ok();
    }

    // 3. environment variables
    // 4. command line flags
    // subsequent sources override previous ones.
    let config = Config::parse();

    // when the `--show-config` flag is provided, enc dumps the consolidated config and then exits immediately.
    if config.show_config {
        let sanitized_config = get_sanitized_config(&config);
        println!("{}", serde_json::to_string_pretty(&sanitized_config)?);
        return Ok(());
    }

    // when using a flag parsing library, ... all flags are defined as optional and enforced procedurally
    let input_file = config.input_file.as_ref().context("input file is required")?;
    let output_path = config.output_path.as_ref().context("output path is required")?;

    // each invocation of enc creates a new log file using a timestamp as the file name.
    let log_file_path = setup_logging(&config)?;
    // the full path to the log file is written to the console when the program starts.
    println!("debug log path: {}", log_file_path);

    // the consolidated configuration is logged at the start of each invocation.
    // API keys are omitted from the log output.
    log_message(&format!(
        "consolidated config: {:#?}",
        get_sanitized_config(&config)
    ));

    // the target language is automatically determined based on the file extension of the output file.
    let target_language = determine_target_language(output_path);
    println!(
        "transpiling '{}' to '{}' ({})",
        input_file, output_path, target_language
    );

    // `english_content`: the exact content of the provided `.en` input file
    let english_content =
        fs::read_to_string(input_file).context(format!("failed to read input file '{}'", input_file))?;

    // the prompt template is loaded from the file specified by the PROMPT_TEMPLATE_PATH setting.
    let prompt = build_prompt(&config, &english_content, &target_language)?;
    // the template expanded prompt is logged.
    log_message(&format!("expanded prompt:\n---\n{}\n---", prompt));

    // enc can make use of any OpenAI compatible API backend.
    // enc can make use of the Google Generative AI API backend.
    // enc can make use of Anthropic Claude API backend.
    let (response_text, usage) =
        api::call_provider(&config, &prompt).context("api call failed")?;

    // text content returned by the LLM is logged.
    log_message(&format!("llm response:\n---\n{}\n---", response_text));

    // if the language model returns markdown fencing around the code, it will be removed
    let final_code = strip_markdown_fences(&response_text);

    fs::write(output_path, final_code)
        .context(format!("failed to write output file '{}'", output_path))?;

    println!("successfully transpiled '{}'", output_path);

    // pricing data for language model API calls is shown after the call completes
    print_cost_summary(&config, &usage)?;

    Ok(())
}

// EVERY setting is also available as command line flag.
#[derive(Parser, Debug, Clone, Serialize)]
#[command(name = "enc", version, about = "transpiles english to any programming language")]
struct Config {
    /// english language file to transpile
    input_file: Option<String>,

    #[arg(short = 'o', long, help = "output file path")]
    output_path: Option<String>,

    #[arg(long, env = "PROVIDER", default_value = "google", help = "language model API provider: \"google\", \"anthropic\", or \"openai\"")]
    provider: String,

    #[arg(long, env = "MODEL", default_value = "gemini-2.5-pro", help = "model to use for inference")]
    model: String,

    #[arg(long, env = "MAX_TOKENS", help = "maximum number of tokens for the LLM to generate")]
    max_tokens: Option<u32>,

    #[arg(long, env = "THINKING_BUDGET", help = "maximum tokens for deep thought")]
    thinking_budget: Option<u32>,

    #[arg(long, env = "GROUNDED_MODE", default_value_t = false, help = "adds the existing code (if present) to context files")]
    grounded_mode: bool,

    #[arg(long, env = "SEED", help = "seed used for inference (empty means random)")]
    seed: Option<u64>,

    #[arg(long, env = "HACKING_CONVENTIONS", help = "coding conventions and style guide")]
    hacking_conventions: Option<String>,

    #[arg(long, env = "CONTEXT_FILES", value_delimiter = ':', help = "colon-separated list of additional context files")]
    context_files: Vec<String>,

    #[serde(skip_serializing)]
    #[arg(long, env = "GEMINI_API_KEY", help = "google gemini api key")]
    gemini_api_key: Option<String>,

    #[serde(skip_serializing)]
    #[arg(long, env = "OPENAI_API_KEY", help = "openai api key")]
    openai_api_key: Option<String>,

    #[serde(skip_serializing)]
    #[arg(long, env = "ANTHROPIC_API_KEY", help = "anthropic api key")]
    anthropic_api_key: Option<String>,

    #[arg(long, env = "OPENAI_API_BASE", help = "openai api base url")]
    openai_api_base: Option<String>,

    #[arg(long, env = "LOGS_PATH", default_value = "./log/", help = "path to store api logs")]
    logs_path: String,

    #[arg(long, env = "PROMPT_TEMPLATE_PATH", default_value = "./res/prompt.tmpl", help = "path to the code generation prompt template")]
    prompt_template_path: String,

    #[arg(long, env = "PRICING_DATA_PATH", default_value = "./res/pricing.json", help = "path to the pricing data json file")]
    pricing_data_path: String,

    #[arg(long, help = "show the consolidated config and exit")]
    show_config: bool,
}

// API keys are omitted from the log output.
fn get_sanitized_config(config: &Config) -> Config {
    let mut sanitized = config.clone();
    sanitized.gemini_api_key = None;
    sanitized.openai_api_key = None;
    sanitized.anthropic_api_key = None;
    sanitized
}

// each invocation of enc creates a new log file using a timestamp as the file name.
fn setup_logging(config: &Config) -> Result<String> {
    let log_dir = Path::new(&config.logs_path);
    fs::create_dir_all(log_dir).context("failed to create log directory")?;
    let timestamp = Local::now().format("%Y%m%d_%H%M%S");
    let log_file_path = log_dir.join(format!("{}.log", timestamp));
    let log_file = OpenOptions::new()
        .create(true)
        .write(true)
        .append(true)
        .open(&log_file_path)
        .context("failed to open log file")?;

    *LOGGER.lock().unwrap() = Some(log_file);
    Ok(log_file_path.to_string_lossy().to_string())
}

// enc searches for resource files ... relative to the current working directory. if it can't find them there, it will search in `${XDG_DATA_HOME}/enc/`
fn find_resource_path(base_path_str: &str) -> Result<PathBuf> {
    let cwd_path = PathBuf::from(base_path_str);
    if cwd_path.exists() {
        return Ok(cwd_path);
    }
    if let Some(data_dir) = dirs::data_dir() {
        let xdg_path = data_dir.join("enc").join(base_path_str);
        if xdg_path.exists() {
            return Ok(xdg_path);
        }
    }
    Err(anyhow!("resource not found: {}", base_path_str))
}

// there ISN'T any prompt template embedded in the code for enc.
// the prompt template is loaded from the file specified by the PROMPT_TEMPLATE_PATH setting.
fn build_prompt(config: &Config, english_content: &str, target_language: &str) -> Result<String> {
    let template_path = find_resource_path(&config.prompt_template_path)?;
    let mut template = fs::read_to_string(template_path)?;

    // template expansion is done ONCE as a SINGLE PASS
    // `generation_command`: the command line args enc was called with
    let mut generation_command = env::args().collect::<Vec<_>>().join(" ");
    // if the target format is a derivative of XML (eg. SVG), enc will strip double-hyphens (`--`)
    if ["xml", "html", "svg", "xhtml"].contains(&target_language) {
        generation_command = generation_command.replace("--", "");
    }
    template = template.replace("{{generation_command}}", &generation_command);

    // `target_language`: the target programming language.
    template = template.replace("{{target_language}}", target_language);
    // `output_path`: the output path of the file being generated
    template = template.replace(
        "{{output_path}}",
        config.output_path.as_deref().unwrap_or(""),
    );
    // `english_content`: the exact content of the provided `.en` input file
    template = template.replace("{{english_content}}", english_content);

    // `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
    let hacking_content = if let Some(path) = &config.hacking_conventions {
        fs::read_to_string(path).unwrap_or_default()
    } else {
        String::new()
    };
    template = template.replace("{{hacking_conventions}}", &hacking_content);

    // `context_files`: the exact contents of the files configured in CONTEXT_FILES.
    let mut context_files_content = String::new();
    // grounded mode adds the existing code (if present) to context files
    if config.grounded_mode {
        if let Some(path) = &config.output_path {
            if let Ok(content) = fs::read_to_string(path) {
                context_files_content.push_str(&format!(
                    "### {}\n```\n{}\n```\n\n",
                    path, content
                ));
            }
        }
    }
    for path in &config.context_files {
        if let Ok(content) = fs::read_to_string(path) {
            context_files_content.push_str(&format!("### {}\n```\n{}\n```\n\n", path, content));
        }
    }
    template = template.replace("{{context_files}}", &context_files_content);

    Ok(template)
}

// the target language is automatically determined based on the file extension of the output file.
// if the target language is not recognized by the file extension, the file extension or the file base name are passed to the LLM
fn determine_target_language(output_path: &str) -> String {
    let path = Path::new(output_path);
    path.extension()
        .and_then(|s| s.to_str())
        .map(|s| match s {
            "rs" => "rust",
            "py" => "python",
            "js" => "javascript",
            "ts" => "typescript",
            "go" => "go",
            "c" => "c",
            "cpp" => "c++",
            "java" => "java",
            "sh" => "shell",
            "rb" => "ruby",
            "html" => "html",
            "css" => "css",
            "json" => "json",
            "md" => "markdown",
            "svg" => "svg",
            "xml" => "xml",
            _ => s,
        })
        .unwrap_or_else(|| path.file_name().and_then(|s| s.to_str()).unwrap_or("unknown"))
        .to_string()
}

// if the language model returns markdown fencing around the code, it will be removed
fn strip_markdown_fences(content: &str) -> String {
    let trimmed = content.trim();
    if let Some(captures) = MARKDOWN_FENCE_RE.captures(trimmed) {
        captures.get(1).map_or(trimmed, |m| m.as_str()).to_string()
    } else {
        trimmed.to_string()
    }
}

// pricing data for language model API calls is shown after the call completes
fn print_cost_summary(config: &Config, usage: &api::ApiUsage) -> Result<()> {
    let pricing_data = pricing::load_pricing(config)?;
    let model_key = format!("{}/{}", config.provider, config.model);

    if let Some(model_pricing) = pricing_data.0.get(&model_key) {
        let input_cost = usage.input_tokens as f64 * model_pricing.input_cost_per_token;
        let output_cost = usage.output_tokens as f64 * model_pricing.output_cost_per_token;
        // if thinking token costs are not known, they should be priced the same as output tokens.
        let thinking_cost = usage.thinking_tokens as f64
            * model_pricing
                .thinking_cost_per_token
                .unwrap_or(model_pricing.output_cost_per_token);
        let total_cost = input_cost + output_cost + thinking_cost;

        println!("\n--- api cost summary ---");
        println!("provider: {}, model: {}", config.provider, config.model);
        println!(
            "tokens: {} input, {} output, {} thinking",
            usage.input_tokens, usage.output_tokens, usage.thinking_tokens
        );
        println!("estimated cost:");
        println!("  - input   : ${:.6}", input_cost);
        println!("  - output  : ${:.6}", output_cost);
        println!("  - thinking: ${:.6}", thinking_cost);
        println!("total: ${:.6}", total_cost);
    } else {
        log_message(&format!("warning: no pricing data found for {}", model_key));
    }
    Ok(())
}

// language model prices
mod pricing {
    use super::*;

    // model pricing data should be loaded from `./res/pricing.json`
    #[derive(Deserialize, Debug)]
    pub struct PricingData(pub HashMap<String, ModelPricing>);

    #[derive(Deserialize, Debug)]
    #[allow(dead_code)]
    pub struct ModelPricing {
        pub max_tokens: Option<u32>,
        pub input_cost_per_token: f64,
        pub output_cost_per_token: f64,
        pub thinking_cost_per_token: Option<f64>,
    }

    pub fn load_pricing(config: &Config) -> Result<PricingData> {
        let path = find_resource_path(&config.pricing_data_path)?;
        let file = File::open(path)?;
        let data: PricingData =
            serde_json::from_reader(file).context("failed to parse pricing.json")?;
        Ok(data)
    }
}

mod api {
    use super::*;

    // token budgets are enforced for all models.
    #[derive(Debug, Default)]
    pub struct ApiUsage {
        pub input_tokens: u32,
        pub output_tokens: u32,
        pub thinking_tokens: u32,
    }

    // common interface for different providers
    trait LLMProvider {
        fn generate(&self, config: &Config, prompt: &str) -> Result<(String, ApiUsage)>;
    }

    // factory function to select and call the correct provider
    pub fn call_provider(config: &Config, prompt: &str) -> Result<(String, ApiUsage)> {
        match config.provider.as_str() {
            "google" => {
                let api_key = config.gemini_api_key.as_ref().context("GEMINI_API_KEY is not set")?;
                GoogleClient::new(api_key).generate(config, prompt)
            }
            "openai" => {
                let api_key = config.openai_api_key.as_ref().context("OPENAI_API_KEY is not set")?;
                OpenAIClient::new(api_key, config.openai_api_base.as_deref()).generate(config, prompt)
            }
            "anthropic" => {
                let api_key = config.anthropic_api_key.as_ref().context("ANTHROPIC_API_KEY is not set")?;
                AnthropicClient::new(api_key).generate(config, prompt)
            }
            _ => Err(anyhow!("unsupported provider: {}", config.provider)),
        }
    }

    // enc can make use of the Google Generative AI API backend.
    struct GoogleClient<'a> {
        api_key: &'a str,
        client: reqwest::blocking::Client,
    }

    impl<'a> GoogleClient<'a> {
        fn new(api_key: &'a str) -> Self {
            Self {
                api_key,
                client: reqwest::blocking::Client::new(),
            }
        }
    }

    impl LLMProvider for GoogleClient<'_> {
        fn generate(&self, config: &Config, prompt: &str) -> Result<(String, ApiUsage)> {
            #[derive(Serialize)]
            struct GoogleRequest<'p> {
                contents: Vec<Content<'p>>,
                #[serde(rename = "generationConfig")]
                generation_config: GenerationConfig,
            }
            #[derive(Serialize)]
            struct Content<'p> {
                parts: Vec<Part<'p>>,
            }
            #[derive(Serialize)]
            struct Part<'p> {
                text: &'p str,
            }
            #[derive(Serialize)]
            struct GenerationConfig {
                #[serde(skip_serializing_if = "Option::is_none")]
                temperature: Option<f32>,
                #[serde(rename = "maxOutputTokens", skip_serializing_if = "Option::is_none")]
                max_output_tokens: Option<u32>,
            }

            #[derive(Deserialize)]
            struct GoogleResponse {
                candidates: Vec<Candidate>,
                #[serde(rename = "usageMetadata")]
                usage_metadata: UsageMetadata,
            }
            #[derive(Deserialize)]
            struct Candidate {
                content: ResponseContent,
            }
            #[derive(Deserialize)]
            struct ResponseContent {
                parts: Vec<ResponsePart>,
            }
            #[derive(Deserialize)]
            struct ResponsePart {
                text: String,
            }
            #[derive(Deserialize)]
            struct UsageMetadata {
                #[serde(rename = "promptTokenCount")]
                prompt_token_count: u32,
                #[serde(rename = "candidatesTokenCount")]
                candidates_token_count: u32,
                #[serde(rename = "totalTokenCount")]
                total_token_count: u32,
            }

            let url = format!(
                "https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}",
                config.model, self.api_key
            );

            // with the "google" provider, the seed parameter ISN'T supported. instead, temperature is set to 0.
            let temperature = if config.seed.is_some() { Some(0.0) } else { None };

            let request_body = GoogleRequest {
                contents: vec
![Content {
                    parts: vec![Part { text: prompt }],
                }],
                generation_config: GenerationConfig {
                    temperature,
                    max_output_tokens: config.max_tokens,
                },
            };

            // enc uses explicit timeouts for all API calls (default 900 seconds).
            let response = self.client.post(url)
                .json(&request_body)
                .timeout(Duration::from_secs(900))
                .send()?
                .error_for_status()?;

            let response_data: GoogleResponse = response.json()?;
            let text = response_data.candidates.get(0).and_then(|c| c.content.parts.get(0)).map_or("", |p| &p.text).to_string();
            
            // with some providers, thinking token usage must be calculated by subtracting
            let usage = ApiUsage {
                input_tokens: response_data.usage_metadata.prompt_token_count,
                output_tokens: response_data.usage_metadata.candidates_token_count,
                thinking_tokens: response_data.usage_metadata.total_token_count
                    - response_data.usage_metadata.prompt_token_count
                    - response_data.usage_metadata.candidates_token_count,
            };

            Ok((text, usage))
        }
    }

    // enc can make use of any OpenAI compatible API backend.
    struct OpenAIClient<'a> {
        api_key: &'a str,
        base_url: String,
        client: reqwest::blocking::Client,
    }

    impl<'a> OpenAIClient<'a> {
        fn new(api_key: &'a str, base_url: Option<&str>) -> Self {
            Self {
                api_key,
                base_url: base_url.unwrap_or("https://api.openai.com/v1").to_string(),
                client: reqwest::blocking::Client::new(),
            }
        }
    }

    impl LLMProvider for OpenAIClient<'_> {
        fn generate(&self, config: &Config, prompt: &str) -> Result<(String, ApiUsage)> {
            #[derive(Serialize)]
            struct OpenAIRequest<'p> {
                model: &'p str,
                messages: Vec<Message<'p>>,
                #[serde(skip_serializing_if = "Option::is_none")]
                seed: Option<u64>,
                #[serde(skip_serializing_if = "Option::is_none")]
                max_tokens: Option<u32>,
            }
            #[derive(Serialize)]
            struct Message<'p> {
                role: &'p str,
                content: &'p str,
            }

            #[derive(Deserialize)]
            struct OpenAIResponse {
                choices: Vec<Choice>,
                usage: Usage,
            }
            #[derive(Deserialize)]
            struct Choice {
                message: ResponseMessage,
            }
            #[derive(Deserialize)]
            struct ResponseMessage {
                content: String,
            }
            #[derive(Deserialize)]
            struct Usage {
                prompt_tokens: u32,
                completion_tokens: u32,
            }

            let url = format!("{}/chat/completions", self.base_url);
            let request_body = OpenAIRequest {
                model: &config.model,
                messages: vec
![Message { role: "user", content: prompt }],
                seed: config.seed,
                max_tokens: config.max_tokens,
            };

            let response = self.client.post(url)

                .bearer_auth(self.api_key)
                .json(&request_body)
                .timeout(Duration::from_secs(900))
                .send()?
                .error_for_status()?;

            let response_data: OpenAIResponse = response.json()?;
            let text = response_data.choices.get(0).map_or("", |c| &c.message.content).to_string();
            let usage = ApiUsage {
                input_tokens: response_data.usage.prompt_tokens,
                output_tokens: response_data.usage.completion_tokens,
                thinking_tokens: 0, // openai doesn't have a separate thinking phase/cost
            };

            Ok((text, usage))
        }
    }

    // enc can make use of Anthropic Claude API backend.
    struct AnthropicClient<'a> {
        api_key: &'a str,
        client: reqwest::blocking::Client,
    }

    impl<'a> AnthropicClient<'a> {
        fn new(api_key: &'a str) -> Self {
            Self {
                api_key,
                client: reqwest::blocking::Client::new(),
            }
        }
    }

    impl LLMProvider for AnthropicClient<'_> {
        fn generate(&self, config: &Config, prompt: &str) -> Result<(String, ApiUsage)> {
            #[derive(Serialize)]
            struct AnthropicRequest<'p> {
                model: &'p str,
                messages: Vec<Message<'p>>,
                max_tokens: u32,
            }
            #[derive(Serialize)]
            struct Message<'p> {
                role: &'p str,
                content: &'p str,
            }

            #[derive(Deserialize)]
            struct AnthropicResponse {
                content: Vec<ResponseContent>,
                usage: Usage,
            }
            #[derive(Deserialize)]
            struct ResponseContent {
                text: String,
            }
            #[derive(Deserialize)]
            struct Usage {
                input_tokens: u32,
                output_tokens: u32,
            }

            let url = "https://api.anthropic.com/v1/messages";
            let request_body = AnthropicRequest {
                model: &config.model,
                messages: vec
![Message { role: "user", content: prompt }],
                max_tokens: config.max_tokens.unwrap_or(4096)
, // anthropic requires max_tokens
            };

            let response = self.client.post(url)
                .header("x-api-key", self.api_key)
                .header("anthropic-version", "2023-06-01")
                .json(&request_body)
                .timeout(Duration::from_secs(900))
                .send()?
                .error_for_status()?;

            let response_data: AnthropicResponse = response.json()?;
            let text = response_data.content.get(0).map_or("", |c| &c.text).to_string();
            let usage = ApiUsage {
                input_tokens: response_data.usage.input_tokens,
                output_tokens: response_data.usage.output_tokens,
                thinking_tokens: 0, // anthropic doesn't have a separate thinking phase/cost without tools
            };

            Ok((text, usage))
        }
    }
}
