// automatically generated by https://github.com/khimaros/enc using the following invocation: ./enc-release src/enc.en -o src/main.rs --context-files ./.enc.env.example:./Cargo.toml

use anyhow::{anyhow, Context, Result};
use chrono::Local;
use clap::Parser;
use futures::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::BTreeMap;
use std::fs;
use std::io::Write;
use std::path::{Path, PathBuf};
use std::process::{Command, Stdio};
use std::time::{Duration, Instant};

// "enc uses explicit timeouts (TIMEOUT from settings) for all API calls."
const DEFAULT_TIMEOUT: u64 = 1800;
const DEFAULT_TEST_ITERATIONS: i32 = 3;
const DEFAULT_HACKING_CONVENTIONS: &str = "./HACKING.md";
const DEFAULT_LOGS_PATH: &str = "./log/";
const DEFAULT_RESOURCES_PATH: &str = "./res/:${XDG_DATA_HOME}/enc/res/";
const DEFAULT_OPENAI_BASE: &str = "https://api.openai.com/v1";

const PRICING_JSON: &str = include_str!("../res/pricing.json");
const LANGUAGES_JSON: &str = include_str!("../res/languages.json");
const PROMPT_TMPL: &str = include_str!("../res/prompt.tmpl");

#[derive(Parser, Debug, Clone, Serialize, Deserialize)]
#[command(author, version, about, long_about = None)]
struct Args {
    // "the command-line interface MUST follow this exact pattern where the input file is a positional argument"
    #[arg(help = "the input file is the first positional argument, NOT a flag.")]
    input_file: Option<PathBuf>,

    #[arg(short, long, help = "the output file is specified via the -o or --output flag.")]
    output: Option<PathBuf>,

    #[arg(long, env = "PROVIDER")]
    provider: Option<String>,

    #[arg(long, env = "MODEL")]
    model: Option<String>,

    #[arg(long, env = "MAX_TOKENS")]
    max_tokens: Option<u32>,

    #[arg(long, env = "THINKING_BUDGET")]
    thinking_budget: Option<u32>,

    #[arg(long, env = "SEED")]
    seed: Option<i64>,

    #[arg(long, env = "HACKING_CONVENTIONS")]
    hacking_conventions: Option<String>,

    #[arg(long, env = "TIMEOUT")]
    timeout: Option<u64>,

    #[arg(long, env = "CONTEXT_FILES")]
    context_files: Option<String>,

    #[arg(long, env = "GEMINI_API_KEY")]
    gemini_api_key: Option<String>,

    #[arg(long, env = "ANTHROPIC_API_KEY")]
    anthropic_api_key: Option<String>,

    #[arg(long, env = "OPENAI_API_KEY")]
    openai_api_key: Option<String>,

    #[arg(long, env = "OPENAI_API_BASE")]
    openai_api_base: Option<String>,

    #[arg(long, env = "LOGS_PATH")]
    logs_path: Option<String>,

    #[arg(long, env = "RESOURCES_PATH")]
    resources_path: Option<String>,

    #[arg(long, env = "TEST_COMMAND")]
    test_command: Option<String>,

    #[arg(long, env = "TEST_ITERATIONS")]
    test_iterations: Option<i32>,

    #[arg(long)]
    show_config: bool,
}

struct Logger {
    file_path: PathBuf,
}

impl Logger {
    // "each invocation of enc creates a new log file in the directory specified by the LOGS_PATH setting"
    fn new(logs_dir: &str) -> Result<Self> {
        fs::create_dir_all(logs_dir).ok();
        let ts = Local::now().format("%Y%m%d_%H%M%S").to_string();
        let path = PathBuf::from(logs_dir).join(format!("{}.log", ts));
        Ok(Self { file_path: path })
    }

    // "detailed logging goes ONLY to the log file, not to the console."
    fn log(&self, msg: &str) {
        if let Ok(mut f) = fs::OpenOptions::new().create(true).append(true).open(&self.file_path) {
            let _ = writeln!(f, "{}", msg);
        }
    }
}

#[derive(Serialize, Deserialize, Clone, Debug)]
struct Pricing {
    input_cost_per_token: f64,
    input_cost_per_token_above_200k_tokens: Option<f64>,
    output_cost_per_token: f64,
    output_cost_per_token_above_200k_tokens: Option<f64>,
}

#[derive(Default)]
struct Stats {
    input_tokens: u32,
    output_tokens: u32,
    thinking_tokens: u32,
    total_time: Duration,
}

// "enc uses a layered configuration. settings are sourced with the following precedence"
fn load_consolidated_config() -> Result<Args> {
    let mut args = Args::parse();
    let mut load_env_file = |path: PathBuf| {
        if let Ok(content) = fs::read_to_string(path) {
            for line in content.lines() {
                let line = line.trim();
                if line.is_empty() || line.starts_with('#') { continue; }
                if let Some((k, v)) = line.split_once('=') {
                    let v = v.trim_matches('"').trim_matches('\'').to_string();
                    match k.trim() {
                        "PROVIDER" if args.provider.is_none() => args.provider = Some(v),
                        "MODEL" if args.model.is_none() => args.model = Some(v),
                        "MAX_TOKENS" if args.max_tokens.is_none() => args.max_tokens = v.parse().ok(),
                        "THINKING_BUDGET" if args.thinking_budget.is_none() => args.thinking_budget = v.parse().ok(),
                        "SEED" if args.seed.is_none() => args.seed = v.parse().ok(),
                        "HACKING_CONVENTIONS" if args.hacking_conventions.is_none() => args.hacking_conventions = Some(v),
                        "TIMEOUT" if args.timeout.is_none() => args.timeout = v.parse().ok(),
                        "CONTEXT_FILES" if args.context_files.is_none() => args.context_files = Some(v),
                        "GEMINI_API_KEY" if args.gemini_api_key.is_none() => args.gemini_api_key = Some(v),
                        "ANTHROPIC_API_KEY" if args.anthropic_api_key.is_none() => args.anthropic_api_key = Some(v),
                        "OPENAI_API_KEY" if args.openai_api_key.is_none() => args.openai_api_key = Some(v),
                        "OPENAI_API_BASE" if args.openai_api_base.is_none() => args.openai_api_base = Some(v),
                        "LOGS_PATH" if args.logs_path.is_none() => args.logs_path = Some(v),
                        "RESOURCES_PATH" if args.resources_path.is_none() => args.resources_path = Some(v),
                        "TEST_COMMAND" if args.test_command.is_none() => args.test_command = Some(v),
                        "TEST_ITERATIONS" if args.test_iterations.is_none() => args.test_iterations = v.parse().ok(),
                        _ => {}
                    }
                }
            }
        }
    };

    load_env_file(PathBuf::from("./.enc.env"));
    if let Some(home) = dirs::home_dir() { load_env_file(home.join(".enc.env")); }

    if args.provider.is_none() { args.provider = Some("google".to_string()); }
    if args.model.is_none() {
        args.model = Some(match args.provider.as_deref() {
            Some("anthropic") => "claude-sonnet-4-20250514".to_string(),
            Some("openai") => "gpt-5-2025-08-07".to_string(),
            _ => "gemini-2.5-pro".to_string(),
        });
    }
    if args.hacking_conventions.is_none() { args.hacking_conventions = Some(DEFAULT_HACKING_CONVENTIONS.to_string()); }
    if args.timeout.is_none() { args.timeout = Some(DEFAULT_TIMEOUT); }
    if args.logs_path.is_none() { args.logs_path = Some(DEFAULT_LOGS_PATH.to_string()); }
    if args.resources_path.is_none() { args.resources_path = Some(DEFAULT_RESOURCES_PATH.to_string()); }
    if args.test_iterations.is_none() { args.test_iterations = Some(DEFAULT_TEST_ITERATIONS); }
    if args.openai_api_base.is_none() { args.openai_api_base = Some(DEFAULT_OPENAI_BASE.to_string()); }
    if args.context_files.is_none() { args.context_files = Some("".to_string()); }
    if args.test_command.is_none() { args.test_command = Some("".to_string()); }
    
    Ok(args)
}

fn redact_path(p: &str) -> String {
    if Path::new(p).is_absolute() { "[REDACTED]".to_string() } else { p.to_string() }
}

// "the "redacted configuration" is what results from filtering API keys from the consolidated configuration."
fn redact_config(config: &Args, redact_paths: bool) -> serde_json::Value {
    let mut map = BTreeMap::new();
    let v = serde_json::to_value(config).unwrap();
    if let Some(obj) = v.as_object() {
        for (k, val) in obj {
            if ["input_file", "output", "show_config"].contains(&k.as_str()) { continue; }
            if k.ends_with("_api_key") {
                map.insert(k.clone(), serde_json::Value::String("[REDACTED]".into()));
                continue;
            }
            if ["max_tokens", "seed", "thinking_budget"].contains(&k.as_str()) && val.is_null() { continue; }
            if ["hacking_conventions", "context_files", "openai_api_base", "test_command"].contains(&k.as_str()) && val.as_str() == Some("") { continue; }
            let mut final_val = val.clone();
            if redact_paths {
                if let Some(s) = final_val.as_str() { final_val = serde_json::Value::String(redact_path(s)); }
            }
            map.insert(k.clone(), final_val);
        }
    }
    serde_json::to_value(map).unwrap()
}

// "enc searches for resource files (any file under `./res/`) in a specific order"
fn get_resource(name: &str, resources_path: &str, embedded: &str) -> String {
    let path_str = resources_path.replace("${XDG_DATA_HOME}", 
        &dirs::data_local_dir().map(|p| p.to_string_lossy().into_owned()).unwrap_or_else(|| {
            dirs::home_dir().map(|h| h.join(".local/share").to_string_lossy().into_owned()).unwrap_or_default()
        }));
    for p in path_str.split(':') {
        let full_path = Path::new(p).join(name);
        if let Ok(content) = fs::read_to_string(full_path) { return content; }
    }
    embedded.to_string()
}

// "the target language name is loaded from `./res/languages.json`."
fn detect_language(path: &Path, lang_map_json: &str) -> String {
    let map: BTreeMap<String, String> = serde_json::from_str(lang_map_json).unwrap_or_default();
    let ext = path.extension().and_then(|e| e.to_str()).map(|e| format!(".{}", e));
    let stem = path.file_name().and_then(|n| n.to_str());
    if let Some(ref e) = ext { if let Some(l) = map.get(e) { return l.clone(); } }
    if let Some(s) = stem { if let Some(l) = map.get(s) { return l.clone(); } }
    ext.or(stem.map(|s| s.to_string())).unwrap_or_else(|| "unknown".into())
}

fn expand_prompt(template: &str, vars: &BTreeMap<String, String>) -> String {
    let mut result = template.to_string();
    for (k, v) in vars { result = result.replace(&format!("{{{{{}}}}}", k), v); }
    result
}

// "the test command is executed with a restricted environment"
fn run_test_command(cmd: &str) -> Result<(i32, String)> {
    let whitelist = ["PATH", "RUSTUP_HOME", "CARGO_HOME"];
    let child = Command::new("sh")
        .arg("-c").arg(cmd).env_clear()
        .envs(std::env::vars().filter(|(k, _)| whitelist.contains(&k.as_str())))
        .stdout(Stdio::piped()).stderr(Stdio::piped()).spawn()?;
    let output = child.wait_with_output()?;
    let combined = format!("{}{}", String::from_utf8_lossy(&output.stdout), String::from_utf8_lossy(&output.stderr));
    Ok((output.status.code().unwrap_or(1), combined))
}

fn clean_code(code: &str) -> String {
    let lines: Vec<&str> = code.lines().collect();
    if lines.is_empty() { return "".into(); }
    let mut start = 0;
    let mut end = lines.len();
    if lines[0].starts_with("```") { start = 1; }
    if end > start && lines[end - 1].starts_with("```") { end -= 1; }
    lines[start..end].join("\n").trim().to_string()
}

// "pricing data for language model API calls is shown after each individual call completes"
fn report_call(stats: &Stats, provider: &str, model: &str, pricing_json: &str) {
    println!("\n--- api request summary ---");
    let thinking_str = if stats.thinking_tokens > 0 { format!(", {} thinking", stats.thinking_tokens) } else { "".into() };
    println!("tokens: {} input, {} output{}", stats.input_tokens, stats.output_tokens, thinking_str);
    let prices: BTreeMap<String, Pricing> = serde_json::from_str(pricing_json).unwrap_or_default();
    let key = format!("{}/{}", provider, model);
    if let Some(p) = prices.get(&key) {
        let calc = |toks: u32, base: f64, tier: Option<f64>| {
            if let Some(t) = tier { if toks > 200_000 { return toks as f64 * t; } }
            toks as f64 * base
        };
        let ic = calc(stats.input_tokens, p.input_cost_per_token, p.input_cost_per_token_above_200k_tokens);
        let oc = calc(stats.output_tokens, p.output_cost_per_token, p.output_cost_per_token_above_200k_tokens);
        let tc = calc(stats.thinking_tokens, p.output_cost_per_token, p.output_cost_per_token_above_200k_tokens);
        println!("estimated cost:");
        println!("  - input: ${:.6}", ic);
        println!("  - output: ${:.6}", oc);
        if stats.thinking_tokens > 0 { println!("  - thinking: ${:.6}", tc); }
        println!("total: ${:.6}", ic + oc + tc);
    } else {
        println!("warning: pricing data for model {} not found", key);
    }
    println!("elapsed time: {:.2}s", stats.total_time.as_secs_f64());
}

async fn call_google(config: &Args, prompt: &str, logger: &Logger) -> Result<(String, u32, u32, u32)> {
    let client = reqwest::Client::new();
    let url = format!("https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}", 
        config.model.as_ref().unwrap(), config.gemini_api_key.as_ref().context("missing GEMINI_API_KEY")?);
    let body = serde_json::json!({
        "contents": [{ "parts": [{ "text": prompt }] }],
        "generationConfig": { "maxOutputTokens": config.max_tokens, "temperature": if config.seed.is_some() { 0.0 } else { 1.0 } }
    });
    let resp = client.post(url).json(&body).timeout(Duration::from_secs(config.timeout.unwrap())).send().await?;
    let status = resp.status();
    let text = resp.text().await?;
    logger.log(&format!("google response ({}): {}", status, text));
    if !status.is_success() { return Err(anyhow!("api call failed: {}", status)); }
    let json: serde_json::Value = serde_json::from_str(&text)?;
    let content = json["candidates"][0]["content"]["parts"].as_array()
        .map(|parts| parts.iter().filter_map(|p| p["text"].as_str()).collect::<String>())
        .ok_or_else(|| anyhow!("empty response from google"))?;
    let usage = &json["usageMetadata"];
    let input = usage["promptTokenCount"].as_u64().unwrap_or(0) as u32;
    let output = usage["candidatesTokenCount"].as_u64().unwrap_or(0) as u32;
    // "for the "google" provider, thinking tokens may be included... calculate thinking tokens using the formula"
    let thinking = usage["thinkingTokenCount"].as_u64().or_else(|| 
        usage["totalTokenCount"].as_u64().map(|t| t.saturating_sub(input as u64 + output as u64))
    ).unwrap_or(0) as u32;
    Ok((content, input, output, thinking))
}

async fn call_anthropic(config: &Args, prompt: &str, logger: &Logger) -> Result<(String, u32, u32, u32)> {
    let client = reqwest::Client::new();
    let mut body = serde_json::json!({
        "model": config.model.as_ref().unwrap(),
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": config.max_tokens.unwrap_or(4096),
        "stream": true,
        "temperature": if config.thinking_budget.unwrap_or(0) > 0 { 1.0 } else if config.seed.is_some() { 0.0 } else { 1.0 }
    });
    if let Some(budget) = config.thinking_budget {
        if budget > 0 { body.as_object_mut().unwrap().insert("thinking".into(), serde_json::json!({"type": "enabled", "budget_tokens": budget})); }
    }
    let mut stream = client.post("https://api.anthropic.com/v1/messages")
        .header("x-api-key", config.anthropic_api_key.as_ref().context("missing ANTHROPIC_API_KEY")?)
        .header("anthropic-version", "2023-06-01")
        .json(&body).timeout(Duration::from_secs(config.timeout.unwrap())).send().await?.bytes_stream();
    let mut full_text = String::new();
    let mut thinking_text = String::new();
    let (mut input_tokens, mut output_tokens) = (0, 0);
    let mut buffer = String::new();
    while let Some(chunk) = stream.next().await {
        buffer.push_str(&String::from_utf8_lossy(&chunk?));
        while let Some(idx) = buffer.find('\n') {
            let line = buffer.drain(..idx + 1).collect::<String>();
            if line.starts_with("data: ") {
                let data = line[6..].trim();
                if let Ok(json) = serde_json::from_str::<serde_json::Value>(data) {
                    match json["type"].as_str() {
                        Some("message_start") => input_tokens = json["message"]["usage"]["input_tokens"].as_u64().unwrap_or(0) as u32,
                        Some("content_block_delta") => {
                            let delta = &json["delta"];
                            match delta["type"].as_str() {
                                Some("text_delta") => full_text.push_str(delta["text"].as_str().unwrap_or("")),
                                Some("thinking_delta") => {
                                    let dt = delta["thinking"].as_str().unwrap_or("");
                                    thinking_text.push_str(dt);
                                    logger.log(&format!("thinking_delta: {}", dt));
                                },
                                Some("signature_delta") => logger.log(&format!("signature_delta: {}", delta["signature"].as_str().unwrap_or(""))),
                                _ => {}
                            }
                        }
                        Some("message_delta") => output_tokens = json["usage"]["output_tokens"].as_u64().unwrap_or(0) as u32,
                        _ => {}
                    }
                }
            }
        }
    }
    logger.log(&format!("final response length: {}", full_text.len()));
    // "thinking token count should be estimated by counting tokens... character count divided by 4"
    Ok((full_text, input_tokens, output_tokens, (thinking_text.len() as f32 / 4.0) as u32))
}

async fn call_openai(config: &Args, prompt: &str, logger: &Logger) -> Result<(String, u32, u32, u32)> {
    let client = reqwest::Client::new();
    let base = config.openai_api_base.as_ref().filter(|s| !s.is_empty()).map(|s| s.as_str()).unwrap_or(DEFAULT_OPENAI_BASE);
    let mut body = serde_json::json!({ "model": config.model, "messages": [{"role": "user", "content": prompt}], "seed": config.seed });
    let mt_key = if base.contains("api.openai.com") { "max_completion_tokens" } else { "max_tokens" };
    if let Some(mt) = config.max_tokens { body.as_object_mut().unwrap().insert(mt_key.into(), mt.into()); }
    let resp = client.post(format!("{}/chat/completions", base))
        .bearer_auth(config.openai_api_key.as_ref().context("missing OPENAI_API_KEY")?)
        .json(&body).timeout(Duration::from_secs(config.timeout.unwrap())).send().await?;
    let status = resp.status();
    let text = resp.text().await?;
    logger.log(&format!("openai response ({}): {}", status, text));
    if !status.is_success() { return Err(anyhow!("openai api failed: {}", status)); }
    let json: serde_json::Value = serde_json::from_str(&text)?;
    let content = json["choices"][0]["message"]["content"].as_str().ok_or_else(|| anyhow!("empty response"))?.to_string();
    let usage = &json["usage"];
    let input = usage["prompt_tokens"].as_u64().unwrap_or(0) as u32;
    let output = usage["completion_tokens"].as_u64().unwrap_or(0) as u32;
    let thinking = usage["completion_tokens_details"]["reasoning_tokens"]
        .as_u64().or(usage["reasoning_tokens"].as_u64()).or(usage["thinking_tokens"].as_u64()).unwrap_or(0) as u32;
    Ok((content, input, output, thinking))
}

#[tokio::main]
async fn main() -> Result<()> {
    let config = load_consolidated_config()?;
    if config.show_config {
        println!("{}", serde_json::to_string_pretty(&redact_config(&config, false))?);
        return Ok(());
    }
    let input_path = config.input_file.clone().context("input_file is a required positional argument")?;
    let output_path = config.output.clone().context("-o/--output is required")?;
    let logger = Logger::new(config.logs_path.as_ref().unwrap())?;
    println!("debug log path: {}", logger.file_path.display());
    let pricing_data = get_resource("pricing.json", config.resources_path.as_ref().unwrap(), PRICING_JSON);
    let lang_map = get_resource("languages.json", config.resources_path.as_ref().unwrap(), LANGUAGES_JSON);
    let tmpl = get_resource("prompt.tmpl", config.resources_path.as_ref().unwrap(), PROMPT_TMPL);
    let target_lang = detect_language(&output_path, &lang_map);
    println!("transpiling '{}' to '{}' ({})", input_path.display(), output_path.display(), target_lang);
    let mut context_str = String::new();
    if let Some(ctx) = &config.context_files {
        for path in ctx.split(':').filter(|s| !s.is_empty()) {
            let content = fs::read_to_string(path).with_context(|| format!("failed to read context file: {}", path))?;
            context_str.push_str(&format!("### {}\n\n```\n{}\n```\n\n", path, content));
        }
    }
    let mut gen_cmd = std::env::args().collect::<Vec<_>>().join(" ");
    if ["svg", "xml", "html"].contains(&target_lang.as_str()) { gen_cmd = gen_cmd.replace("--", ""); }
    let mut vars = BTreeMap::new();
    vars.insert("generation_command".into(), gen_cmd);
    vars.insert("generation_config".into(), serde_json::to_string(&redact_config(&config, true))?);
    vars.insert("target_language".into(), target_lang);
    vars.insert("output_path".into(), output_path.to_string_lossy().into_owned());
    vars.insert("english_content".into(), fs::read_to_string(&input_path)?);
    vars.insert("hacking_conventions".into(), config.hacking_conventions.as_ref().and_then(|p| fs::read_to_string(p).ok()).unwrap_or_default());
    vars.insert("context_files".into(), context_str);
    let mut prompt = expand_prompt(&tmpl, &vars);
    let mut stats = Stats::default();
    let mut attempts = 0;
    let max_attempts = config.test_iterations.unwrap_or(DEFAULT_TEST_ITERATIONS);
    loop {
        attempts += 1;
        let provider = config.provider.as_ref().unwrap().as_str();
        println!("calling api (provider: {}, model: {})...", provider, config.model.as_ref().unwrap());
        logger.log(&format!("prompt:\n{}", prompt));
        let start_time = Instant::now();
        let call_res = match provider {
            "google" => call_google(&config, &prompt, &logger).await,
            "anthropic" => call_anthropic(&config, &prompt, &logger).await,
            _ => call_openai(&config, &prompt, &logger).await,
        };
        let elapsed = start_time.elapsed();
        stats.total_time += elapsed;
        match call_res {
            Ok((raw_code, input, output, thinking)) => {
                println!("api call completed with response code 200 after {:.2}s", elapsed.as_secs_f64());
                stats.input_tokens += input; stats.output_tokens += output; stats.thinking_tokens += thinking;
                let code = clean_code(&raw_code);
                if code.is_empty() {
                    if max_attempts != -1 && attempts >= max_attempts as u32 { report_call(&stats, provider, config.model.as_ref().unwrap(), &pricing_data); return Err(anyhow!("llm returned an empty response")); }
                    continue;
                }
                fs::write(&output_path, &code)?;
                if let Some(tcmd) = config.test_command.as_ref().filter(|s| !s.is_empty()) {
                    println!("executing test command (`{}`), attempt {}/{}", tcmd, attempts, if max_attempts == -1 { "âˆž".into() } else { max_attempts.to_string() });
                    let (status, output) = run_test_command(tcmd)?;
                    if status == 0 { println!("test command succeeded!\nsuccessfully transpiled '{}'", output_path.display()); break; }
                    println!("test command failed, see log for details");
                    logger.log(&format!("test failed (code {}):\n{}", status, output));
                    if max_attempts != -1 && attempts >= max_attempts as u32 { report_call(&stats, provider, config.model.as_ref().unwrap(), &pricing_data); std::process::exit(1); }
                    prompt = format!("{}\n\nPREVIOUS_GENERATED_CODE:\n{}\n\nTEST_COMMAND: {}\nEXIT_CODE: {}\nOUTPUT:\n{}", prompt, code, tcmd, status, output);
                } else { println!("successfully transpiled '{}'", output_path.display()); break; }
            }
            Err(e) => { report_call(&stats, provider, config.model.as_ref().unwrap(), &pricing_data); return Err(e); }
        }
    }
    report_call(&stats, config.provider.as_ref().unwrap(), config.model.as_ref().unwrap(), &pricing_data);
    Ok(())
}
// shebang lines are not needed as this is a compiled binary.
// the Notice comment is at the top of the file.
// functions are kept short and comments use quotes from the input content.