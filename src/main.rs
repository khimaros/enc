// NOTICE: this file was automatically generated by https://github.com/khimaros/enc using the following invocation: ./enc-release src/enc.en -o src/main.rs --context-files ./.enc.env.example:./res/pricing.json:./res/languages.json:./Cargo.toml
#
![allow(clippy::too_many_arguments)
]
#
![allow(clippy::upper_case_acronyms)
]

use anyhow::{anyhow, Context, Result};
use chrono::Local;
use clap::Parser;
use dirs::home_dir;
use rand::Rng;
use regex::Regex;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::collections::HashMap;
use std::env;
use std::fs::{self, File};
use std::io::Write;
use std::path::PathBuf;
use std::time::Duration;

// HACKING: keep dependencies to a minimum.
// HACKING: ensure that all dependencies are imported at the top of the file.

// ENGLISH DESCRIPTION: the default value for all settings are commented out in `./.enc.env.example`.
// ENGLISH DESCRIPTION: EVERY configuration setting is also available as command line flag.
// ENGLISH DESCRIPTION: when using a flag parsing library, all short flags are explicitly defined to avoid conflicts with automatic naming. required arguments are declared as such, unless they are optional when another flag (like --show-config) is present.
#[derive(Parser, Debug, Clone)]
#[command(version, about, long_about = None)]
struct Cli {
    /// path to the english language file to transpile
    #[arg(required_unless_present("show_config"))]
    input_file: Option<String>,

    /// path to the output file
    #[arg(short = 'o', long, required_unless_present("show_config"))]
    output_file: Option<String>,

    /// language model API provider: "google", "anthropic", or "openai"
    #[arg(long, env = "PROVIDER")]
    provider: Option<String>,

    /// model to use for inference
    #[arg(long, env = "MODEL")]
    model: Option<String>,

    /// maximum number of tokens for the LLM to generate
    #[arg(long, env = "MAX_TOKENS")]
    max_tokens: Option<u32>,

    /// maximum tokens for deep thought
    #[arg(long, env = "THINKING_BUDGET")]
    thinking_budget: Option<u32>,

    /// seed used for inference (empty means random)
    #[arg(long, env = "SEED")]
    seed: Option<u64>,

    /// coding conventions and style guide
    #[arg(long, env = "HACKING_CONVENTIONS", default_value = "./HACKING.md")]
    hacking_conventions: String,

    /// timeout for API requests in seconds
    #[arg(long, env = "TIMEOUT")]
    timeout: Option<u64>,

    /// additional file paths to include as context (colon separated)
    #[arg(long, env = "CONTEXT_FILES", value_delimiter = ':')]
    context_files: Option<Vec<String>>,

    /// google gemini API key
    #[arg(long, env = "GEMINI_API_KEY", hide_env = true)]
    gemini_api_key: Option<String>,

    /// anthropic API key
    #[arg(long, env = "ANTHROPIC_API_KEY", hide_env = true)]
    anthropic_api_key: Option<String>,

    /// openAI API key
    #[arg(long, env = "OPENAI_API_KEY", hide_env = true)]
    openai_api_key: Option<String>,

    /// openAI compatible API base url
    #[arg(long, env = "OPENAI_API_BASE")]
    openai_api_base: Option<String>,

    /// path to store API logs
    #[arg(long, env = "LOGS_PATH")]
    logs_path: Option<String>,

    /// path to resources (pricing, language map, templates)
    #[arg(long, env = "RESOURCES_PATH", value_delimiter = ':')]
    resources_path: Option<Vec<String>>,

    /// show the consolidated configuration and exit
    #[arg(long)]
    show_config: bool,
}

// ENGLISH DESCRIPTION: the "consolidated configuration" is what results from processing all layered configuration sources.
#[derive(Debug, Clone, Serialize)]
struct Config {
    input_file: PathBuf,
    output_file: PathBuf,
    provider: String,
    model: String,
    max_tokens: Option<u32>,
    thinking_budget: u32,
    seed: u64,
    user_provided_seed: bool,
    hacking_conventions: Option<PathBuf>,
    timeout: u64,
    context_files: Vec<PathBuf>,
    gemini_api_key: String,
    anthropic_api_key: String,
    openai_api_key: String,
    openai_api_base: String,
    logs_path: PathBuf,
    resources_path: Vec<PathBuf>,
}

// ENGLISH DESCRIPTION: the "redacted configuration" is what results from filtering the consolidated configuration.
// ENGLISH DESCRIPTION: API keys and absolute paths are ALWAYS omitted or redacted from output to the console, to the LLM, or to logs.
impl Config {
    fn redacted(&self) -> Value {
        let mut config_json = serde_json::to_value(self.clone()).unwrap();
        if !self.gemini_api_key.is_empty() {
            config_json["gemini_api_key"] = Value::String("[REDACTED]".to_string());
        }
        if !self.anthropic_api_key.is_empty() {
            config_json["anthropic_api_key"] = Value::String("[REDACTED]".to_string());
        }
        if !self.openai_api_key.is_empty() {
            config_json["openai_api_key"] = Value::String("[REDACTED]".to_string());
        }
        config_json
    }
}

// ENGLISH DESCRIPTION: language model pricing data will be loaded from `./res/pricing.json`
#[derive(Debug, Deserialize, Clone)]
struct ModelPricing {
    max_tokens: Option<u32>,
    input_cost_per_token: Option<f64>,
    output_cost_per_token: Option<f64>,
    thinking_cost_per_token: Option<f64>,
}
type PricingData = HashMap<String, ModelPricing>;

// ENGLISH DESCRIPTION: there is a language map of the form `{"rs": "rust"}` in `./res/languages.json`.
type LanguageMap = HashMap<String, String>;

#[derive(Debug, Default)]
struct ApiUsage {
    input_tokens: u32,
    output_tokens: u32,
    thinking_tokens: u32,
}

// a single comment per function describing its purpose is adequate.
/// main entry point of the application.
#[tokio::main]
async fn main() -> Result<()> {
    // ENGLISH DESCRIPTION: enc uses a layered configuration. settings are loaded in the following order:
    // ENGLISH DESCRIPTION: 1. home config: ~/.enc.env
    // ENGLISH DESCRIPTION: 2. working dir config: ./.enc.env
    // ENGLISH DESCRIPTION: 3. command line flags
    // ENGLISH DESCRIPTION: subsequent configuration sources override earlier ones.
    // ENGLISH DESCRIPTION: when using an external dotenv library, all env file paths are explicitly defined to ensure the correct path is used.
    // hacking: dotenvy gives precedence to the first file loaded, so we load them in order of precedence (highest first).
    dotenvy::from_path(".enc.env").ok();
    if let Some(home) = home_dir() {
        dotenvy::from_path(home.join(".enc.env")).ok();
    }

    let cli = Cli::parse();
    let config = build_config(cli.clone())?;

    // ENGLISH DESCRIPTION: when the --show-config flag is provided, enc should first finish loading all sources of configuration. all other options are OPTIONAL in this case. after it has shown the config, the program exits immediately.
    if cli.show_config {
        let redacted_config = config.redacted();
        println!("{}", serde_json::to_string_pretty(&redacted_config)?);
        return Ok(());
    }

    validate_config_for_run(&config)?;

    // ENGLISH DESCRIPTION: each invocation of enc creates a new log file using a timestamp as the file name.
    fs::create_dir_all(&config.logs_path)
        .with_context(|| format!("failed to create log directory: {:?}", &config.logs_path))?;
    let log_path = config
        .logs_path
        .join(format!("{}.log", Local::now().format("%Y%m%d_%H%M%S")));
    let mut log_file = File::create(&log_path)
        .with_context(|| format!("failed to create log file: {:?}", &log_path))?;

    // ENGLISH DESCRIPTION: the full path to the log file is written to the console when the program starts.
    println!("debug log path: {}", log_path.display());

    // ENGLISH DESCRIPTION: the consolidated configuration is logged at the start of each invocation.
    // ENGLISH DESCRIPTION: API keys are redacted from the log output.
    writeln!(
        log_file,
        "--- consolidated config ---\n{}\n",
        serde_json::to_string_pretty(&config.redacted())?
    )?;

    // ENGLISH DESCRIPTION: enc is a program that can be used to transpile any english language file (eg. "hello.en") into any other programming language (eg. "hello.c")
    let result = run_transpile(&config, &mut log_file).await;

    // ENGLISH DESCRIPTION: errors always include the file and line number, at least when built or run in debug mode.
    if let Err(e) = result {
        eprintln!("error: {e:?}");
        writeln!(log_file, "error: {e:?}")?;
        std::process::exit(1);
    }

    Ok(())
}

// a single comment per function describing its purpose is adequate.
/// builds the final configuration from command line arguments and defaults.
fn build_config(cli: Cli) -> Result<Config> {
    // ENGLISH DESCRIPTION: the user can provide a seed to request deterministic outputs. if no seed is provided, a random seed is chosen at startup time.
    let user_provided_seed = cli.seed.is_some();
    let seed = cli.seed.unwrap_or_else(|| rand::thread_rng().gen());

    let input_file = cli.input_file.clone().map(PathBuf::from).unwrap_or_default();
    let output_file = cli.output_file.clone().map(PathBuf::from).unwrap_or_default();

    let hacking_conventions = if cli.hacking_conventions.is_empty() {
        None
    } else {
        Some(PathBuf::from(cli.hacking_conventions))
    };

    let config = Config {
        input_file,
        output_file,
        provider: cli.provider.unwrap_or_else(|| "google".to_string()),
        model: cli.model.unwrap_or_else(|| "gemini-2.5-pro".to_string()),
        max_tokens: cli.max_tokens,
        thinking_budget: cli.thinking_budget.unwrap_or(2048),
        seed,
        user_provided_seed,
        hacking_conventions,
        timeout: cli.timeout.unwrap_or(1800),
        context_files: cli
            .context_files
            .unwrap_or_default()
            .into_iter()
            .map(PathBuf::from)
            .collect(),
        gemini_api_key: cli.gemini_api_key.unwrap_or_default(),
        anthropic_api_key: cli.anthropic_api_key.unwrap_or_default(),
        openai_api_key: cli.openai_api_key.unwrap_or_default(),
        openai_api_base: cli
            .openai_api_base
            .unwrap_or_else(|| "https://api.openai.com/v1".to_string()),
        logs_path: cli.logs_path.map(PathBuf::from).unwrap_or_else(|| PathBuf::from("./log/")),
        resources_path: cli
            .resources_path
            .unwrap_or_else(|| vec
!["./res/".to_string()
, "${XDG_DATA_HOME}/enc/res/".to_string()])
            .into_iter()
            .map(PathBuf::from)
            .collect(),
    };

    Ok(config)
}

// a single comment per function describing its purpose is adequate.
/// validates the configuration for a normal run, checking for required values.
fn validate_config_for_run(config: &Config) -> Result<()> {
    // ENGLISH DESCRIPTION: the API key of the SELECTED provider is the ONLY one that is required at runtime.
    match config.provider.as_str() {
        "google" if config.gemini_api_key.is_empty() => {
            Err(anyhow!("GEMINI_API_KEY is required for google provider"))
        }
        "anthropic" if config.anthropic_api_key.is_empty() => {
            Err(anyhow!(
                "ANTHROPIC_API_KEY is required for anthropic provider"
            ))
        }
        "openai" if config.openai_api_key.is_empty() => {
            Err(anyhow!("OPENAI_API_KEY is required for openai provider"))
        }
        _ => Ok(()),
    }
}

// a single comment per function describing its purpose is adequate.
/// orchestrates the transpilation process.
async fn run_transpile(config: &Config, log_file: &mut File) -> Result<()> {
    println!(
        "provider: {}, model: {}",
        config.provider, config.model
    );
    // ENGLISH DESCRIPTION: example usage
    println!(
        "transpiling '{}' to '{}' ({})",
        config.input_file.display(),
        config.output_file.display(),
        get_target_language(config, log_file)?.unwrap_or_else(|| "unknown".to_string())
    );

    let (prompt, _target_language) = build_prompt(config, log_file)?;

    // ENGLISH DESCRIPTION: the template expanded prompt is logged.
    writeln!(log_file, "--- prompt ---\n{prompt}\n")?;

    let (response_text, usage) = call_llm(config, &prompt, log_file).await?;

    // ENGLISH DESCRIPTION: text content returned by the LLM is logged.
    writeln!(log_file, "--- llm response ---\n{response_text}\n")?;

    // ENGLISH DESCRIPTION: if the language model returns markdown fencing around the code, it will be removed before being written to the output file.
    let final_code = strip_markdown_fences(&response_text);

    fs::write(&config.output_file, final_code)
        .with_context(|| format!("failed to write to output file: {:?}", &config.output_file))?;

    println!("successfully transpiled '{}'", config.output_file.display());

    // ENGLISH DESCRIPTION: pricing data for language model API calls is shown after the call completes
    print_api_cost_summary(config, &usage, log_file)?;

    Ok(())
}

// a single comment per function describing its purpose is adequate.
/// loads a resource file by searching specified paths and falling back to embedded content.
// ENGLISH DESCRIPTION: enc searches for resource files (any file under `./res/`) in a specific order
fn load_resource(config: &Config, name: &str) -> Result<String> {
    // ENGLISH DESCRIPTION: 1. each path (colon separated) provided in the `RESOURCES_PATH` setting. `${XDG_DATA_HOME}` will be expanded to the env variable contents.
    for base in &config.resources_path {
        let path_str = base.to_string_lossy();
        let expanded_path = if path_str.contains("${XDG_DATA_HOME}") {
            let xdg_data_home = env::var("XDG_DATA_HOME")
                .or_else(|_| env::var("HOME").map(|h| format!("{h}/.local/share")))?;
            PathBuf::from(path_str.replace("${XDG_DATA_HOME}", &xdg_data_home))
        } else {
            base.clone()
        };

        let file_path = expanded_path.join(name);
        if file_path.exists() {
            return fs::read_to_string(file_path).context("failed to read resource file");
        }
    }

    // ENGLISH DESCRIPTION: 2. (for compiled languages like rust) all resource files will also be compiled into the binary using macros and read from memory.
    match name {
        "prompt.tmpl" => Ok(include_str!("../res/prompt.tmpl").to_string()),
        "pricing.json" => Ok(include_str!("../res/pricing.json").to_string()),
        "languages.json" => Ok(include_str!("../res/languages.json").to_string()),
        _ => Err(anyhow!("resource not found and not embedded: {name}")),
    }
}

// a single comment per function describing its purpose is adequate.
/// builds the full prompt string by substituting variables into the template.
// ENGLISH DESCRIPTION: the prompt template is loaded from resource `./res/prompt.tmpl`.
fn build_prompt(config: &Config, log_file: &mut File) -> Result<(String, String)> {
    let mut prompt = load_resource(config, "prompt.tmpl")?;

    // ENGLISH DESCRIPTION: `generation_command`: the command line args enc was called with
    let mut generation_command = env::args().collect::<Vec<String>>().join(" ");
    let target_language = get_target_language(config, log_file)?.unwrap_or_default();

    // ENGLISH DESCRIPTION: if the target format is a derivative of XML (eg. SVG), enc will strip double-hyphens (`--`) from `generation_command`
    if target_language == "xml" || target_language == "svg" {
        generation_command = generation_command.replace("--", "");
    }
    prompt = prompt.replace("{{generation_command}}", &generation_command);

    // ENGLISH DESCRIPTION: `generation_config`: the consolidated configuration, with API keys redacted
    let redacted_config_str = serde_json::to_string_pretty(&config.redacted())?;
    prompt = prompt.replace("{{generation_config}}", &redacted_config_str);

    // ENGLISH DESCRIPTION: `target_language`: the target programming language.
    prompt = prompt.replace("{{target_language}}", &target_language);

    // ENGLISH DESCRIPTION: `output_path`: the output path of the file being generated
    prompt = prompt.replace(
        "{{output_path}}",
        &config.output_file.to_string_lossy(),
    );

    // ENGLISH DESCRIPTION: `english_content`: the exact content of the provided `.en` input file
    let english_content = fs::read_to_string(&config.input_file)
        .with_context(|| format!("failed to read input file: {:?}", &config.input_file))?;
    prompt = prompt.replace("{{english_content}}", &english_content);

    // ENGLISH DESCRIPTION: `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
    // ENGLISH DESCRIPTION: if the HACKING_CONVENTIONS source file is not present, that section can be skipped.
    let hacking_conventions = if let Some(path) = &config.hacking_conventions {
        fs::read_to_string(path)
            .or_else(|e| {
                if e.kind() == std::io::ErrorKind::NotFound {
                    Ok("".to_string())
                } else {
                    Err(e)
                }
            })
            .with_context(|| format!("failed to read hacking conventions file: {path:?}"))?
    } else {
        "".to_string()
    };
    prompt = prompt.replace("{{hacking_conventions}}", &hacking_conventions);

    // ENGLISH DESCRIPTION: `context_files`: the exact, unmodified paths and contents of the files configured in CONTEXT_FILES.
    // ENGLISH DESCRIPTION: this applies to all files from CONTEXT FILES under ./res/
    let mut context_content = String::new();
    for path in &config.context_files {
        let content = if path.starts_with("./res/") {
            let resource_name = path
                .strip_prefix("./res/")
                .unwrap()
                .to_str()
                .context("invalid resource path")?;
            load_resource(config, resource_name)?
        } else {
            fs::read_to_string(path)
                .with_context(|| format!("failed to read context file: {path:?}"))?
        };

        // ENGLISH DESCRIPTION: for each file, include a markdown header with the relative path to that file, followed by the full contents of the file inside a markdown fenced code block.
        context_content.push_str(&format!(
            "### {}\n```\n{}\n```\n\n",
            path.to_string_lossy(),
            content
        ));
    }
    prompt = prompt.replace("{{context_files}}", &context_content);

    // ENGLISH DESCRIPTION: template expansion is done ONCE as a SINGLE PASS
    Ok((prompt, target_language))
}

// a single comment per function describing its purpose is adequate.
/// determines the target programming language from the output file extension.
// ENGLISH DESCRIPTION: the target language is automatically determined based on the file extension of the output file.
fn get_target_language(config: &Config, log_file: &mut File) -> Result<Option<String>> {
    let lang_map_str = load_resource(config, "languages.json")?;
    let lang_map: LanguageMap = serde_json::from_str(&lang_map_str)?;

    let extension = config
        .output_file
        .extension()
        .and_then(|s| s.to_str())
        .map(|s| format!(".{s}"))
        .unwrap_or_default();

    if let Some(lang) = lang_map.get(&extension) {
        return Ok(Some(lang.clone()));
    }

    // ENGLISH DESCRIPTION: if the target language is not recognized by the file extension, the file extension or the file base name are passed to the LLM at inference time.
    writeln!(
        log_file,
        "warning: unknown file extension '{}', using extension as language.",
        extension
    )?;
    if !extension.is_empty() {
        return Ok(Some(extension));
    }

    let filename = config
        .output_file
        .file_name()
        .and_then(|s| s.to_str())
        .unwrap_or_default();
    Ok(Some(filename.to_string()))
}

// a single comment per function describing its purpose is adequate.
/// dispatches the API call to the configured provider.
async fn call_llm(config: &Config, prompt: &str, log_file: &mut File) -> Result<(String, ApiUsage)> {
    match config.provider.as_str() {
        "google" => call_google_api(config, prompt, log_file).await,
        "anthropic" => call_anthropic_api(config, prompt, log_file).await,
        "openai" => call_openai_api(config, prompt, log_file).await,
        p => Err(anyhow!("unsupported provider: {p}")),
    }
}

// a single comment per function describing its purpose is adequate.
/// handles the API call to google's gemini models.
async fn call_google_api(
    config: &Config,
    prompt: &str,
    log_file: &mut File,
) -> Result<(String, ApiUsage)> {
    let url = format!(
        "https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}",
        config.model, config.gemini_api_key
    );

    let mut generation_config = serde_json::json!({});
    // ENGLISH DESCRIPTION: with the "google" provider, the seed parameter ISN'T supported. instead, temperature is set to 0.
    // ENGLISH DESCRIPTION: the temperature workaround is only used if seed is EXPLICITLY provided by the user in settings or flags.
    if config.user_provided_seed {
        generation_config["temperature"] = serde_json::json!(0.0);
    }

    let body = serde_json::json!({
        "contents": [{ "parts": [{ "text": prompt }] }],
        "generationConfig": generation_config
    });

    let client = reqwest::Client::builder()
        .timeout(Duration::from_secs(config.timeout))
        .build()?;

    let response = client.post(&url).json(&body).send().await?;

    if !response.status().is_success() {
        let status = response.status();
        let body_text = response
            .text()
            .await
            .unwrap_or_else(|_| "could not read response body".to_string());
        return Err(anyhow!(
            "google api error: status: {}\nbody: {}",
            status,
            body_text
        ));
    }

    let response_body: Value = response.json().await?;
    writeln!(
        log_file,
        "--- raw api response ---\n{}\n",
        serde_json::to_string_pretty(&response_body)?
    )?;

    let full_text = response_body["candidates"][0]["content"]["parts"]
        .as_array()
        .unwrap_or(&vec![])
        .iter()
        .filter_map(|part| part["text"].as_str())
        .collect::<String>();

    let mut usage = ApiUsage::default();
    if let Some(usage_meta) = response_body.get("usageMetadata") {
        let input_tokens = usage_meta["promptTokenCount"].as_u64().unwrap_or(0) as u32;
        let output_tokens = usage_meta["candidatesTokenCount"].as_u64().unwrap_or(0) as u32;
        let total_tokens = usage_meta["totalTokenCount"].as_u64().unwrap_or(0) as u32;

        usage.input_tokens = input_tokens;
        usage.output_tokens = output_tokens;
        // ENGLISH DESCRIPTION: some providers don't supply thinking token counts. in that case, they will be calculated by subtracting the known input/output token usage from the total token usage to calculate the thinking tokens.
        usage.thinking_tokens = total_tokens
            .saturating_sub(input_tokens)
            .saturating_sub(output_tokens);
    }

    Ok((full_text, usage))
}

// a single comment per function describing its purpose is adequate.
/// handles the API call to anthropic's claude models.
async fn call_anthropic_api(
    config: &Config,
    prompt: &str,
    log_file: &mut File,
) -> Result<(String, ApiUsage)> {
    let url = "https://api.anthropic.com/v1/messages";

    let mut body = serde_json::json!({
        "model": config.model,
        "max_tokens": get_max_tokens(config, log_file)?,
        "messages": [{ "role": "user", "content": prompt }],
        "stream": false,
    });

    // ENGLISH DESCRIPTION: with the "anthropic" provider, the seed parameter ISN'T supported. instead, temperature is set to 0.
    // ENGLISH DESCRIPTION: the temperature workaround is only used if seed is EXPLICITLY provided by the user in settings or flags.
    if config.user_provided_seed {
        body["temperature"] = serde_json::json!(0.0);
    }

    let client = reqwest::Client::builder()
        .timeout(Duration::from_secs(config.timeout))
        .build()?;

    let response = client
        .post(url)
        .header("x-api-key", &config.anthropic_api_key)
        .header("anthropic-version", "2023-06-01")
        .json(&body)
        .send()
        .await?;

    if !response.status().is_success() {
        let status = response.status();
        let body_text = response
            .text()
            .await
            .unwrap_or_else(|_| "could not read response body".to_string());
        return Err(anyhow!(
            "anthropic api error: status: {}\nbody: {}",
            status,
            body_text
        ));
    }

    let response_body: Value = response.json().await?;
    writeln!(
        log_file,
        "--- raw api response ---\n{}\n",
        serde_json::to_string_pretty(&response_body)?
    )?;

    let full_text = response_body["content"]
        .as_array()
        .unwrap_or(&vec![])
        .iter()
        .filter_map(|block| block["text"].as_str())
        .collect::<String>();

    let usage = ApiUsage {
        input_tokens: response_body["usage"]["input_tokens"]
            .as_u64()
            .unwrap_or(0) as u32,
        output_tokens: response_body["usage"]["output_tokens"]
            .as_u64()
            .unwrap_or(0) as u32,
        thinking_tokens: 0,
    };

    Ok((full_text, usage))
}

// a single comment per function describing its purpose is adequate.
/// handles the API call to openai-compatible endpoints.
async fn call_openai_api(
    config: &Config,
    prompt: &str,
    log_file: &mut File,
) -> Result<(String, ApiUsage)> {
    let url = format!("{}/chat/completions", config.openai_api_base);

    let body = serde_json::json!({
        "model": config.model,
        "messages": [{ "role": "user", "content": prompt }],
        "seed": config.seed,
        "max_tokens": get_max_tokens(config, log_file)?,
        "stream": false,
    });

    let client = reqwest::Client::builder()
        .timeout(Duration::from_secs(config.timeout))
        .build()?;

    let response = client
        .post(&url)
        .bearer_auth(&config.openai_api_key)
        .json(&body)
        .send()
        .await?;

    if !response.status().is_success() {
        let status = response.status();
        let body_text = response
            .text()
            .await
            .unwrap_or_else(|_| "could not read response body".to_string());
        return Err(anyhow!(
            "openai api error: status: {}\nbody: {}",
            status,
            body_text
        ));
    }

    let response_body: Value = response.json().await?;
    writeln!(
        log_file,
        "--- raw api response ---\n{}\n",
        serde_json::to_string_pretty(&response_body)?
    )?;

    let full_text = response_body["choices"][0]["message"]["content"]
        .as_str()
        .unwrap_or_default()
        .to_string();

    let input_tokens = response_body["usage"]["prompt_tokens"]
        .as_u64()
        .unwrap_or(0) as u32;
    let output_tokens = response_body["usage"]["completion_tokens"]
        .as_u64()
        .unwrap_or(0) as u32;
    let total_tokens = response_body["usage"]["total_tokens"]
        .as_u64()
        .unwrap_or(0) as u32;

    let usage = ApiUsage {
        input_tokens,
        output_tokens,
        // ENGLISH DESCRIPTION: some providers don't supply thinking token counts. in that case, they will be calculated by subtracting the known input/output token usage from the total token usage to calculate the thinking tokens.
        thinking_tokens: total_tokens
            .saturating_sub(input_tokens)
            .saturating_sub(output_tokens),
    };

    Ok((full_text, usage))
}

// a single comment per function describing its purpose is adequate.
/// calculates the effective max_tokens value, respecting both user settings and model limits.
// ENGLISH DESCRIPTION: token limits are enforced for all models. the `MAX_TOKENS` setting (if explicitly set) can be used to override the per-model `max_tokens`
fn get_max_tokens(config: &Config, log_file: &mut File) -> Result<u32> {
    let pricing_str = load_resource(config, "pricing.json")?;
    let pricing_data: PricingData = serde_json::from_str(&pricing_str)?;
    let model_key = format!("{}/{}", config.provider, config.model);

    let model_limit = pricing_data
        .get(&model_key)
        .and_then(|p| p.max_tokens)
        .unwrap_or(8192);

    match config.max_tokens {
        Some(user_max) => {
            // ENGLISH DESCRIPTION: the setting cannot be made higher than the pricing limit.
            if user_max > model_limit {
                writeln!(log_file, "warning: user-defined MAX_TOKENS ({user_max}) exceeds model limit ({model_limit}). using model limit.")?;
                Ok(model_limit)
            } else {
                Ok(user_max)
            }
        }
        None => Ok(model_limit),
    }
}

// a single comment per function describing its purpose is adequate.
/// removes markdown code fences from the start and end of a string.
// ENGLISH DESCRIPTION: markdown code fences will ONLY be removed from the very first and very last line of the LLM output!
fn strip_markdown_fences(text: &str) -> String {
    lazy_static::lazy_static! {
        static ref FENCE_RE: Regex = Regex::new(r"(?s)^\s*```[^\n]*\n(.*)\n```\s*$").unwrap();
    }
    if let Some(captures) = FENCE_RE.captures(text) {
        return captures.get(1).map_or(text, |m| m.as_str()).to_string();
    }
    text.to_string()
}

// a single comment per function describing its purpose is adequate.
/// calculates and prints the estimated cost of the API call.
// ENGLISH DESCRIPTION: pricing data for language model API calls is shown after the call completes based on the pricing data in `./res/pricing.json`.
fn print_api_cost_summary(
    config: &Config,
    usage: &ApiUsage,
    log_file: &mut File,
) -> Result<()> {
    let pricing_str = load_resource(config, "pricing.json")?;
    let pricing_data: PricingData = serde_json::from_str(&pricing_str)?;
    let model_key = format!("{}/{}", config.provider, config.model);

    let model_pricing = match pricing_data.get(&model_key) {
        Some(p) => p,
        None => {
            writeln!(
                log_file,
                "warning: no pricing data for model '{}', cannot calculate cost.",
                model_key
            )?;
            return Ok(());
        }
    };

    let input_cost = model_pricing.input_cost_per_token.unwrap_or(0.0) * usage.input_tokens as f64;
    let output_cost =
        model_pricing.output_cost_per_token.unwrap_or(0.0) * usage.output_tokens as f64;
    // ENGLISH DESCRIPTION: if cost per thinking token is not known, they are priced as output tokens.
    let thinking_cost_per_token = model_pricing
        .thinking_cost_per_token
        .or(model_pricing.output_cost_per_token)
        .unwrap_or(0.0);
    let thinking_cost = thinking_cost_per_token * usage.thinking_tokens as f64;

    let total_cost = input_cost + output_cost + thinking_cost;

    // ENGLISH DESCRIPTION: example usage
    // ENGLISH DESCRIPTION: costs accounted by input, output, and thinking costs.
    println!("\n--- api cost summary ---");
    println!(
        "tokens: {} input, {} output, {} thinking",
        usage.input_tokens, usage.output_tokens, usage.thinking_tokens
    );
    println!("estimated cost:");
    println!("  - input   : ${input_cost:.6}");
    println!("  - output  : ${output_cost:.6}");
    println!("  - thinking: ${thinking_cost:.6}");
    println!("total: ${total_cost:.6}");

    Ok(())
}
