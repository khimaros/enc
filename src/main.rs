// NOTICE: This file was automatically generated by https://github.com/khimaros/enc using the following invocation: ./enc src/enc.en -o src/main.rs --context-files ./.enc.env.example:./res/pricing.json:./Cargo.toml --grounded-mode true
use anyhow::{anyhow, Context, Result};
use chrono::Local;
use clap::Parser;
use rand::Rng;
use regex::Regex;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::collections::HashMap;
use std::fs::{self, File};
use std::io::Write;
use std::path::Path;
use std::time::Duration;

// ENGLISH DESCRIPTION: the default value for all settings are commented out in `./.enc.env.example`.
// ENGLISH DESCRIPTION: EVERY configuration setting is also available as command line flag.
// ENGLISH DESCRIPTION: when using a flag parsing library, all short flags are explicitly definied to avoid conflicts with automatic naming.
#[derive(Parser, Debug, Clone, Serialize)]
#[clap(author, version, about = "Transpiles english to any programming language.")]
struct Config {
    /// English language file to transpile (e.g. "hello.en")
    input_file: Option<String>,

    /// Output file path (e.g. "hello.rs")
    #[clap(short = 'o', long)]
    output_file: Option<String>,

    /// Show the consolidated configuration and exit
    #[clap(long)]
    show_config: bool,

    /// Language model API provider: "google", "anthropic", or "openai"
    #[clap(long, env = "PROVIDER", default_value = "google")]
    provider: String,

    /// Model to use for inference
    #[clap(long, env = "MODEL", default_value = "gemini-2.5-pro")]
    model: String,

    /// API key for Google Gemini
    #[clap(long, env = "GEMINI_API_KEY")]
    gemini_api_key: Option<String>,

    /// API key for OpenAI
    #[clap(long, env = "OPENAI_API_KEY")]
    openai_api_key: Option<String>,

    /// Base URL for OpenAI-compatible API
    #[clap(long, env = "OPENAI_API_BASE")]
    openai_api_base: Option<String>,

    /// API key for Anthropic Claude
    #[clap(long, env = "ANTHROPIC_API_KEY")]
    anthropic_api_key: Option<String>,

    /// Maximum number of tokens for the LLM to generate
    #[clap(long, env = "MAX_TOKENS")]
    max_tokens: Option<u32>,

    /// Maximum tokens for deep thought
    #[clap(long, env = "THINKING_BUDGET")]
    thinking_budget: Option<u32>,

    /// Seed for deterministic output (empty means random)
    #[clap(long, env = "SEED")]
    seed: Option<u64>,

    /// Grounded mode adds the existing code (if present) to context files
    #[clap(long, env = "GROUNDED_MODE")]
    grounded_mode: bool,

    /// Path to coding conventions and style guide
    #[clap(long, env = "HACKING_CONVENTIONS", default_value = "./HACKING.md")]
    hacking_conventions_path: String,

    /// Colon-separated list of additional file paths to include as context
    #[clap(long, env = "CONTEXT_FILES", value_delimiter = ':', num_args = 0..)]
    context_files: Vec<String>,

    /// Path to store API logs
    #[clap(long, env = "LOGS_PATH", default_value = "./log/")]
    logs_path: String,

    /// Path to the code generation prompt template
    #[clap(long, env = "PROMPT_TEMPLATE_PATH", default_value = "./res/prompt.tmpl")]
    prompt_template_path: String,

    /// Path to the pricing data json file
    #[clap(long, env = "PRICING_DATA_PATH", default_value = "./res/pricing.json")]
    pricing_data_path: String,
}

impl Config {
    // ENGLISH DESCRIPTION: the "redacted configuration" is what results from filtering the consolidated configuration.
    // ENGLISH DESCRIPTION: API keys are redacted from the log output.
    fn redact(&self) -> Self {
        let mut redacted = self.clone();
        if redacted.gemini_api_key.is_some() {
            redacted.gemini_api_key = Some("[REDACTED]".to_string());
        }
        if redacted.openai_api_key.is_some() {
            redacted.openai_api_key = Some("[REDACTED]".to_string());
        }
        if redacted.anthropic_api_key.is_some() {
            redacted.anthropic_api_key = Some("[REDACTED]".to_string());
        }
        redacted
    }
}

#[derive(Debug, Default)]
struct TokenUsage {
    input: u32,
    output: u32,
    thinking: u32,
}

// ENGLISH DESCRIPTION: all referenced files under `./res/` (such as `./res/pricing.json` and `./res/prompt.tmpl`) are considered resource files.
// ENGLISH DESCRIPTION: enc searches for resource files in a specific order:
// ENGLISH DESCRIPTION: 1. relative to the current working directory
// ENGLISH DESCRIPTION: 2. relative to `${XDG_DATA_HOME}/enc/`.
// ENGLISH DESCRIPTION: 3. (for compiled languages like rust) all resource files will also be compiled into the binary using macros and read from memory.
fn load_resource(path_str: &str) -> Result<String> {
    let path = Path::new(path_str);
    if path.exists() {
        return fs::read_to_string(path).with_context(|| format!("Failed to read resource from CWD: {}", path_str));
    }

    if let Some(data_dir) = dirs::data_dir() {
        let xdg_path = data_dir.join("enc").join(path);
        if xdg_path.exists() {
            return fs::read_to_string(&xdg_path)
                .with_context(|| format!("Failed to read resource from XDG path: {}", xdg_path.display()));
        }
    }

    match path_str {
        "./res/prompt.tmpl" => Ok(include_str!("../res/prompt.tmpl").to_string()),
        "./res/pricing.json" => Ok(include_str!("../res/pricing.json").to_string()),
        _ => Err(anyhow!("Resource file not found: {}", path_str)),
    }
}

// ENGLISH DESCRIPTION: the target language is automatically determined based on the file extension of the output file.
// ENGLISH DESCRIPTION: if the target language is not recognized by the file extension, the file extension or the file base name are passed to the LLM at inference time.
fn get_target_language(output_path: &str) -> String {
    let path = Path::new(output_path);
    let extension = path.extension().and_then(|s| s.to_str()).unwrap_or("");
    match extension {
        "rs" => "rust", "c" => "c", "h" => "c",
        "cpp" | "cxx" | "hpp" => "c++",
        "py" => "python", "js" => "javascript", "ts" => "typescript",
        "go" => "go", "java" => "java", "kt" => "kotlin",
        "swift" => "swift", "rb" => "ruby", "php" => "php",
        "sh" => "shell", "html" => "html", "css" => "css",
        "json" => "json", "xml" => "xml", "svg" => "svg",
        "" => return path.file_name().and_then(|s| s.to_str()).unwrap_or("unknown").to_string(),
        _ => return extension.to_string(),
    }.to_string()
}

// ENGLISH DESCRIPTION: the prompt template is loaded from the file specified by the PROMPT_TEMPLATE_PATH setting.
// ENGLISH DESCRIPTION: template expansion is done ONCE as a SINGLE PASS to ensure that template formatting in the referended files is NOT expanded recursively.
fn expand_prompt_template(config: &Config, template: &str) -> Result<String> {
    let mut prompt = template.to_string();

    // ENGLISH DESCRIPTION: `generation_command`: the command line args enc was called with
    let mut generation_command = std::env::args().collect::<Vec<_>>().join(" ");
    // ENGLISH DESCRIPTION: if the target format is a derivative of XML (eg. SVG), enc will strip double-hyphens (`--`) from `generation_command` before expanding that variable in the prompt.
    let target_lang = get_target_language(config.output_file.as_ref().unwrap());
    if ["xml", "svg", "html"].contains(&target_lang.as_str()) {
        generation_command = generation_command.replace("--", "-");
    }
    prompt = prompt.replace("{{generation_command}}", &generation_command);

    // ENGLISH DESCRIPTION: `generation_config`: the consolidated configuration, with API keys redacted
    let redacted_config_json = serde_json::to_string_pretty(&config.redact())?;
    prompt = prompt.replace("{{generation_config}}", &redacted_config_json);

    // ENGLISH DESCRIPTION: `target_language`: the target programming language.
    prompt = prompt.replace("{{target_language}}", &target_lang);

    // ENGLISH DESCRIPTION: `output_path`: the output path of the file being generated
    prompt = prompt.replace("{{output_path}}", config.output_file.as_ref().unwrap());

    // ENGLISH DESCRIPTION: `english_content`: the exact content of the provided `.en` input file
    let english_content = fs::read_to_string(config.input_file.as_ref().unwrap())?;
    prompt = prompt.replace("{{english_content}}", &english_content);

    // ENGLISH DESCRIPTION: `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
    // ENGLISH DESCRIPTION: if the HACKING_CONVENTIONS source file is not present, that section can be skipped.
    let hacking_conventions = fs::read_to_string(&config.hacking_conventions_path).unwrap_or_default();
    prompt = prompt.replace("{{hacking_conventions}}", &hacking_conventions);

    // ENGLISH DESCRIPTION: `context_files`: the exact, unmodified paths and contents of the files configured in CONTEXT_FILES.
    // ENGLISH DESCRIPTION: for each file, include a markdown header with the relative path to that file, followed by the full contents of the file inside a markdown fenced code block.
    // ENGLISH DESCRIPTION: in particular, file paths SHOULDN'T have "./" prepended to them automatically.
    let mut context_files_content = String::new();
    for path_str in &config.context_files {
        match fs::read_to_string(path_str) {
            Ok(content) => {
                context_files_content.push_str(&format!("### {}\n```\n{}\n```\n\n", path_str, content));
            }
            Err(e) => eprintln!("Warning: could not read context file {}: {}", path_str, e),
        }
    }
    prompt = prompt.replace("{{context_files}}", &context_files_content);

    Ok(prompt)
}

// ENGLISH DESCRIPTION: enc can make use of the Google Generative AI API backend.
fn call_google_api(config: &Config, prompt: &str, client: &reqwest::blocking::Client) -> Result<(String, TokenUsage)> {
    // ENGLISH DESCRIPTION: the API key of the SELECTED provider is the ONLY one that is required at runtime.
    let api_key = config.gemini_api_key.as_ref().ok_or_else(|| anyhow!("GEMINI_API_KEY is required for google provider"))?;
    let url = format!("https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}", config.model, api_key);

    // ENGLISH DESCRIPTION: with the "google" provider, the seed parameter ISN'T supported. instead, temperature is set to 0.
    let mut generation_config = serde_json::json!({ "temperature": 0.0 });
    if let Some(max_tokens) = config.max_tokens {
        generation_config["maxOutputTokens"] = max_tokens.into();
    }

    let body = serde_json::json!({
        "contents": [{ "parts": [{ "text": prompt }] }],
        "generationConfig": generation_config,
    });

    let res = client.post(&url).json(&body).send()?.error_for_status()?;
    let res_json: Value = res.json()?;

    let text = res_json["candidates"][0]["content"]["parts"][0]["text"].as_str().unwrap_or("").to_string();
    
    let mut usage = TokenUsage::default();
    if let Some(usage_meta) = res_json.get("usageMetadata") {
        usage.input = usage_meta["promptTokenCount"].as_u64().unwrap_or(0) as u32;
        usage.output = usage_meta["candidatesTokenCount"].as_u64().unwrap_or(0) as u32;
        let total = usage_meta["totalTokenCount"].as_u64().unwrap_or(0) as u32;
        // ENGLISH DESCRIPTION: some providers don't supply thinking token counts. in that case, they will be calculated by subtracting the known input/output token usage from the total token usage to calculate the thinking tokens.
        if total > usage.input + usage.output {
            usage.thinking = total - (usage.input + usage.output);
        }
    }
    Ok((text, usage))
}

// ENGLISH DESCRIPTION: enc can make use of any OpenAI compatible API backend.
fn call_openai_api(config: &Config, prompt: &str, client: &reqwest::blocking::Client) -> Result<(String, TokenUsage)> {
    let api_key = config.openai_api_key.as_ref().ok_or_else(|| anyhow!("OPENAI_API_KEY is required for openai provider"))?;
    let base_url = config.openai_api_base.as_deref().unwrap_or("https://api.openai.com/v1");
    let url = format!("{}/chat/completions", base_url);

    let mut body = serde_json::json!({
        "model": &config.model,
        "messages": [{"role": "user", "content": prompt}],
    });
    if let Some(seed) = config.seed {
        body["seed"] = seed.into();
    }
    if let Some(max_tokens) = config.max_tokens {
        body["max_tokens"] = max_tokens.into();
    }

    let res = client.post(&url).bearer_auth(api_key).json(&body).send()?.error_for_status()?;
    let res_json: Value = res.json()?;

    let text = res_json["choices"][0]["message"]["content"].as_str().unwrap_or("").to_string();
    
    let mut usage = TokenUsage::default();
    if let Some(usage_meta) = res_json.get("usage") {
        usage.input = usage_meta["prompt_tokens"].as_u64().unwrap_or(0) as u32;
        usage.output = usage_meta["completion_tokens"].as_u64().unwrap_or(0) as u32;
        let total = usage_meta["total_tokens"].as_u64().unwrap_or(0) as u32;
        if total > usage.input + usage.output {
            usage.thinking = total - (usage.input + usage.output);
        }
    }
    Ok((text, usage))
}

// ENGLISH DESCRIPTION: enc can make use of Anthropic Claude API backend.
fn call_anthropic_api(config: &Config, prompt: &str, client: &reqwest::blocking::Client) -> Result<(String, TokenUsage)> {
    let api_key = config.anthropic_api_key.as_ref().ok_or_else(|| anyhow!("ANTHROPIC_API_KEY is required for anthropic provider"))?;
    let url = "https://api.anthropic.com/v1/messages";

    let mut body = serde_json::json!({
        "model": &config.model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": config.max_tokens.unwrap_or(4096), // Anthropic requires max_tokens
    });
    if let Some(seed) = config.seed {
        body["seed"] = seed.into();
    }

    let res = client.post(url)
        .header("x-api-key", api_key)
        .header("anthropic-version", "2023-06-01")
        .json(&body).send()?.error_for_status()?;
    let res_json: Value = res.json()?;

    let text = res_json["content"][0]["text"].as_str().unwrap_or("").to_string();
    
    let mut usage = TokenUsage::default();
    if let Some(usage_meta) = res_json.get("usage") {
        usage.input = usage_meta["input_tokens"].as_u64().unwrap_or(0) as u32;
        usage.output = usage_meta["output_tokens"].as_u64().unwrap_or(0) as u32;
    }
    Ok((text, usage))
}

fn main() -> Result<()> {
    // ENGLISH DESCRIPTION: enc uses a layered configuration. settings are loaded in the following order:
    // ENGLISH DESCRIPTION: 1. home config: ~/.enc.env
    // ENGLISH DESCRIPTION: 2. working dir config: ./.enc.env
    // ENGLISH DESCRIPTION: 3. command line flags
    // ENGLISH DESCRIPTION: when using an external dotenv library, all env file paths are explicitly defined to ensure the correct path is used.
    if let Some(home_path) = dirs::home_dir().map(|p| p.join(".enc.env")) {
        dotenvy::from_path(home_path).ok();
    }
    dotenvy::from_path(".enc.env").ok();

    let mut config = Config::parse();

    // ENGLISH DESCRIPTION: when the `--show-config` flag is provided, enc dumps the redacted config and then exits immediately. all other flags and settings are OPTIONAL in this case.
    if config.show_config {
        println!("{}", serde_json::to_string_pretty(&config.redact())?);
        return Ok(());
    }

    // ENGLISH DESCRIPTION: when using a flag parsing library... all flags are defined as optional and enforced procedurally at the point where the configuration values are consumed.
    let input_file = config.input_file.as_ref().ok_or_else(|| anyhow!("Missing required argument: input_file"))?;
    let output_file = config.output_file.as_ref().ok_or_else(|| anyhow!("Missing required argument: --output-file"))?;

    // ENGLISH DESCRIPTION: grounded mode adds the existing code (if present) to context files
    if config.grounded_mode && Path::new(output_file).exists() {
        if !config.context_files.contains(output_file) {
            config.context_files.push(output_file.clone());
        }
    }

    // ENGLISH DESCRIPTION: each invocation of enc creates a new log file using a timestamp as the file name.
    let timestamp = Local::now().format("%Y%m%d_%H%M%S").to_string();
    fs::create_dir_all(&config.logs_path).with_context(|| format!("Failed to create log directory: {}", &config.logs_path))?;
    let log_path = Path::new(&config.logs_path).join(format!("{}.log", timestamp));
    // ENGLISH DESCRIPTION: the full path to the log file is written to the console when the program starts.
    println!("debug log path: {}", log_path.display());
    let mut log_file = File::create(&log_path).with_context(|| format!("Failed to create log file: {}", log_path.display()))?;

    // ENGLISH DESCRIPTION: the consolidated configuration is logged at the start of each invocation.
    // ENGLISH DESCRIPTION: API keys are redacted from the log output.
    writeln!(log_file, "--- Consolidated Configuration ---\n{}\n--- End Configuration ---\n", serde_json::to_string_pretty(&config.redact())?)?;

    println!("transpiling '{}' to '{}' ({})", input_file, output_file, get_target_language(output_file));

    // ENGLISH DESCRIPTION: the user can provide a seed to request deterministic outputs. if no seed is provided, a random seed is chosen at startup time.
    if config.seed.is_none() {
        config.seed = Some(rand::thread_rng().gen());
    }

    // ENGLISH DESCRIPTION: the prompt template is loaded from the file specified by the PROMPT_TEMPLATE_PATH setting.
    let template_content = load_resource(&config.prompt_template_path)?;
    let prompt = expand_prompt_template(&config, &template_content)?;

    // ENGLISH DESCRIPTION: the template expanded prompt is logged.
    writeln!(log_file, "--- Expanded Prompt ---\n{}\n--- End Prompt ---\n", prompt)?;

    // ENGLISH DESCRIPTION: enc uses explicit timeouts for all API calls (default 900 seconds).
    let client = reqwest::blocking::Client::builder()
        .timeout(Duration::from_secs(900))
        .build()?;

    let (llm_text, usage) = match config.provider.as_str() {
        "google" => call_google_api(&config, &prompt, &client),
        "openai" => call_openai_api(&config, &prompt, &client),
        "anthropic" => call_anthropic_api(&config, &prompt, &client),
        p => Err(anyhow!("Unsupported provider: {}", p)),
    }?;

    // ENGLISH DESCRIPTION: text content returned by the LLM is logged.
    writeln!(log_file, "--- LLM Output ---\n{}\n--- End LLM Output ---\n", llm_text)?;

    // ENGLISH DESCRIPTION: if the language model returns markdown fencing around the code, it will be removed before being written to the output file.
    // ENGLISH DESCRIPTION: markdown code fences will ONLY be removed from the very first and very last line of the LLM output!
    let re_start = Regex::new(r"^\s*```[^\n]*\n").unwrap();
    let re_end = Regex::new(r"\n```\s*$").unwrap();
    let code = re_start.replace(&llm_text, "");
    let code = re_end.replace(&code, "");

    fs::write(output_file, code.as_bytes())?;
    println!("successfully transpiled '{}'", output_file);

    // ENGLISH DESCRIPTION: pricing data for language model API calls is shown after the call completes based on the pricing data in `./res/pricing.json`.
    // ENGLISH DESCRIPTION: language model prices... will be loaded from `./res/pricing.json`
    let pricing_data_str = load_resource(&config.pricing_data_path)?;
    let pricing_map: HashMap<String, Value> = serde_json::from_str(&pricing_data_str)?;
    let model_key = format!("{}/{}", config.provider, config.model);

    if let Some(pricing) = pricing_map.get(&model_key) {
        let input_cost_per_token = pricing["input_cost_per_token"].as_f64().unwrap_or(0.0);
        let output_cost_per_token = pricing["output_cost_per_token"].as_f64().unwrap_or(0.0);
        
        let input_cost = usage.input as f64 * input_cost_per_token;
        let output_cost = usage.output as f64 * output_cost_per_token;
        // ENGLISH DESCRIPTION: if cost per thinking token is not known, they are priced as output tokens.
        let thinking_cost = usage.thinking as f64 * output_cost_per_token;
        let total_cost = input_cost + output_cost + thinking_cost;

        println!("\n--- api cost summary ---");
        println!("provider: {}, model: {}", config.provider, config.model);
        // ENGLISH DESCRIPTION: costs accounted by input, output, and thinking costs.
        println!("tokens: {} input, {} output, {} thinking", usage.input, usage.output, usage.thinking);
        println!("estimated cost:");
        println!("  - input   : ${:.6}", input_cost);
        println!("  - output  : ${:.6}", output_cost);
        println!("  - thinking: ${:.6}", thinking_cost);
        println!("total: ${:.6}", total_cost);
    } else {
        eprintln!("\nWarning: No pricing data found for model '{}'", model_key);
    }

    Ok(())
}