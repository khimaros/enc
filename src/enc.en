# enc

enc is a program that can be used to transpile any english language file (eg. "hello.en") into any other programming language (eg. "hello.c")

## example usage

```console
$ enc ./examples/hello.en -o ./examples/hello.rs

debug log path: ./log/20250624_173200.log
transpiling './examples/hello.en' to './examples/hello.rs' (rust)
successfully transpiled './examples/hello.rs'

--- api cost summary ---
provider: google, model: gemini-2.5-pro
tokens: 422 input, 139 output, 1095 thinking
estimated cost:
  - input   : $0.000590
  - output  : $0.001780
  - thinking: $0.010950
total: $0.013320
```

## providers

enc can make use of any OpenAI compatible API backend. supported models include: gpt-4o-mini, gpt-4.5-preview

enc can make use of the Google Generative AI API backend. supported models include: gemini-2.5-pro, gemini-2.5-pro-preview-06-05, gemini-2.5-flash.

enc can make use of Anthropic Claude API backend. supported models include: claude-3-7-sonnet-20250219.

enc DOESN'T support any form of tool calling.

enc uses explicit timeouts for all API calls (default 900 seconds).

the API key of the SELECTED provider is the ONLY one that is required at runtime.

## configuration

the default value for all settings are commented out in `./.enc.env.example`.

enc uses a layered configuration. settings are loaded in the following order:

1. home config: ~/.enc.env
2. working dir config: ./.enc.env
3. command line flags

subsequent configuration sources override earlier ones.

the "consolidated configuration" is what results from processing all layered configuration sources. the "redacted configuration" is what results from filtering the consolidated configuration.

EVERY configuration setting is also available as command line flag.

when the `--show-config` flag is provided, enc dumps the redacted config and then exits immediately. all other flags and settings are OPTIONAL in this case.

## logging

each invocation of enc creates a new log file using a timestamp as the file name.

the full path to the log file is written to the console when the program starts.

detailed logging goes ONLY to the log file, not to the console.

API keys are redacted from the log output.

the consolidated configuration is logged at the start of each invocation.

the template expanded prompt is logged.

text content returned by the LLM is logged.

## prompt template

there ISN'T any prompt template embedded in the code for enc.

the prompt template is loaded from the file specified by the PROMPT_TEMPLATE_PATH setting.

template variables are written between double curly braces as in `{{some_variable}}`.

the following keys will be substituted before sending to the LLM provider:

- `generation_command`: the command line args enc was called with
- `generation_config`: the consolidated configuration, with API keys redacted
- `target_language`: the target programming language. if the language is unknown, the file extension or filename are used
- `output_path`: the output path of the file being generated
- `english_content`: the exact content of the provided `.en` input file
- `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
- `context_files`: the exact, unmodified paths and contents of the files configured in CONTEXT_FILES. for each file, include a markdown header with the relative path to that file, followed by the full contents of the file inside a markdown fenced code block. in particular, file paths SHOULDN'T have "./" prepended to them automatically.

template expansion is done ONCE as a SINGLE PASS to ensure that template formatting in the referended files is NOT expanded recursively.

if the target format is a derivative of XML (eg. SVG), enc will strip double-hyphens (`--`) from `generation_command` before expanding that variable in the prompt.

if the HACKING_CONVENTIONS source file is not present, that section can be skipped.

## resources

all referenced files under `./res/` (such as `./res/pricing.json` and `./res/prompt.tmpl`) are considered resource files.

enc searches for resource files in a specific order:

1. relative to the current working directory
2. relative to `${XDG_DATA_HOME}/enc/`. for example, `./res/prompt.json` then `${XDG_DATA_HOME}/enc/res/prompt.json`
3. (for compiled languages like rust) all resource files will also be compiled into the binary using macros and read from memory. this will be the final fallback source.

## determinism

the user can provide a seed to request deterministic outputs. if no seed is provided, a random seed is chosen at startup time.

## provider workarounds

with the "google" provider, the seed parameter ISN'T supported. instead, temperature is set to 0.

## library workarounds

the available libraries are specified in language specific project config in CONTEXT FILES.

when using an external dotenv library, all env file paths are explicitly defined to ensure the correct path is used.

when using a flag parsing library, all short flags are explicitly definied to avoid conflicts with automatic naming. also, all flags are defined as optional and enforced procedurally at the point where the configuration values are consumed.

## token budgets

token limits are enforced for all models. thinking budgets are enforced for all LLMs which support extended thinking or reasoning.

pricing data for language model API calls is shown after the call completes based on the pricing data in `./res/pricing.json`. costs accounted by input, output, and thinking costs.

if cost per thinking token is not known, they are priced as output tokens.

some providers don't supply thinking token counts. in that case, they will be calculated by subtracting the known input/output token usage from the total token usage to calculate the thinking tokens.

## code generation

the target language is automatically determined based on the file extension of the output file.

if the target language is not recognized by the file extension, the file extension or the file base name are passed to the LLM at inference time.

if the language model returns markdown fencing around the code, it will be removed before being written to the output file. markdown code fences will ONLY be removed from the very first and very last line of the LLM output!

## language model prices

model pricing data will be loaded from `./res/pricing.json` in CONTEXT FILES.
