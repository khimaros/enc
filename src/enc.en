# enc

enc is a program that can be used to transpile any english language file (eg. "hello.en") into any other programming language (eg. "hello.c")

## example usage

```console
$ enc ./examples/hello.en -o ./examples/hello.rs

debug log path: ./log/20250624_173200.log
provider: google, model: gemini-2.5-pro
transpiling './examples/hello.en' to './examples/hello.rs' (rust)
successfully transpiled './examples/hello.rs'

--- api cost summary ---
tokens: 422 input, 139 output, 1095 thinking
estimated cost:
  - input   : $0.000590
  - output  : $0.001780
  - thinking: $0.010950
total: $0.013320
```

## providers

enc can make use of any OpenAI compatible API backend via the `/chat/completions` endpoint. supported models include: gpt-4o-mini, gpt-4.5-preview

enc can make use of the Google Generative AI API backend via the `v1beta/models/{model}:generateContent` endpoint. supported models include: gemini-2.5-pro, gemini-2.5-pro-preview-06-05, gemini-2.5-flash.

enc can make use of the Anthropic Claude API backend via the `/v1/messages` endpoint. API calls include the header `anthropic-version: 2023-06-01`. supported models include: claude-sonnet-4-20250514, claude-3-7-sonnet-20250219.

enc DOESN'T support any form of tool calling.

enc uses explicit timeouts (TIMEOUT from settings) for all API calls.

the API key of the SELECTED provider is the ONLY one that is required at runtime.

## configuration

enc uses a layered configuration. settings are loaded in the following order:

1. command line flags
2. working dir config: ./.enc.env
3. home config: ~/.enc.env
4. hardcoded fallbacks in the application based on those in ./.enc.env.example

enc uses the first value it finds for environment variables; the working directory config (`./.enc.env`) overrides the home config (`~/.enc.env`), and command line flags override all environment configurations.

the "consolidated configuration" is what results from processing all layered configuration sources. the "redacted configuration" is what results from filtering the consolidated configuration.

EVERY configuration setting is also available as command line flag.

when the `--show-config` flag is provided, enc FIRST loads all sources of configuration and then writes the redacted config to the console. the program exits immediately after the configuration has been written.

## library workarounds

when using a dotenv library, all env file paths are explicitly defined to ensure the correct load order.

when using a flag parsing library, all short flags are explicitly defined. input and output file arguments are declared as required unless the `--show-config` flag is present.

## logging

each invocation of enc creates a new log file using a timestamp as the file name.

the full path to the log file is written to the console when the program starts.

detailed logging goes ONLY to the log file, not to the console.

API keys are redacted from the log output.

the consolidated configuration is logged at the start of each invocation.

the template expanded prompt is logged.

text content returned by the LLM is logged.

errors always include the file and line number, at least when built or run in debug mode.

## prompt template

the prompt template is loaded from resource `./res/prompt.tmpl`.

template variables are written between double curly braces as in `{{some_variable}}`.

the following keys will be substituted before sending to the LLM provider:

- `generation_command`: the command line args enc was called with
- `generation_config`: the consolidated configuration, with API keys redacted
- `target_language`: the target programming language. if the language is unknown, the file extension or filename are used
- `output_path`: the output path of the file being generated
- `english_content`: the exact content of the provided `.en` input file
- `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
- `context_files`: the exact, unmodified paths and contents of the files configured in CONTEXT_FILES. for each file, include a markdown header with the relative path to that file, followed by the full contents of the file inside a markdown fenced code block. in particular, file paths SHOULDN'T have "./" prepended to them automatically.

template expansion is done ONCE as a SINGLE PASS to ensure that template formatting in the referended files is NOT expanded recursively.

if the target format is a derivative of XML (eg. SVG), enc will strip double-hyphens (`--`) from `generation_command` before expanding that variable in the prompt.

if the HACKING_CONVENTIONS source file is not present, that section of the prompt will be empty, and the program will continue without error.

## resources

enc searches for resource files (any file under `./res/`) in a specific order:

1. each path (colon separated) provided in the `RESOURCES_PATH` setting. `${XDG_DATA_HOME}` will be expanded to the env variable contents. if it is not set, it will fall back to `$HOME/.local/share`.
2. as a final fallback, resource files are embedded directly into the compiled binary and read from memory.

this applies to all files from CONTEXT FILES under ./res/ including:

- ./res/languages.json
- ./res/pricing.json
- ./res/prompt.tmpl

## determinism

the user can provide a seed to request deterministic outputs. if no seed is provided, a random seed is chosen at startup time.

## provider workarounds

with the "google" and "anthropic" providers, the seed parameter ISN'T supported. instead, temperature is set to 0.

the temperature workaround is only used if seed is EXPLICITLY provided by the user in settings or flags. for providers which support seeds, the seed can be generated randomly at invocation time.

## token budgets

token limits are enforced for all models.

if `max_tokens` is not present in the pricing data, a default of 8192 will be assumed.

the `MAX_TOKENS` setting (if explicitly set) can be used to override the per-model `max_tokens` from `./res/pricing.json`. the `MAX_TOKENS` setting is NOT sent to the Google provider API.

the `THINKING_BUDGET` is a configured value that is not currently passed to any provider.

pricing data for language model API calls is shown after the call completes, based on the pricing data in `./res/pricing.json`. costs accounted by input, output, and thinking costs.

if cost per thinking token is not known, they are priced as output tokens.

some providers don't supply thinking token counts in the LLM response. in these cases, thinking token counts will be calculated by subtracting the known input/output token usage from the total token usage.

## code generation

the target language is automatically determined based on the file extension of the output file.

there is a language map of the form `{"rs": "rust"}` in `./res/languages.json`.

if the target language is not recognized by the file extension, the file extension or the file base name are passed to the LLM at inference time.

if the language model returns markdown fencing around the code, it will be removed before being written to the output file. markdown code fences will ONLY be removed from the very first and very last line of the LLM output!

## language model prices

model pricing data will be loaded from `./res/pricing.json` in CONTEXT FILES.

## security

API keys and absolute paths are ALWAYS omitted or redacted from output to the console, to the LLM, or to logs. this requirement covers `--help` and `--show-config` output.
