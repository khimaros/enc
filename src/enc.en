# enc

enc is a program that can be used to transpile any english language file (eg. "hello.en") into any other programming language (eg. "hello.c")

## example usage

```console
$ enc ./examples/hello.en -o ./examples/hello.rs

debug log path: ./log/20250624_173200.log
provider: google, model: gemini-2.5-pro
transpiling './examples/hello.en' to './examples/hello.rs' (rust)
successfully transpiled './examples/hello.rs'

--- api cost summary ---
tokens: 422 input, 139 output, 1095 thinking
estimated cost:
  - input   : $0.000590
  - output  : $0.001780
  - thinking: $0.010950
total: $0.013320
```

## providers

enc can make use of any OpenAI compatible API backend via the `/chat/completions` endpoint. supported models include: gpt-4o-mini, gpt-4.5-preview

when using the official OpenAI API (api.openai.com), the `max_completion_tokens` parameter is used. for all other OpenAI-compatible endpoints (eg. llama.cpp, openrouter), the `max_tokens` parameter is used instead.

enc can make use of the Google Generative AI API backend via the `v1beta/models/{model}:generateContent` endpoint. supported models include: gemini-2.5-pro, gemini-2.5-pro-preview-06-05, gemini-2.5-flash.

enc can make use of the Anthropic Claude API backend via the `/v1/messages` endpoint with streaming support enabled. API calls include the header `anthropic-version: 2023-06-01` and use `"stream": true` for real-time response generation. supported models include: claude-sonnet-4-20250514, claude-3-7-sonnet-20250219, claude-opus-4-1-20250805.

enc DOESN'T support any form of tool calling.

enc uses explicit timeouts (TIMEOUT from settings) for all API calls.

the API key of the SELECTED provider is the ONLY one that is required at runtime.

when an API call fails (eg. due to insufficient credits, rate limiting, or network errors), enc will return an error and NOT overwrite the output file.

if the LLM returns an empty response, enc will refuse to overwrite the output file and exit with an error.

## configuration

enc uses a layered configuration. settings are sourced with the following precedence (from highest to lowest):

1. command line flags
2. environment variables (eg. `MODEL=xx PROVIDER=yy enc ...`)
3. working dir config: ./.enc.env
4. home config: ~/.enc.env
5. hardcoded fallbacks in the application based on those in ./.enc.env.example

configuration from sources with higher precedence override values from sources with lower precedence.

the "consolidated configuration" is what results from processing all layered configuration sources. the "redacted configuration" is what results from filtering API keys from the consolidated configuration.

EVERY configuration setting is also available as command line flag.

## test iteration mode

enc supports iterative refinement through automated testing. when `TEST_COMMAND` is provided, enc will run the specified command after each transpilation.

the test command is executed with a restricted environment to prevent parent process environment variables from leaking into the test subprocess. the only variables that are preserved for the child process are those in the following whitelist:

- PATH
- RUSTUP_HOME
- CARGO_HOME

when test commands are executed, the relevant status line is written to the console:

```
executing test command (`make test`), attempt 1/5
test command failed, see log for details
executing test command (`make test`), attempt 2/5
test command succeeded!
```

if the test command exits with a non-zero status code, enc will:
1. capture the test output (stdout and stderr)
2. read the generated code from the output file
3. pass both the generated code and test failures back to the LLM
4. retry transpilation with this context

this loop continues until either:

- the test command succeeds (exits with code 0), or
- `TEST_ITERATIONS` is reached

on each retry iteration, the LLM receives:

- the previously generated source code
- the test command that was executed
- the exit code from the test
- the complete stdout and stderr output

this allows the LLM to iteratively fix issues based on actual test failures.

if (and only if) API calls are made, the total API cost across all iterations is accumulated and displayed at the end of the invocation.

if tests still fail after reaching `TEST_ITERATIONS`, enc exits with code 1.

if an error occurs during any iteration (eg. API error, network failure), the error is printed to the console along with the accumulated API cost summary before exiting.

## default models

when no model is explicitly specified, enc uses provider-specific defaults:

- google: gemini-2.5-pro
- anthropic: claude-sonnet-4-20250514
- openai: gpt-5-2025-08-07

when the `--show-config` flag is provided, enc FIRST loads all sources of configuration and then writes the redacted config to the console in JSON format. the program exits immediately after the configuration has been written. the config keys should be lexically sorted when output.

## library workarounds

.env files are read manually to extract config values WITHOUT modifying the process environment. this ensures that config values from .env files (like TEST_COMMAND) do NOT become actual environment variables that would be inherited by child processes. .env files are read in precedence order (working dir overrides home dir).

when using a flag parsing library, all short flags are explicitly defined. input and output file arguments are declared as required unless the `--show-config` flag is present.

## logging

each invocation of enc creates a new log file in the directory specified by the LOGS_PATH setting, using a timestamp as the file name in the format `YYYYMMDD_HHMMSS.log`.

the full path to the log file is written to the console when the program starts.

detailed logging goes ONLY to the log file, not to the console.

API keys are redacted from the log output and replaced with `[REDACTED]`.

the redacted and consolidated configuration is logged in JSON format at the start of each invocation.

the template expanded prompt is logged.

text content returned by the LLM is logged. for streaming responses, both the incremental deltas and final accumulated response are logged. thinking deltas and signature deltas are also logged when present.

errors always include the file and line number, at least when built or run in debug mode.

## prompt template

the prompt template is loaded from resource `./res/prompt.tmpl`.

template variables are written between double curly braces as in `{{some_variable}}`.

the following keys will be substituted before sending to the LLM provider:

- `generation_command`: the command line args enc was called with
- `generation_config`: the consolidated configuration, with API keys redacted
- `target_language`: the target programming language. if the language is unknown, the file extension or filename are used
- `output_path`: the output path of the file being generated
- `english_content`: the exact content of the provided `.en` input file
- `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
- `context_files`: the exact, unmodified paths and contents of the files configured in CONTEXT_FILES. for each file, include a markdown header with the relative path to that file, followed by the full contents of the file inside a markdown fenced code block. in particular, file paths SHOULDN'T have "./" prepended to them automatically. if any context file cannot be read, enc will exit with an error.

template expansion is done ONCE as a SINGLE PASS to ensure that template formatting in the referended files is NOT expanded recursively.

if the target format is a derivative of XML (eg. SVG), enc will strip all double-hyphens (`--`) from `generation_command` before expanding that variable in the prompt. for example, `--show-config` would become `show-config`.

if the HACKING_CONVENTIONS source file is not present, that section of the prompt will be empty, and the program will continue without error.

## resources

enc searches for resource files (any file under `./res/`) in a specific order:

1. each path (colon separated) provided in the `RESOURCES_PATH` setting. `${XDG_DATA_HOME}` will be expanded to the env variable contents. if it is not set, it will fall back to `$HOME/.local/share`.
2. ONLY for compiled languages, as a final fallback, compile-time macros are used to embed the actual resource files into the compiled binary. in Rust, this is done using the `include_str!` macro with relative paths to the actual files (eg. `include_str!("../res/prompt.tmpl")`). these embedded resources are loaded from the actual files at compile time, NOT hardcoded as string literals.

this applies to all files under ./res/ including:

- ./res/languages.json
- ./res/pricing.json
- ./res/prompt.tmpl

the embedded fallbacks should reference the actual resource files so that changes to those files are automatically reflected when the program is recompiled.

## determinism

the user can provide a seed to request deterministic outputs. if no seed is provided, a random seed is chosen at startup time. seeds are signed 64-bit integers (i64) to ensure compatibility with OpenAI's API limits.

## provider workarounds

with the "google" and "anthropic" providers, the seed parameter ISN'T supported. instead, temperature is set to 0. the temperature workaround is only used if seed is EXPLICITLY provided by the user in settings or flags. for providers which support seeds, the seed can be generated randomly at invocation time.

with the "anthropic" provider, extended thinking can be enabled by including a `thinking` object in the request when `thinking_budget > 0`:

```json
{
  "thinking": {
    "type": "enabled",
    "budget_tokens": 2048
  }
}
```

with "anthropic" provider, `temperature` may only be set to 1 when thinking is enabled.

if the provider returns markdown backtick fencing around the code, it will be removed before being written to the output file. markdown code fences will ONLY be removed from the very first and very last lines of the LLM output. fences with a language name should also be removed.

it is CRITICAL that long responses are handled correctly. for example, with the "google" provider it is necessary to concatenate the "text" parts in `candidates/content/parts` as in: `{"candidates": [{"content": {"parts": [{ "text": "some " }, {"text": "more" }]}}]}`

a similar pattern is required for the "anthropic" provider which can contain multiple "content" blocks as in: `{"content": [{"type": "text", "text": "some "}, {"type": "text", "text": "more"}]}`

### anthropic streaming implementation

enc uses server-sent events (SSE) streaming for real-time response generation with the anthropic provider. the streaming implementation handles the following event types:

- `message_start`: contains initial message metadata and input token count
- `content_block_start`: signals the start of a new content block (text, tool_use, or thinking)
- `content_block_delta`: contains incremental content updates with the following delta types:
  - `text_delta`: incremental text content that is immediately displayed to the user
  - `thinking_delta`: incremental thinking content that is logged but not displayed
  - `signature_delta`: cryptographic signature for thinking block verification
- `content_block_stop`: signals the end of a content block
- `message_delta`: contains final message metadata and output token counts
- `message_stop`: signals the end of the streaming response
- `ping`: keepalive events that are ignored
- `error`: error events that should be handled gracefully

the streaming parser uses line buffering to handle partial SSE events and accumulates text deltas for the final response. the parser ignores empty lines and lines starting with ':' (SSE comment events). thinking content and signatures are logged for debugging but not included in the final output.

streaming accumulates the complete response internally without displaying partial output to the user, then writes the final result to the output file.

## token budgets

most models have token limits. if available, they are found in `./res/pricing.json`. if `max_tokens` is not present in the pricing data, a default of 8192 will be assumed.

the `MAX_TOKENS` setting (if explicitly set) is used to override the per-model `max_tokens` from `./res/pricing.json`. this value is passed directly to the provider, and may result in an error if it exceeds a model's context window limit.

pricing data for language model API calls is shown after the call completes, based on the pricing data in `./res/pricing.json`. costs are accounted by input, output, and thinking costs.

if token cost data is not available, a warning will be printed and the cost data will be elided. this allows the user to use models which are unknown to enc.

if thinking tokens are 0 or not present in the response, the thinking line is omitted from the cost summary output.

if cost per thinking token is not known, they are priced as output tokens.

some providers don't supply thinking token counts in the LLM response. in these cases, thinking token counts will be calculated using the formula: `thinking_tokens = total_tokens - input_tokens - output_tokens`. for anthropic, thinking tokens are calculated by tokenizing the accumulated thinking content from `thinking_delta` events. if no tokenizer is available, thinking tokens will be reported as 0.

the format of `./res/pricing.json` is as follows:

```json
{
    "anthropic/claude-4-sonnet-20250514": {
        "input_cost_per_token": 3e-06,
        "input_cost_per_token_above_200k_tokens": 6e-06,
        "max_input_tokens": 1000000,
        "max_tokens": 1000000,
        "output_cost_per_token": 1.5e-05,
        "output_cost_per_token_above_200k_tokens": 2.25e-05
    },
    "google/gemini-2.5-pro": {
        "input_cost_per_token": 1.25e-06,
        "input_cost_per_token_above_200k_tokens": 2.5e-06,
        "max_input_tokens": 1048576,
        "max_tokens": 65535,
        "output_cost_per_token": 1e-05,
        "output_cost_per_token_above_200k_tokens": 1.5e-05,
        "source": "https://cloud.google.com/vertex-ai/generative-ai/pricing"
    },
    // ... and so on ...
}
```

any missing fields will be gracefully handled by the parser.

## code generation

the target language name is loaded from `./res/languages.json`. that resource file contains an exhaustive language map of the form `{".rs": "rust", ".py": "python"}`.

the key is the file extension or base file name of the output file. for files without extensions (eg. Makefile), the base name is used as the key.

if the target language is not found in the map, the key used for the lookup (the file extension, or the file base name if no extension is present) is passed to the LLM at inference time.

## security

API keys and absolute paths are ALWAYS omitted or redacted from output to the console, to the LLM, or to logs. this requirement covers `--help` and `--show-config` output.
