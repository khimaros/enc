# enc

enc is a program that can be used to transpile any english language file (eg. "hello.en") into any other programming language (eg. "hello.c")

## example usage

```console
$ enc ./examples/hello.en -o ./examples/hello.c --test-command "make test"

debug log path: ./log/20250624_173200.log
transpiling './examples/hello.en' to './examples/hello.c' (c)
calling api (provider: google, model: gemini-3-flash-preview)...
api call completed with response code 200 after 1.75s
executing test command (`make test`), attempt 1/5
test command failed, see log for details
calling api (provider: google, model: gemini-3-flash-preview)...
api call completed with response code 200 after 3.25s
executing test command (`make test`), attempt 2/5
test command succeeded!
successfully transpiled './examples/hello.c'

--- api request summary ---
tokens: 411 input, 167 output, 1611 thinking
estimated cost:
  - input: $0.000205
  - output: $0.000501
  - thinking: $0.004833
total: $0.005540
elapsed time: 10.06s
```

the command-line interface MUST follow this exact pattern where the input file is a positional argument:

```
enc <INPUT_FILE> -o <OUTPUT_FILE> [OPTIONS]
```

the input file is the first positional argument, NOT a flag. the output file is specified via the `-o` or `--output` flag.

## providers

enc can make use of any OpenAI compatible API backend via the `/chat/completions` endpoint. by default, the OpenAI API base is set to `https://api.openai.com/v1`. supported models include: gpt-4o-mini, gpt-4.5-preview

when using the official OpenAI API (api.openai.com), the `max_completion_tokens` parameter is used. for all other OpenAI-compatible endpoints (eg. llama.cpp, openrouter), the `max_tokens` parameter is used instead.

enc can make use of the Google Generative AI API backend via the `v1beta/models/{model}:generateContent` endpoint. supported models include: gemini-2.5-pro, gemini-2.5-pro-preview-06-05, gemini-2.5-flash.

enc can make use of the Anthropic Claude API backend via the `/v1/messages` endpoint with streaming support enabled. API calls include the header `anthropic-version: 2023-06-01` and use `"stream": true` for real-time response generation. supported models include: claude-sonnet-4-20250514, claude-3-7-sonnet-20250219, claude-opus-4-1-20250805.

enc DOESN'T support any form of tool calling.

enc uses explicit timeouts (TIMEOUT from settings) for all API calls.

the API key of the SELECTED provider is the ONLY one that is required at runtime.

when an API call fails (eg. due to insufficient credits, rate limiting, or network errors), enc will return an error and NOT overwrite the output file.

if the LLM returns an empty response, enc will refuse to overwrite the output file and exit with an error.

## configuration

enc uses a layered configuration. settings are sourced with the following precedence (from highest to lowest):

1. command line flags
2. environment variables (eg. `MODEL=xx PROVIDER=yy enc ...`)
3. working dir config: ./.enc.env
4. home config: ~/.enc.env
5. hardcoded fallbacks in the application based on those in ./.enc.env.example

configuration from sources with higher precedence override values from sources with lower precedence.

the "consolidated configuration" is the FINAL merged result from processing all layered configuration sources (flags > env > .enc.env > ~/.enc.env > hardcoded defaults). this is the configuration that the program actually uses at runtime. the "redacted configuration" is what results from filtering API keys from the consolidated configuration. the --show-config output shows the consolidated configuration with redactions applied.

EVERY configuration setting is also available as command line flag.

## test iteration mode

enc supports iterative refinement through automated testing. when `TEST_COMMAND` is provided, enc will run the specified command after each transpilation.

the test command is executed with a restricted environment to prevent parent process environment variables from leaking into the test subprocess. the only variables that are preserved for the child process are those in the following whitelist:

- PATH
- RUSTUP_HOME
- CARGO_HOME

when test commands are executed, the relevant status line is written to the console:

```
executing test command (`make test`), attempt 1/5
test command failed, see log for details
```

if the test command exits with a non-zero status code, enc will:
1. capture the test output (stdout and stderr)
2. read the generated code from the output file
3. pass both the generated code and test failures back to the LLM
4. retry transpilation with this context

this loop continues until either:

- the test command succeeds (exits with code 0), or
- `TEST_ITERATIONS` is reached (if `TEST_ITERATIONS` is -1, retry forever)

on each retry iteration, the LLM receives:

- the previously generated source code
- the test command that was executed
- the exit code from the test
- the complete stdout and stderr output

this allows the LLM to iteratively fix issues based on actual test failures.

## api reporting

before each API call is made, a status line is written to the console in the format:

```
calling api (provider: <provider>, model: <model>)...
```

after each API call completes, response code and call time are written:

```
api call completed with response code 200 after 1.75s
```

no per-call token or cost summary is displayed. if the transpilation is successful, the 'successfully transpiled' message is printed. at the end of the invocation (after all iterations and test commands complete), a final accumulated summary is displayed showing totals across all API calls with an empty line before it. if 'successfully transpiled' was printed, the empty line should appear between it and the summary.

```
--- api request summary ---
tokens: X input, Y output[, Z thinking]
estimated cost:
  - input: $A.BBBBBB
  - output: $C.DDDDDD
  [- thinking: $E.FFFFFF]
total: $G.HHHHHH
elapsed time: I.JJs
```

the total API cost summary is displayed regardless of whether test commands succeed or fail.

if tests still fail after reaching `TEST_ITERATIONS`, enc exits with code 1 after displaying the total API cost summary.

if an error occurs during any iteration (eg. API error, network failure), the error is printed to the console along with the accumulated API cost summary before exiting.

## default models

when no model is explicitly specified, enc uses provider-specific defaults:

- google: gemini-2.5-pro
- anthropic: claude-sonnet-4-20250514
- openai: gpt-5-2025-08-07

when the `--show-config` flag is provided, enc FIRST loads all sources of configuration and then writes the redacted config to the console in JSON format. the program exits immediately after the configuration has been written. the config keys should be lexically sorted when output.

fields are omitted or redacted from --show-config output based on these rules:

1. numeric fields (max_tokens, seed, thinking_budget): omit if not explicitly set by any configuration source
2. string fields (hacking_conventions, context_files, openai_api_base, test_command): omit ONLY if the consolidated value equals the empty string ("")
3. API key fields: MUST be included with value "[REDACTED]" (never show actual keys, never omit the field)

CRITICAL: for string fields, the decision to include/omit is based SOLELY on whether the final consolidated value is "" (empty string) or non-empty. the source of the value (command line, env var, config file, or hardcoded default) is irrelevant. if the consolidated configuration contains hacking_conventions = "./HACKING.md", it MUST appear in --show-config output, regardless of where that value came from.

hardcoded defaults (from .enc.env.example) ARE part of the consolidated configuration. for example, the hardcoded default for hacking_conventions is "./HACKING.md" (not "") and the default for openai_api_base is "https://api.openai.com/v1". therefore, if these settings are not set in flags, env vars, or .enc.env files, the consolidated values will be those defaults and they MUST appear in --show-config output. the only way for these fields to be omitted from --show-config is if they are explicitly set to "" by a higher-precedence configuration source.

### --show-config examples

example 1: if hacking_conventions is NOT set in flags, env vars, or .enc.env files (uses hardcoded default "./HACKING.md"), the output MUST include:
```json
{
  "hacking_conventions": "./HACKING.md",
  ...
}
```

example 2: if hacking_conventions is explicitly set to "" (empty string) via flag, env var, or .enc.env, the output MUST NOT include that field:
```json
{
  ...
  (no hacking_conventions field)
}
```

example 3: if max_tokens is explicitly set to 8192, include it. if not explicitly set, omit it:
```json
{
  "max_tokens": 8192,
  ...
}
```

## library workarounds

.env files are read manually to extract config values WITHOUT modifying the process environment. this ensures that config values from .env files (like TEST_COMMAND) do NOT become actual environment variables that would be inherited by child processes. .env files are read in precedence order (working dir overrides home dir).

when using a flag parsing library, all short flags are explicitly defined.

the command-line interface follows this pattern: `enc <INPUT_FILE> -o <OUTPUT_FILE> [OPTIONS]`

- INPUT_FILE is a required POSITIONAL argument (the first positional arg, not a flag)
- OUTPUT_FILE is provided via the `-o`/`--output` FLAG
- both are required unless `--show-config` is present

the `--show-config` flag MUST NOT require input_file or output_file arguments.

## logging

each invocation of enc creates a new log file in the directory specified by the LOGS_PATH setting, using a timestamp as the file name in the format `YYYYMMDD_HHMMSS.log`.

the full path to the log file is written to the console when the program starts.

detailed logging goes ONLY to the log file, not to the console.

API keys are redacted from the log output and replaced with `[REDACTED]`.

the redacted and consolidated configuration is logged in JSON format at the start of each invocation.

the template expanded prompt is logged.

text content returned by the LLM is logged. for streaming responses, both the incremental deltas and final accumulated response are logged. thinking deltas and signature deltas are also logged when present.

errors always include the file and line number, at least when built or run in debug mode.

## prompt template

the prompt template is loaded from resource `./res/prompt.tmpl`.

template variables are written between double curly braces as in `{{some_variable}}`.

the following keys will be substituted before sending to the LLM provider:

- `generation_command`: the command line args enc was called with
- `generation_config`: the consolidated configuration, with API keys redacted
- `target_language`: the target programming language. if the language is unknown, the file extension or filename are used
- `output_path`: the output path of the file being generated
- `english_content`: the exact content of the provided `.en` input file
- `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
- `context_files`: the exact, unmodified paths and contents of the files configured in CONTEXT_FILES. for each file, include a markdown header with the relative path to that file, followed by the full contents of the file inside a markdown fenced code block. in particular, file paths SHOULDN'T have "./" prepended to them automatically. if any context file cannot be read, enc will exit with an error.

template expansion is done ONCE as a SINGLE PASS to ensure that template formatting in the referended files is NOT expanded recursively.

if the target format is a derivative of XML (eg. SVG), enc will strip all double-hyphens (`--`) from `generation_command` before expanding that variable in the prompt. for example, `--show-config` would become `show-config`.

if the HACKING_CONVENTIONS source file is not present, that section of the prompt will be empty, and the program will continue without error.

when a configuration value is loaded from ANY source (including .env files and hardcoded defaults), and that value is non-empty for strings or explicitly set for numeric fields, it MUST be included in both the consolidated configuration and the --show-config output. this requirement is tested by the test suite which expects specific fields to appear in --show-config output based on the consolidated configuration values.

## resources

enc searches for resource files (any file under `./res/`) in a specific order:

1. each path (colon separated) provided in the `RESOURCES_PATH` setting. `${XDG_DATA_HOME}` will be expanded to the env variable contents. if it is not set, it will fall back to `$HOME/.local/share`.
2. ONLY for compiled languages, as a final fallback, compile-time macros are used to embed the actual resource files into the compiled binary. in Rust, this is done using the `include_str!` macro with relative paths to the actual files (eg. `include_str!("../res/prompt.tmpl")`). these embedded resources are loaded from the actual files at compile time, NOT hardcoded as string literals.

this applies to all files under ./res/ including:

- ./res/languages.json
- ./res/pricing.json
- ./res/prompt.tmpl

the embedded fallbacks should reference the actual resource files so that changes to those files are automatically reflected when the program is recompiled.

## determinism

the user can provide a seed to request deterministic outputs. if no seed is provided, a random seed is chosen at startup time. seeds are signed 64-bit integers (i64) to ensure compatibility with OpenAI's API limits.

## provider workarounds

with the "google" and "anthropic" providers, the seed parameter ISN'T supported. instead, temperature is set to 0. the temperature workaround is only used if seed is EXPLICITLY provided by the user in settings or flags. for providers which support seeds, the seed can be generated randomly at invocation time.

with the "anthropic" provider, extended thinking can be enabled by including a `thinking` object in the request when `thinking_budget > 0`:

```json
{
  "thinking": {
    "type": "enabled",
    "budget_tokens": 2048
  }
}
```

with "anthropic" provider, `temperature` may only be set to 1 when thinking is enabled. additionally, when thinking is enabled, `max_tokens` MUST be greater than `thinking_budget` or the API will return an error.

if the provider returns markdown backtick fencing around the code, it will be removed before being written to the output file. markdown code fences will ONLY be removed from the very first and very last lines of the LLM output. fences with a language name should also be removed.

it is CRITICAL that long responses are handled correctly. for example, with the "google" provider it is necessary to concatenate the "text" parts in `candidates/content/parts` as in: `{"candidates": [{"content": {"parts": [{ "text": "some " }, {"text": "more" }]}}]}`

a similar pattern is required for the "anthropic" provider which can contain multiple "content" blocks as in: `{"content": [{"type": "text", "text": "some "}, {"type": "text", "text": "more"}]}`

### anthropic streaming implementation

enc uses server-sent events (SSE) streaming for real-time response generation with the anthropic provider. the streaming implementation handles the following event types:

- `message_start`: contains initial message metadata and input token count. the message id is logged.
- `content_block_start`: signals the start of a new content block (text, tool_use, or thinking). the block type is logged.
- `content_block_delta`: contains incremental content updates with the following delta types:
  - `text_delta`: incremental text content that is immediately displayed to the user
  - `thinking_delta`: incremental thinking content that is logged but not displayed
  - `signature_delta`: cryptographic signature for thinking block verification
- `content_block_stop`: signals the end of a content block
- `message_delta`: contains final message metadata and output token counts. the `stop_reason` is logged.
- `message_stop`: signals the end of the streaming response
- `ping`: keepalive events that are ignored
- `error`: error events that cause enc to return an error. the error message is logged.

the streaming implementation logs the HTTP status and the `request-id` header for every request. if the HTTP status is NOT a success (2xx), the full response body is logged and the program returns an error.

the streaming parser uses line buffering to handle partial SSE events and accumulates text deltas for the final response. the parser ignores empty lines and lines starting with ':' (SSE comment events). thinking content and signatures are logged for debugging but not included in the final output.

streaming accumulates the complete response internally without displaying partial output to the user, then writes the final result to the output file.

the final response length is logged. if both the accumulated text and thinking content are empty after the stream completes, a warning is logged.

## token budgets

most models have token limits. if available, they are found in `./res/pricing.json`. if `max_tokens` is not present in the pricing data, a default of 8192 will be assumed.

the `MAX_TOKENS` setting (if explicitly set) is used to override the per-model `max_tokens` from `./res/pricing.json`. this value is passed directly to the provider, and may result in an error if it exceeds a model's context window limit.

pricing data for language model API calls is shown after each individual call completes, based on the pricing data in `./res/pricing.json`. costs are accounted by input, output, and thinking costs. at the end of the invocation, accumulated totals across all API calls are displayed.

the time spent in each LLM API call is tracked and displayed in the per-call summary. the total time across all calls is accumulated and displayed in the final cost summary. call time measures the elapsed time from when the request is sent until the response is fully received. the time is displayed in seconds with two decimal places (e.g., "3.45s").

if token cost data is not available, a warning will be printed and the cost data will be elided. this allows the user to use models which are unknown to enc.

if thinking tokens are 0 or not present in the response, thinking tokens are omitted from BOTH the tokens summary line AND the cost breakdown. for example, if thinking tokens are 0, the output should be "tokens: X input, Y output" (without mentioning thinking), and the cost breakdown should not include a "- thinking: $Z" line.

if cost per thinking token is not known, they are priced as output tokens.

some providers don't supply thinking token counts in the LLM response. in these cases, thinking token counts will be calculated using the formula: `thinking_tokens = total_tokens - input_tokens - output_tokens`.

for the "google" provider, thinking tokens may be included in the response metadata (e.g., in usageMetadata as a separate field). if present, extract them from the response. if total_tokens is provided but thinking_tokens is not, calculate thinking tokens using the formula above.

for the "anthropic" provider with streaming, thinking tokens are NOT included in the usage metadata. if thinking content was received via `thinking_delta` events, the thinking token count should be estimated by counting tokens in the accumulated thinking content. a simple heuristic is to use character count divided by 4 (approximately 4 characters per token for English text). if no thinking content was received, thinking tokens will be reported as 0.

for the "openai" provider and openai-compatible endpoints, thinking tokens are typically included in the response usage object if the model supports extended thinking. check for thinking tokens in the following order:
1. `usage.completion_tokens_details.reasoning_tokens` (newer models like o1, o3-mini)
2. `usage.reasoning_tokens` (direct field)
3. `usage.thinking_tokens` (alternate field name)
4. fallback: calculate using `total_tokens - prompt_tokens - completion_tokens` if total_tokens is provided
if none of these are present or all are 0, report thinking tokens as 0.

the format of `./res/pricing.json` is as follows:

```json
{
    "anthropic/claude-4-sonnet-20250514": {
        "input_cost_per_token": 3e-06,
        "input_cost_per_token_above_200k_tokens": 6e-06,
        "max_input_tokens": 1000000,
        "max_tokens": 1000000,
        "output_cost_per_token": 1.5e-05,
        "output_cost_per_token_above_200k_tokens": 2.25e-05
    },
    "google/gemini-2.5-pro": {
        "input_cost_per_token": 1.25e-06,
        "input_cost_per_token_above_200k_tokens": 2.5e-06,
        "max_input_tokens": 1048576,
        "max_tokens": 65535,
        "output_cost_per_token": 1e-05,
        "output_cost_per_token_above_200k_tokens": 1.5e-05,
        "source": "https://cloud.google.com/vertex-ai/generative-ai/pricing"
    },
    // ... and so on ...
}
```

any missing fields will be gracefully handled by the parser.

## code generation

the target language name is loaded from `./res/languages.json`. that resource file contains an exhaustive language map of the form `{".rs": "rust", ".py": "python"}`.

the key is the file extension or base file name of the output file. for files without extensions (eg. Makefile), the base name is used as the key.

if the target language is not found in the map, the key used for the lookup (the file extension, or the file base name if no extension is present) is used as the language name for BOTH the template variable AND the console output.

the LLM MUST return ONLY valid source code in the target language. the response MUST NOT include:
- explanatory text or commentary before/after the code
- markdown formatting (except code fences on first/last lines only, which will be stripped)
- numbered lists or prose descriptions
- review comments or suggestions

the ENTIRE response should be compilable/executable code in the target language.

## output validation

before writing the generated code to the output file, enc performs the following validations:

- strip ONLY first/last line markdown code fences (```language or ```)
- check that the response is non-empty after stripping fences

if validation fails, treat it as an error and retry if iterations remain.

## security

API keys are ALWAYS redacted to "[REDACTED]" in all output (console, LLM prompts, logs, --show-config). absolute paths are redacted when shown to the LLM or in console help output.
