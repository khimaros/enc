#!/usr/bin/env python3
# NOTICE: This file was automatically generated by https://github.com/khimaros/enc using the following invocation:
# ./enc src/enc.en -o src/enc.py --context-files ./.enc.env.example:./res/pricing.json:./requirements.txt

# For each function or significant code block you generate, include a comment immediately before it, indicating the specific English sentence(s) from the description that prompted its creation.
# Ensure that all dependencies are imported at the top of the file.
import os
import sys
import argparse
import datetime
import json
import logging
import random
import re
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List

# Ensure that all dependencies are imported at the top of the file.
# CONTEXT FILES - ./requirements.txt
try:
    from dotenv import load_dotenv
    import anthropic
    import openai
    import google.generativeai as genai
except ImportError as e:
    print(f"Error: A required dependency is not installed. Please run 'pip install -r requirements.txt'. Details: {e}", file=sys.stderr)
    sys.exit(1)

# EVERY configuration setting is also available as command line flag.
def get_cli_args() -> argparse.Namespace:
    """Parses and returns command line arguments."""
    # when using a flag parsing library, all short flags are explicitly definied to avoid conflicts with automatic naming. also, all flags are defined as optional and enforced procedurally at the point where the configuration values are consumed.
    parser = argparse.ArgumentParser(description="Transpile english language files to any other programming language.", add_help=False)

    # all other flags and settings are OPTIONAL in this case.
    parser.add_argument("input_file", nargs="?", help="The input english language file (e.g., 'hello.en').")
    parser.add_argument("-o", "--output-file", help="The output file path (e.g., 'hello.c').")

    # when the `--show-config` flag is provided, enc dumps the redacted config and then exits immediately.
    parser.add_argument("--show-config", action="store_true", help="Dump the redacted config and exit.")

    # ./.enc.env.example
    parser.add_argument("--provider", help='language model API provider: "google", "anthropic", or "openai"')
    parser.add_argument("--model", help="model to use for inference")
    parser.add_argument("--max-tokens", type=int, help="maximum number of tokens for the LLM to generate")
    parser.add_argument("--thinking-budget", type=int, help="maximum tokens for deep thought")
    parser.add_argument("--grounded-mode", action=argparse.BooleanOptionalAction, help="grounded mode adds the existing code (if present) to context files")
    parser.add_argument("--seed", type=int, help="seed used for inference (empty means random)")
    parser.add_argument("--hacking-conventions", help="coding conventions and style guide")
    parser.add_argument("--context-files", help="additional file paths to include as context (colon separated)")
    parser.add_argument("--gemini-api-key", help="API key for Google Gemini")
    parser.add_argument("--openai-api-key", help="API key for OpenAI")
    parser.add_argument("--anthropic-api-key", help="API key for Anthropic")
    parser.add_argument("--openai-api-base", help="OpenAI API base URL")
    parser.add_argument("--logs-path", help="path to store API logs")
    parser.add_argument("--prompt-template-path", help="path to the code generation prompt template")
    parser.add_argument("--pricing-data-path", help="path to the pricing data json file")
    parser.add_argument("-h", "--help", action="help", help="show this help message and exit")

    return parser.parse_args()

# the default value for all settings are commented out in `./.enc.env.example`.
def get_default_config() -> Dict[str, Any]:
    """Returns the default configuration values."""
    return {
        "PROVIDER": "google",
        "MODEL": "gemini-2.5-pro",
        "MAX_TOKENS": None,
        "THINKING_BUDGET": 2048,
        "GROUNDED_MODE": False,
        "SEED": None,
        "HACKING_CONVENTIONS": "./HACKING.md",
        "CONTEXT_FILES": "",
        "GEMINI_API_KEY": None,
        "OPENAI_API_KEY": None,
        "ANTHROPIC_API_KEY": None,
        "OPENAI_API_BASE": None,
        "LOGS_PATH": "./log/",
        "PROMPT_TEMPLATE_PATH": "./res/prompt.tmpl",
        "PRICING_DATA_PATH": "./res/pricing.json",
    }

# enc uses a layered configuration. settings are loaded in the following order:
# 1. home config: ~/.enc.env
# 2. working dir config: ./.enc.env
# 3. command line flags
# subsequent configuration sources override earlier ones.
def load_configuration(cli_args: argparse.Namespace) -> Dict[str, Any]:
    """Loads configuration from defaults, env files, and CLI arguments."""
    config = get_default_config()

    # when using an external dotenv library, all env file paths are explicitly defined to ensure the correct path is used.
    home_config_path = Path.home() / ".enc.env"
    work_dir_config_path = Path.cwd() / ".enc.env"

    if home_config_path.exists():
        load_dotenv(dotenv_path=home_config_path, override=True)
    if work_dir_config_path.exists():
        load_dotenv(dotenv_path=work_dir_config_path, override=True)

    for key in config:
        env_val = os.getenv(key)
        if env_val is not None:
            # Handle boolean and integer types from env strings
            if isinstance(config[key], bool):
                config[key] = env_val.lower() in ('true', '1', 't', 'y', 'yes')
            elif isinstance(config[key], int) and key != "SEED": # SEED can be None
                try:
                    config[key] = int(env_val)
                except (ValueError, TypeError):
                    pass # Keep default if env var is not a valid int
            else:
                config[key] = env_val

    # Command line flags override everything else
    cli_dict = vars(cli_args)
    for key, value in cli_dict.items():
        if value is not None:
            config_key = key.upper().replace("-", "_")
            if config_key in config:
                config[config_key] = value

    # Assign positional args
    if cli_args.input_file:
        config["INPUT_FILE"] = cli_args.input_file
    if cli_args.output_file:
        config["OUTPUT_FILE"] = cli_args.output_file

    # the API key of the SELECTED provider is the ONLY one that is required at runtime.
    provider = config["PROVIDER"].lower()
    if provider == "google":
        config["API_KEY"] = config["GEMINI_API_KEY"]
    elif provider == "openai":
        config["API_KEY"] = config["OPENAI_API_KEY"]
    elif provider == "anthropic":
        config["API_KEY"] = config["ANTHROPIC_API_KEY"]
    else:
        config["API_KEY"] = None
    
    return config

# the "redacted configuration" is what results from filtering the consolidated configuration.
# API keys are redacted from the log output.
def get_redacted_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """Returns a copy of the configuration with sensitive values redacted."""
    redacted = config.copy()
    for key in redacted:
        if "API_KEY" in key and redacted[key] is not None:
            redacted[key] = f"REDACTED (last 4: {redacted[key][-4:]})"
    return redacted

# each invocation of enc creates a new log file using a timestamp as the file name.
# the full path to the log file is written to the console when the program starts.
# detailed logging goes ONLY to the log file, not to the console.
def setup_logging(logs_path: str) -> str:
    """Configures logging to a timestamped file."""
    log_dir = Path(logs_path)
    log_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = log_dir / f"{timestamp}.log"

    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(levelname)s - %(message)s",
        filename=log_file_path,
        filemode="w",
    )
    # Prevent provider SDKs from logging to stdout/stderr
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("google").setLevel(logging.WARNING)

    return str(log_file_path)

# enc searches for resource files (such as ./res/pricing.json and ./res/prompt.tmpl) relative to the current working directory. if it can't find them there, it will search relative to `${XDG_DATA_HOME}/enc/`.
def find_resource(file_path_str: str) -> Optional[Path]:
    """Searches for a resource file in standard locations."""
    file_path = Path(file_path_str)

    # 1. Relative to current working directory
    if file_path.exists():
        return file_path

    # 2. Relative to XDG_DATA_HOME
    xdg_data_home = os.environ.get("XDG_DATA_HOME", Path.home() / ".local/share")
    xdg_path = Path(xdg_data_home) / "enc" / file_path.name
    if xdg_path.exists():
        return xdg_path
        
    # for example, `./res/prompt.json` then `${XDG_DATA_HOME}/enc/res/prompt.json`
    # The original logic covers `./res/file`, not `res/file`. Let's also check for that structure in XDG.
    if len(file_path.parts) > 1:
        xdg_path_structured = Path(xdg_data_home) / "enc" / file_path
        if xdg_path_structured.exists():
            return xdg_path_structured

    return None

# template expansion is done ONCE as a SINGLE PASS to ensure that template formatting in the referended files is NOT expanded recursively.
def expand_template(template_content: str, substitutions: Dict[str, str]) -> str:
    """Performs a single-pass replacement of variables in the template."""
    # template variables are written between double curly braces as in `{{some_variable}}`.
    for key, value in substitutions.items():
        template_content = template_content.replace(f"{{{{{key}}}}}", value)
    return template_content

# the target language is automatically determined based on the file extension of the output file.
# if the language is not recognized by the file extension, the file extension or the file base name are passed to the LLM at inference time.
def get_target_language(output_path_str: str) -> str:
    """Determines the target programming language from the output file path."""
    path = Path(output_path_str)
    # Common extensions mapping
    lang_map = {
        ".py": "python", ".js": "javascript", ".ts": "typescript", ".java": "java",
        ".c": "c", ".cpp": "c++", ".h": "c", ".hpp": "c++", ".cs": "c#",
        ".go": "go", ".rs": "rust", ".rb": "ruby", ".php": "php", ".swift": "swift",
        ".kt": "kotlin", ".scala": "scala", ".pl": "perl", ".sh": "shell",
        ".html": "html", ".css": "css", ".json": "json", ".xml": "xml",
        ".sql": "sql", ".md": "markdown", ".yml": "yaml", ".yaml": "yaml",
        ".svg": "svg"
    }
    suffix = path.suffix.lower()
    if suffix in lang_map:
        return lang_map[suffix]
    # if the language is unknown, the file extension or filename are used
    return suffix[1:] if suffix else path.name

# with the "google" provider, the seed parameter ISN'T supported. instead, temperature is set to 0.
def call_google(prompt: str, config: Dict[str, Any]) -> Tuple[str, Dict[str, int]]:
    """Calls the Google Generative AI API."""
    genai.configure(api_key=config["API_KEY"])
    # supported models include: gemini-2.5-pro, gemini-2.5-pro-preview-06-05, gemini-2.5-flash.
    model = genai.GenerativeModel(
        config["MODEL"],
        # Determinism workaround for Google
        generation_config=genai.types.GenerationConfig(
            temperature=0.0
        )
    )
    # enc uses explicit timeouts for all API calls (default 900 seconds).
    response = model.generate_content(prompt, request_options={"timeout": 900})

    content = response.text
    
    # some providers don't supply thinking token counts. in that case, they will be calculated by subtracting the known input/output token usage from the total token usage to calculate the thinking tokens.
    usage = response.usage_metadata
    input_tokens = usage.prompt_token_count
    output_tokens = usage.candidates_token_count
    total_tokens = usage.total_token_count
    thinking_tokens = total_tokens - (input_tokens + output_tokens)

    return content, {"input": input_tokens, "output": output_tokens, "thinking": thinking_tokens}

# enc can make use of any OpenAI compatible API backend.
def call_openai(prompt: str, config: Dict[str, Any]) -> Tuple[str, Dict[str, int]]:
    """Calls an OpenAI or compatible API."""
    # supported models include: gpt-4o-mini, gpt-4.5-preview
    client = openai.OpenAI(
        api_key=config["API_KEY"],
        base_url=config.get("OPENAI_API_BASE")
    )
    
    params = {
        "model": config["MODEL"],
        "messages": [{"role": "user", "content": prompt}],
        "timeout": 900, # enc uses explicit timeouts for all API calls
    }
    # the user can provide a seed to request deterministic outputs
    if config.get("SEED") is not None:
        params["seed"] = config["SEED"]
        params["temperature"] = 0.0 # Often used with seed for determinism

    response = client.chat.completions.create(**params)

    content = response.choices[0].message.content or ""
    usage = response.usage
    # OpenAI doesn't report thinking tokens directly.
    return content, {"input": usage.prompt_tokens, "output": usage.completion_tokens, "thinking": 0}

# enc can make use of Anthropic Claude API backend.
def call_anthropic(prompt: str, config: Dict[str, Any]) -> Tuple[str, Dict[str, int]]:
    """Calls the Anthropic Claude API."""
    # supported models include: claude-3-7-sonnet-20250219.
    client = anthropic.Anthropic(api_key=config["API_KEY"])

    params = {
        "model": config["MODEL"],
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": config["MAX_TOKENS"] or 4096, # max_tokens is required for Anthropic
        "timeout": 900, # enc uses explicit timeouts for all API calls
    }
    # Anthropic doesn't support a seed parameter. Use temperature for determinism.
    if config.get("SEED") is not None:
        params["temperature"] = 0.0
    
    response = client.messages.create(**params)
    
    content = response.content[0].text
    # Anthropic doesn't report thinking tokens.
    usage = response.usage
    return content, {"input": usage.input_tokens, "output": usage.output_tokens, "thinking": 0}

# if the language model returns markdown fencing around the code, it will be removed before being written to the output file.
# markdown code fences will ONLY be removed from the very first and very last line of the LLM output!
def strip_markdown_fences(code: str) -> str:
    """Removes markdown code fences from the start and end of a string."""
    lines = code.splitlines()
    if len(lines) >= 2 and lines[0].strip().startswith("```") and lines[-1].strip() == "```":
        # Also remove the language identifier from the first line
        return "\n".join(lines[1:-1])
    return code

# pricing data for language model API calls is shown after the call completes based on the pricing data in `./res/pricing.json`.
# costs accounted by input, output, and thinking costs.
def calculate_and_print_cost(model_id: str, provider: str, usage: Dict[str, int], pricing_data: Dict[str, Any]):
    """Calculates and prints the cost summary for an API call."""
    full_model_id = f"{provider}/{model_id}"
    pricing = pricing_data.get(full_model_id)

    if not pricing:
        print(f"\n--- cost summary ---", file=sys.stdout)
        print(f"Warning: No pricing data found for model '{full_model_id}'", file=sys.stdout)
        return
        
    input_cost = usage['input'] * pricing.get('input_cost_per_token', 0)
    output_cost = usage['output'] * pricing.get('output_cost_per_token', 0)
    
    # if cost per thinking token is not known, they are priced as output tokens.
    thinking_cost_per_token = pricing.get('thinking_cost_per_token', pricing.get('output_cost_per_token', 0))
    thinking_cost = usage['thinking'] * thinking_cost_per_token
    
    total_cost = input_cost + output_cost + thinking_cost

    print(f"\n--- api cost summary ---", file=sys.stdout)
    print(f"provider: {provider}, model: {model_id}", file=sys.stdout)
    print(f"tokens: {usage['input']} input, {usage['output']} output, {usage['thinking']} thinking", file=sys.stdout)
    print(f"estimated cost:", file=sys.stdout)
    print(f"  - input   : ${input_cost:.6f}", file=sys.stdout)
    print(f"  - output  : ${output_cost:.6f}", file=sys.stdout)
    if usage['thinking'] > 0:
        print(f"  - thinking: ${thinking_cost:.6f}", file=sys.stdout)
    print(f"total: ${total_cost:.6f}", file=sys.stdout)

def main():
    """Main execution function for the enc program."""
    # enc uses a layered configuration
    args = get_cli_args()
    config = load_configuration(args)
    
    # when the `--show-config` flag is provided, enc dumps the redacted config and then exits immediately.
    if args.show_config:
        print(json.dumps(get_redacted_config(config), indent=4))
        sys.exit(0)

    # all flags are defined as optional and enforced procedurally at the point where the configuration values are consumed.
    if "INPUT_FILE" not in config or "OUTPUT_FILE" not in config:
        print("Error: The input_file and --output-file arguments are required.", file=sys.stderr)
        sys.exit(1)

    # the full path to the log file is written to the console when the program starts.
    log_file_path = setup_logging(config["LOGS_PATH"])
    print(f"debug log path: {log_file_path}")

    # the consolidated configuration is logged at the start of each invocation.
    logging.info(f"Consolidated configuration: {json.dumps(config, indent=2)}")

    # if no seed is provided, a random seed is chosen at startup time.
    if config["SEED"] is None:
        config["SEED"] = random.randint(0, 2**32 - 1)
        logging.info(f"No seed provided, using random seed: {config['SEED']}")

    # enc is a program that can be used to transpile any english language file...
    print(f"transpiling '{config['INPUT_FILE']}' to '{config['OUTPUT_FILE']}' ({get_target_language(config['OUTPUT_FILE'])})")

    try:
        # Load all file-based resources
        # the prompt template is loaded from the file specified by the PROMPT_TEMPLATE_PATH setting.
        prompt_template_path = find_resource(config["PROMPT_TEMPLATE_PATH"])
        if not prompt_template_path:
            raise FileNotFoundError(f"Prompt template not found at '{config['PROMPT_TEMPLATE_PATH']}'")
        prompt_template = prompt_template_path.read_text()

        # language model prices - model pricing data will be loaded from `./res/pricing.json`
        pricing_data_path = find_resource(config["PRICING_DATA_PATH"])
        if not pricing_data_path:
            raise FileNotFoundError(f"Pricing data not found at '{config['PRICING_DATA_PATH']}'")
        pricing_data = json.loads(pricing_data_path.read_text())

        # `english_content`: the exact content of the provided `.en` input file
        english_content = Path(config["INPUT_FILE"]).read_text()

        # `hacking_conventions`: the exact content of the configured HACKING_CONVENTIONS file
        # if the HACKING_CONVENTIONS source file is not present, that section can be skipped.
        hacking_conventions_content = ""
        hacking_conventions_path_str = config["HACKING_CONVENTIONS"]
        if hacking_conventions_path_str:
            hacking_conventions_path = Path(hacking_conventions_path_str)
            if hacking_conventions_path.exists():
                hacking_conventions_content = hacking_conventions_path.read_text()

        # `context_files`: the exact, unmodified paths and contents of the files configured in CONTEXT_FILES.
        context_files_content = ""
        context_files_list = [f for f in config["CONTEXT_FILES"].split(":") if f]
        
        # grounded mode adds the existing code (if present) to context files
        if config["GROUNDED_MODE"] and Path(config["OUTPUT_FILE"]).exists():
            context_files_list.append(config["OUTPUT_FILE"])

        for file_str in context_files_list:
            file_path = Path(file_str)
            if file_path.exists():
                # for each file, include a markdown header with the relative path to that file
                # file paths SHOULDN'T have "./" prepended to them automatically.
                relative_path_str = file_str
                context_files_content += f"### {relative_path_str}\n"
                # followed by the full contents of the file inside a markdown fenced code block.
                context_files_content += f"```{file_path.suffix[1:] if file_path.suffix else ''}\n"
                context_files_content += file_path.read_text()
                context_files_content += "\n```\n\n"

        # Build substitutions map for the prompt template
        target_language = get_target_language(config["OUTPUT_FILE"])
        
        # `generation_command`: the command line args enc was called with
        gen_command = ' '.join(sys.argv)
        # if the target format is a derivative of XML (eg. SVG), enc will strip double-hyphens (`--`) from `generation_command`
        if target_language in ["xml", "svg", "html"]:
            gen_command = gen_command.replace("--", "")

        # the following keys will be substituted before sending to the LLM provider
        substitutions = {
            "generation_command": gen_command,
            "generation_config": json.dumps(get_redacted_config(config), indent=2),
            "target_language": target_language,
            "output_path": config["OUTPUT_FILE"],
            "english_content": english_content,
            "hacking_conventions": hacking_conventions_content,
            "context_files": context_files_content,
        }

        final_prompt = expand_template(prompt_template, substitutions)
        # the template expanded prompt is logged.
        logging.info(f"Final prompt sent to LLM:\n---\n{final_prompt}\n---")

        # Select provider and make API call
        provider = config["PROVIDER"].lower()
        if provider == "google":
            llm_response_text, usage = call_google(final_prompt, config)
        elif provider == "openai":
            llm_response_text, usage = call_openai(final_prompt, config)
        elif provider == "anthropic":
            llm_response_text, usage = call_anthropic(final_prompt, config)
        else:
            raise ValueError(f"Unsupported provider: {provider}")

        # text content returned by the LLM is logged.
        logging.info(f"LLM response received:\n---\n{llm_response_text}\n---")

        # Process and save the output
        final_code = strip_markdown_fences(llm_response_text)
        Path(config["OUTPUT_FILE"]).write_text(final_code)

        print(f"successfully transpiled '{config['OUTPUT_FILE']}'", file=sys.stdout)

        # pricing data for language model API calls is shown after the call completes
        calculate_and_print_cost(config["MODEL"], provider, usage, pricing_data)

    except FileNotFoundError as e:
        print(f"Error: A required file was not found. {e}", file=sys.stderr)
        logging.error(f"File not found: {e}", exc_info=True)
        sys.exit(1)
    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        logging.error("An unexpected error occurred.", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()