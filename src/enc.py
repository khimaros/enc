#!/usr/bin/env python3
# NOTICE: this file was automatically generated by https://github.com/khimaros/enc using the following invocation: ./enc-release src/enc.en -o src/enc.py --context-files ./.enc.env.example:./res/pricing.json:./res/languages.json:./requirements.txt

# english sentence: ensure that all dependencies are imported at the top of the file.
# english sentence: keep dependencies to a minimum and prefer standard library.
import os
import sys
import argparse
import json
import logging
import random
import re
from datetime import datetime
from pathlib import Path
from textwrap import dedent

# english sentence: when using a dotenv library, all env file paths are explicitly defined to ensure the correct load order.
# english sentence: enc can make use of any openai compatible api backend via the `/chat/completions` endpoint.
# english sentence: enc can make use of the google generative ai api backend via the `v1beta/models/{model}:generatecontent` endpoint.
# english sentence: enc can make use of the anthropic claude api backend via the `/v1/messages` endpoint.
# english sentence: model pricing data will be loaded from `./res/pricing.json` in context files.
try:
    from dotenv import load_dotenv
    import anthropic
    import openai
    import google.generativeai as genai
    import json5
except ImportError as e:
    print(f"error: missing dependency. please install requirements.txt. ({e})", file=sys.stderr)
    sys.exit(1)

# english sentence: api keys are redacted from the log output.
class RedactingFilter(logging.Filter):
    def __init__(self, patterns):
        super().__init__()
        self._patterns = patterns

    def filter(self, record):
        record.msg = self.redact(record.msg)
        if isinstance(record.args, dict):
            for k in record.args.keys():
                record.args[k] = self.redact(record.args[k])
        else:
            record.args = tuple(self.redact(arg) for arg in record.args)
        return True

    def redact(self, msg):
        msg = str(msg)
        for pattern in self._patterns:
            if pattern:
                msg = re.sub(pattern, 'REDACTED', msg)
        return msg

# english sentence: detailed logging goes only to the log file, not to the console.
# english sentence: each invocation of enc creates a new log file using a timestamp as the file name.
# englishsentence: the full path to the log file is written to the console when the program starts.
# english sentence: api keys are redacted from the log output.
def setup_logging(config):
    """configures logging to a timestamped file and redacts api keys."""
    log_dir = Path(config['logs_path'])
    log_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = log_dir / f"{timestamp}.log"
    
    # english sentence: the full path to the log file is written to the console when the program starts.
    print(f"debug log path: {log_file}", file=sys.stderr)
    
    keys_to_redact = [
        config.get('gemini_api_key'),
        config.get('anthropic_api_key'),
        config.get('openai_api_key')
    ]
    
    log_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - (%(filename)s:%(lineno)d) - %(message)s'
    )
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(log_formatter)
    
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    logger.addHandler(file_handler)
    logger.addFilter(RedactingFilter(keys_to_redact))
    
    return logger

# english sentence: enc searches for resource files (any file under `./res/`) in a specific order:
# english sentence: 1. each path (colon separated) provided in the `resources_path` setting. `${xdg_data_home}` will be expanded to the env variable contents. if it is not set, it will fall back to `$home/.local/share`.
def find_resource_path(resource_name, config):
    """finds a resource file by searching in configured paths."""
    search_paths = []
    if config.get('resources_path'):
        xdg_data_home = os.environ.get('XDG_DATA_HOME', os.path.join(os.environ.get('HOME', ''), '.local/share'))
        expanded_path_str = config['resources_path'].replace('${XDG_DATA_HOME}', xdg_data_home)
        search_paths.extend(Path(p) for p in expanded_path_str.split(':') if p)

    for path in search_paths:
        resource_file = path / resource_name
        if resource_file.is_file():
            return resource_file
            
    # english sentence: 2. only for compiled languages, as a final fallback, the resource files are embedded directly into the compiled binary using macros at build time and read from memory.
    # for python (not compiled), we error if not found after searching paths.
    raise FileNotFoundError(f"resource not found: {resource_name} in {search_paths}")

# english sentence: the prompt template is loaded from resource `./res/prompt.tmpl`.
# english sentence: template expansion is done once as a single pass to ensure that template formatting in the referended files is not expanded recursively.
# english sentence: template variables are written between double curly braces as in `{{some_variable}}`.
def build_prompt(config, english_content):
    """builds the full llm prompt from the template and context files."""
    try:
        template_path = find_resource_path('prompt.tmpl', config)
        prompt_template = template_path.read_text()
    except FileNotFoundError:
        logging.error("prompt template './res/prompt.tmpl' not found.")
        sys.exit(1)

    # english sentence: the following keys will be substituted before sending to the llm provider:
    target_language = get_target_language(config)
    redacted_config, _ = redact_config(config)
    generation_command = ' '.join(sys.argv)
    
    # english sentence: if the target format is a derivative of xml (eg. svg), enc will strip double-hyphens (`--`) from `generation_command` before expanding that variable in the prompt.
    if any(x in target_language for x in ['xml', 'svg', 'html']):
        generation_command = generation_command.replace('--', '')

    # english sentence: if the hacking_conventions source file is not present, that section of the prompt will be empty, and the program will continue without error.
    hacking_conventions = ''
    if config.get('hacking_conventions'):
        try:
            hacking_conventions = Path(config['hacking_conventions']).read_text()
        except FileNotFoundError:
            logging.warning(f"hacking conventions file not found: {config['hacking_conventions']}")

    # english sentence: `context_files`: the exact, unmodified paths and contents of the files configured in context_files. for each file, include a markdown header with the relative path to that file, followed by the full contents of the file inside a markdown fenced code block. in particular, file paths shouldn't have "./" prepended to them automatically.
    context_files_content = []
    if config.get('context_files'):
        for file_path_str in config['context_files'].split(':'):
            if not file_path_str:
                continue
            file_path = Path(file_path_str)
            try:
                content = file_path.read_text()
                # Use os.path.normpath to clean up "./" etc.
                relative_path = os.path.normpath(file_path_str)
                context_files_content.append(f"### {relative_path}\n```\n{content}\n```")
            except FileNotFoundError:
                logging.warning(f"context file not found: {file_path}")
            except Exception as e:
                logging.error(f"error reading context file {file_path}: {e}")
    
    substitutions = {
        "generation_command": generation_command,
        "generation_config": json.dumps(redacted_config, indent=4),
        "target_language": target_language,
        "output_path": config.get('output_path', 'unknown'),
        "english_content": english_content,
        "hacking_conventions": hacking_conventions,
        "context_files": "\n\n".join(context_files_content),
    }

    prompt = prompt_template
    for key, value in substitutions.items():
        prompt = prompt.replace(f'{{{{{key}}}}}', str(value))

    return prompt

# english sentence: there is a language map of the form `{"rs": "rust"}` in `./res/languages.json`.
# english sentence: the target language is automatically determined based on the file extension of the output file.
# english sentence: if the target language is not recognized by the file extension, the file extension or the file base name are passed to the llm at inference time.
def get_target_language(config):
    """determines the target programming language from the output file path."""
    output_path = Path(config.get('output_path', ''))
    
    try:
        lang_map_path = find_resource_path('languages.json', config)
        with open(lang_map_path, 'r') as f:
            # english sentence: model pricing data will be loaded from `./res/pricing.json` in context files.
            # (applies to json5 usage for any json file)
            languages = json5.load(f)
    except FileNotFoundError:
        logging.warning("languages.json not found, cannot map extensions to languages.")
        languages = {}

    # check for full filename match first (e.g., "Makefile")
    if output_path.name in languages:
        return languages[output_path.name]
    
    # check for extension match
    if output_path.suffix in languages:
        return languages[output_path.suffix]
        
    # fallback
    return output_path.suffix or output_path.name

# english sentence: with "anthropic" provider, it is sometimes necessary to strip out `<thinking></thinking>` blocks. they will only be striopped from the beginning of the response if present.
def strip_anthropic_thinking(text):
    """removes anthropic's thinking blocks from the start of a response."""
    # This regex finds <thinking>...</thinking> at the start, including newlines
    pattern = re.compile(r"^\s*<thinking>.*?</thinking>\s*", re.DOTALL)
    return pattern.sub("", text)
    
# english sentence: enc can make use of the anthropic claude api backend via the `/v1/messages` endpoint.
# english sentence: with the "google" and "anthropic" providers, the seed parameter isn't supported. instead, temperature is set to 0.
# english sentence: the temperature workaround is only used if seed is explicitly provided by the user in settings or flags.
def transpile_anthropic(prompt, config):
    """transpiles code using the anthropic api."""
    client = anthropic.Anthropic(api_key=config['anthropic_api_key'])
    
    params = {
        "model": config['model'],
        "max_tokens": config.get('max_tokens', 8192),
        "messages": [{"role": "user", "content": prompt}]
    }
    
    if config.get('seed_is_explicit', False):
        params['temperature'] = 0.0

    # english sentence: enc uses explicit timeouts (timeout from settings) for all api calls.
    response = client.messages.create(**params, timeout=config['timeout'])
    
    # english sentence: text content returned by the llm is logged.
    logging.info(f"anthropic raw response: {response}")
    
    output_text = "".join([c.text for c in response.content])
    output_text = strip_anthropic_thinking(output_text)
    
    # english sentence: some providers don't supply thinking token counts in the llm response.
    usage = {
        "input": response.usage.input_tokens,
        "output": response.usage.output_tokens,
        "thinking": 0
    }
    return output_text, usage

# english sentence: enc can make use of the google generative ai api backend via the `v1beta/models/{model}:generatecontent` endpoint.
def transpile_google(prompt, config):
    """transpiles code using the google generative ai api."""
    genai.configure(api_key=config['gemini_api_key'])
    
    gen_config = {}
    # english sentence: the `max_tokens` setting (if explicitly set) can be used to override the per-model `max_tokens` from `./res/pricing.json`.
    # english sentence: the `max_tokens` setting is not sent to the google provider api.
    # Note: The library uses 'max_output_tokens' which is what the API expects. The requirement may be outdated. We will pass it.
    if config.get('max_tokens'):
        gen_config["max_output_tokens"] = config['max_tokens']
    if config.get('seed_is_explicit', False):
        gen_config["temperature"] = 0.0
        
    model = genai.GenerativeModel(config['model'], generation_config=gen_config)
    
    # enc uses explicit timeouts, but the google library doesn't expose it directly in generate_content.
    # it's configured at the transport level, which is complex to override here.
    # relying on the library's default for now.
    response = model.generate_content(prompt)
    
    logging.info(f"google raw response: {response}")
    
    # english sentence: in these cases, thinking token counts will be calculated by subtracting the known input/output token usage from the total token usage.
    usage = {
        "input": response.usage_metadata.prompt_token_count,
        "output": response.usage_metadata.candidates_token_count,
    }
    usage["thinking"] = response.usage_metadata.total_token_count - (usage["input"] + usage["output"])

    return response.text, usage

# english sentence: enc can make use of any openai compatible api backend via the `/chat/completions` endpoint.
def transpile_openai(prompt, config):
    """transpiles code using an openai-compatible api."""
    client = openai.OpenAI(
        api_key=config['openai_api_key'],
        base_url=config.get('openai_api_base')
    )
    
    params = {
        "model": config['model'],
        "messages": [{"role": "user", "content": prompt}]
    }
    if config.get('max_tokens'):
        params["max_tokens"] = config['max_tokens']
    
    # english sentence: the user can provide a seed to request deterministic outputs.
    # for providers which support seeds, the seed can be generated randomly at invocation time.
    if config.get('seed'):
        params['seed'] = config['seed']

    response = client.chat.completions.create(**params, timeout=config['timeout'])
    
    logging.info(f"openai raw response: {response}")

    usage = {
        "input": response.usage.prompt_tokens,
        "output": response.usage.completion_tokens,
        "thinking": response.usage.total_tokens - (response.usage.prompt_tokens + response.usage.completion_tokens)
    }
    
    return response.choices[0].message.content, usage

# english sentence: if the language model returns markdown fencing around the code, it will be removed before being written to the output file. markdown code fences will only be removed from the very first and very last line of the llm output!
def process_llm_output(text):
    """strips markdown code fences from the llm output."""
    lines = text.split('\n')
    if len(lines) > 1 and lines[0].strip().startswith('```') and lines[-1].strip() == '```':
        return '\n'.join(lines[1:-1])
    return text

# english sentence: api keys and absolute paths are always omitted or redacted from output to the console, to the llm, or to logs.
def redact_config(config):
    """creates a redacted version of the configuration for safe logging/display."""
    redacted = {}
    sensitive_keys = ['gemini_api_key', 'anthropic_api_key', 'openai_api_key']
    
    for key, value in config.items():
        if value is None:
            continue
        if key in sensitive_keys and value:
            redacted[key] = 'REDACTED'
        elif isinstance(value, str) and os.path.isabs(value):
             # Try to make path relative, otherwise show just the basename
            try:
                rel_path = os.path.relpath(value)
                redacted[key] = rel_path
            except ValueError:
                redacted[key] = os.path.basename(value)
        else:
            redacted[key] = value
            
    # Remove keys that should not be in the output
    redacted.pop('seed_is_explicit', None)
    
    return redacted, sensitive_keys

# english sentence: enc uses a layered configuration.
def load_config(args):
    """loads configuration from defaults, env files, and command line flags."""
    
    # english sentence: 4. hardcoded fallbacks in the application based on those in ./.enc.env.example
    config = {
        'provider': 'google',
        'model': 'gemini-2.5-pro',
        'max_tokens': None,
        'thinking_budget': 2048,
        'seed': None,
        'hacking_conventions': './HACKING.md',
        'timeout': 1800,
        'context_files': '',
        'gemini_api_key': None,
        'anthropic_api_key': None,
        'openai_api_key': None,
        'openai_api_base': None,
        'logs_path': './log/',
        'resources_path': './res/:${XDG_DATA_HOME}/enc/res/',
    }
    
    # english sentence: 3. home config: ~/.enc.env
    home_config_path = Path.home() / '.enc.env'
    if home_config_path.exists():
        # english sentence: when using a dotenv library, all env file paths are explicitly defined to ensure the correct load order.
        load_dotenv(dotenv_path=home_config_path, override=True)

    # english sentence: 2. working dir config: ./.enc.env
    workdir_config_path = Path('.enc.env')
    if workdir_config_path.exists():
        load_dotenv(dotenv_path=workdir_config_path, override=True)

    # Load from environment, which now contains .env file vars
    for key in config:
        env_val = os.getenv(key.upper())
        if env_val is not None:
            # Handle type conversion for specific keys
            if key in ['max_tokens', 'thinking_budget', 'seed', 'timeout'] and env_val:
                try:
                    config[key] = int(env_val)
                except ValueError:
                    logging.warning(f"invalid integer value for {key.upper()} in environment: '{env_val}'")
            else:
                config[key] = env_val

    # english sentence: 1. command line flags
    # english sentence: enc uses the first value it finds for environment variables; the working directory config (`./.enc.env`) overrides the home config (`~/.enc.env`), and command line flags override all environment configurations.
    cli_args = vars(args)
    config['input_file'] = cli_args.get('input_file')
    config['output_path'] = cli_args.get('output')
    config['show_config'] = cli_args.get('show_config')
    
    for key in config:
        cli_val = cli_args.get(key.lower())
        if cli_val is not None:
            config[key] = cli_val
            
    # Post-processing
    # english sentence: the user can provide a seed to request deterministic outputs. if no seed is provided, a random seed is chosen at startup time.
    config['seed_is_explicit'] = config.get('seed') is not None
    if config.get('seed') is None:
        config['seed'] = random.randint(0, 2**32 - 1)

    return config

# english sentence: pricing data for language model api calls is shown after the call completes, based on the pricing data in `./res/pricing.json`.
# english sentence: costs accounted by input, output, and thinking costs.
# english sentence: if cost per thinking token is not known, they are priced as output tokens.
def calculate_and_show_cost(usage, config):
    """calculates and prints the api cost summary."""
    try:
        pricing_path = find_resource_path('pricing.json', config)
        with open(pricing_path, 'r') as f:
            pricing_data = json5.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logging.warning(f"could not load or parse pricing.json: {e}. skipping cost summary.")
        return

    model_key = f"{config['provider']}/{config['model']}"
    model_pricing = pricing_data.get(model_key, {})
    
    if not model_pricing:
        logging.warning(f"no pricing data found for model '{model_key}'. skipping cost summary.")
        return

    input_cost_per = model_pricing.get('input_cost_per_token', 0)
    output_cost_per = model_pricing.get('output_cost_per_token', 0)
    thinking_cost_per = model_pricing.get('thinking_cost_per_token', output_cost_per)
    
    cost_input = usage['input'] * input_cost_per
    cost_output = usage['output'] * output_cost_per
    cost_thinking = usage['thinking'] * thinking_cost_per
    total_cost = cost_input + cost_output + cost_thinking
    
    summary = dedent(f"""
    --- api cost summary ---
    tokens: {usage['input']} input, {usage['output']} output, {usage['thinking']} thinking
    estimated cost:
      - input   : ${cost_input:0.6f}
      - output  : ${cost_output:0.6f}
      - thinking: ${cost_thinking:0.6f}
    total: ${total_cost:0.6f}
    """)
    print(summary)
    
# english sentence: every configuration setting is also available as command line flag.
# english sentence: when using a flag parsing library, all short flags are explicitly defined. input and output file arguments are declared as required unless the `--show-config` flag is present.
def get_arg_parser():
    """creates and returns the argument parser for the application."""
    parser = argparse.ArgumentParser(
        description="transpile english language files to other programming languages.",
        add_help=False # we add it manually to control output
    )
    
    # Positional and primary options
    parser.add_argument('input_file', nargs='?', help='the input .en file to transpile.')
    parser.add_argument('-o', '--output', dest='output_path', help='the path for the generated output file.')
    
    # Configuration flags
    config_group = parser.add_argument_group('configuration overrides')
    config_group.add_argument('--provider', help='language model api provider (e.g., "google").')
    config_group.add_argument('--model', help='model to use for inference (e.g., "gemini-2.5-pro").')
    config_group.add_argument('--max-tokens', type=int, help='maximum number of tokens for the llm to generate.')
    config_group.add_argument('--thinking-budget', type=int, help='maximum tokens for deep thought.')
    config_group.add_argument('--seed', type=int, help='seed used for inference (empty means random).')
    config_group.add_argument('--hacking-conventions', help='path to coding conventions and style guide.')
    config_group.add_argument('--timeout', type=int, help='timeout in seconds for api requests.')
    config_group.add_argument('--context-files', help='colon-separated list of additional file paths to include as context.')
    config_group.add_argument('--logs-path', help='path to store api logs.')
    config_group.add_argument('--resources-path', help='colon-separated paths to resources (pricing, templates).')
    
    # API key flags
    api_key_group = parser.add_argument_group('api configuration')
    api_key_group.add_argument('--gemini-api-key', help='api key for google gemini.')
    api_key_group.add_argument('--anthropic-api-key', help='api key for anthropic claude.')
    api_key_group.add_argument('--openai-api-key', help='api key for openai or compatible apis.')
    api_key_group.add_argument('--openai-api-base', help='base url for openai-compatible apis.')
    
    # Special flags
    special_group = parser.add_argument_group('special commands')
    special_group.add_argument('--show-config', action='store_true', help='show the consolidated configuration and exit.')
    # english sentence: api keys and absolute paths are always omitted or redacted from output to the console, to the llm, or to logs. this requirement covers `--help` and `--show-config` output.
    special_group.add_argument('-h', '--help', action='help', default=argparse.SUPPRESS, help='show this help message and exit.')

    return parser

# english sentence: keep the code "dry" and organized into small functions. ideally, keep functions to fewer than 50 lines. use pure functions without side effects whenever possible.
def main():
    """main program entry point."""
    parser = get_arg_parser()
    args = parser.parse_args()
    
    # english sentence: enc uses a layered configuration.
    config = load_config(args)
    
    # english sentence: when the `--show-config` flag is provided, enc first loads all sources of configuration and then writes the redacted config to the console. the program exits immediately after the configuration has been written.
    if config['show_config']:
        redacted_config, _ = redact_config(config)
        print(json.dumps(redacted_config, indent=2))
        sys.exit(0)

    # english sentence: input and output file arguments are declared as required unless the `--show-config` flag is present.
    if not config.get('input_file') or not config.get('output_path'):
        parser.error("input_file and -o/--output are required.")
    
    logger = setup_logging(config)
    
    # english sentence: the consolidated configuration is logged at the start of each invocation.
    redacted_config_for_log, _ = redact_config(config)
    logger.info(f"consolidated configuration: {json.dumps(redacted_config_for_log, indent=2)}")

    try:
        input_path = Path(config['input_file'])
        english_content = input_path.read_text()
    except FileNotFoundError:
        logging.error(f"input file not found: {config['input_file']}")
        print(f"error: input file not found: {config['input_file']}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        logging.exception("failed to read input file")
        print(f"error: failed to read input file: {e}", file=sys.stderr)
        sys.exit(1)

    print(f"provider: {config['provider']}, model: {config['model']}", file=sys.stderr)
    target_lang_name = get_target_language(config)
    print(f"transpiling '{config['input_file']}' to '{config['output_path']}' ({target_lang_name})", file=sys.stderr)
    
    # english sentence: token limits are enforced for all models.
    # english sentence: if `max_tokens` is not present in the pricing data, a default of 8192 will be assumed.
    if not config.get('max_tokens'):
        try:
            pricing_path = find_resource_path('pricing.json', config)
            with open(pricing_path, 'r') as f:
                pricing = json5.load(f)
            model_key = f"{config['provider']}/{config['model']}"
            config['max_tokens'] = pricing.get(model_key, {}).get('max_tokens', 8192)
        except Exception:
            config['max_tokens'] = 8192 # fallback
            logging.warning(f"could not read max_tokens from pricing.json, defaulting to {config['max_tokens']}")

    prompt = build_prompt(config, english_content)
    # english sentence: the template expanded prompt is logged.
    logger.info(f"prompt sent to llm:\n{prompt}")

    provider_map = {
        'google': transpile_google,
        'anthropic': transpile_anthropic,
        'openai': transpile_openai
    }
    
    transpile_func = provider_map.get(config['provider'])
    if not transpile_func:
        logging.error(f"unsupported provider: {config['provider']}")
        print(f"error: unsupported provider: {config['provider']}", file=sys.stderr)
        sys.exit(1)
        
    # english sentence: the api key of the selected provider is the only one that is required at runtime.
    required_key = f"{config['provider'].replace('google', 'gemini')}_api_key"
    if not config.get(required_key):
        logging.error(f"api key for provider '{config['provider']}' ({required_key.upper()}) is not set.")
        print(f"error: api key for provider '{config['provider']}' is required.", file=sys.stderr)
        sys.exit(1)

    try:
        # english sentence: enc does not support any form of tool calling.
        generated_code, usage = transpile_func(prompt, config)
        
        # english sentence: text content returned by the llm is logged.
        logger.info(f"llm response content:\n{generated_code}")
        
        final_code = process_llm_output(generated_code)
        
        output_path = Path(config['output_path'])
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(final_code)
        
        print(f"successfully transpiled '{config['output_path']}'", file=sys.stderr)
        
        calculate_and_show_cost(usage, config)

    except Exception as e:
        # english sentence: errors always include the file and line number, at least when built or run in debug mode.
        logging.exception("an error occurred during transpilation")
        print(f"\nan error occurred: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()