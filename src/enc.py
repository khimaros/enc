#!/usr/bin/env python3

# automatically generated by https://github.com/khimaros/enc using the following invocation: ./enc-release src/enc.en -o src/enc.py --context-files .[REDACTED_PATH]

import os
import sys
import json
import re
import time
import datetime
import subprocess
import argparse
import traceback
from pathlib import Path

# "keep dependencies to a minimum and prefer standard library."
try:
    import json5
except ImportError:
    json5 = json

try:
    import requests
except ImportError:
    # "if an error occurs during any iteration... the error is printed to the console"
    print("error: 'requests' library is required. install with 'pip install requests'")
    sys.exit(1)

# "enc searches for resource files... compile-time macros are used to embed the actual resource files into the compiled binary."
# in python, we use constants to emulate embedded resources.
EMBEDDED_LANGUAGES = """
{
    ".c": "c",
    ".cpp": "cpp",
    ".py": "python",
    ".rs": "rust",
    ".go": "go",
    ".js": "javascript",
    ".ts": "typescript",
    ".rb": "ruby",
    ".java": "java",
    "Makefile": "makefile"
}
"""

EMBEDDED_PRICING = """
{
    "anthropic/claude-sonnet-4-20250514": {
        "input_cost_per_token": 3e-06,
        "max_tokens": 8192,
        "output_cost_per_token": 1.5e-05
    },
    "google/gemini-2.5-pro": {
        "input_cost_per_token": 1.25e-06,
        "max_tokens": 65535,
        "output_cost_per_token": 1e-05
    },
    "openai/gpt-5-2025-08-07": {
        "input_cost_per_token": 5e-06,
        "max_tokens": 16384,
        "output_cost_per_token": 1.5e-05
    }
}
"""

EMBEDDED_PROMPT = """
Target Language: {{target_language}}
Output Path: {{output_path}}

Hacking Conventions:
{{hacking_conventions}}

Context Files:
{{context_files}}

Source Description:
{{english_content}}

Generation Command: {{generation_command}}
Configuration:
{{generation_config}}

Instructions:
Generate the source code for the file at {{output_path}}.
The response must contain ONLY valid {{target_language}} code.
"""

# "enc uses a layered configuration. settings are sourced with the following precedence..."
DEFAULT_CONFIG = {
    "PROVIDER": "google",
    "MODEL": None,
    "MAX_TOKENS": None,
    "THINKING_BUDGET": None,
    "SEED": None,
    "HACKING_CONVENTIONS": "./HACKING.md",
    "TIMEOUT": 1800,
    "CONTEXT_FILES": "",
    "GEMINI_API_KEY": "",
    "ANTHROPIC_API_KEY": "",
    "OPENAI_API_KEY": "",
    "OPENAI_API_BASE": "https://api.openai.com/v1",
    "LOGS_PATH": "./log/",
    "RESOURCES_PATH": "./res/:${XDG_DATA_HOME}/enc/res/",
    "TEST_COMMAND": "",
    "TEST_ITERATIONS": 3
}

# "code comments and command line output should be all lowercase"
# "keep the code 'DRY' and organized into small functions. ideally, keep functions to fewer than 50 lines."

def log_debug(msg, path):
    """detailed logging goes ONLY to the log file, not to the console."""
    with open(path, "a") as f:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
        f.write(f"[{timestamp}] {msg}\n")

def redact_api_keys(config):
    """API keys are redacted from the log output and replaced with [REDACTED]."""
    redacted = config.copy()
    for k in redacted:
        if "API_KEY" in k:
            redacted[k] = "[REDACTED]"
    return redacted

def redact_paths(text):
    """absolute paths are redacted when shown to the LLM or in console help output."""
    if not text: return text
    # basic heuristic for absolute paths
    return re.sub(r'/[a-zA-Z0-9._/-]+', lambda m: "[REDACTED_PATH]" if m.group(0).count('/') > 2 else m.group(0), text)

# ".env files are read manually to extract config values WITHOUT modifying the process environment."
def read_env_file(path):
    expanded_path = os.path.expanduser(path)
    if not os.path.exists(expanded_path): return {}
    res = {}
    with open(expanded_path) as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"): continue
            if "=" in line:
                k, v = line.split("=", 1)
                res[k.strip()] = v.strip().strip('"').strip("'")
    return res

def get_consolidated_config(args):
    """settings are sourced with the following precedence (from highest to lowest): 1. flags, 2. env, 3. ./.enc.env, 4. ~/.enc.env, 5. defaults"""
    conf = DEFAULT_CONFIG.copy()
    
    # 4. ~/.enc.env
    conf.update(read_env_file("~/.enc.env"))
    # 3. ./.enc.env
    conf.update(read_env_file("./.enc.env"))
    # 2. environment variables
    for k in conf:
        if k in os.environ:
            conf[k] = os.environ[k]
            
    # 1. command line flags
    args_dict = vars(args)
    for k in conf:
        flag_val = args_dict.get(k.lower())
        if flag_val is not None:
            conf[k] = flag_val

    # fix types for numeric fields
    for k in ["MAX_TOKENS", "THINKING_BUDGET", "SEED", "TIMEOUT", "TEST_ITERATIONS"]:
        if conf[k] is not None and conf[k] != "":
            conf[k] = int(conf[k])
        else:
            conf[k] = None if k != "TEST_ITERATIONS" else 3
            
    return conf

# "enc searches for resource files (any file under ./res/) in a specific order"
def load_resource(name, config, fallback):
    res_paths = config["RESOURCES_PATH"].split(":")
    xdg_data = os.environ.get("XDG_DATA_HOME", os.path.expanduser("~/.local/share"))
    for p in res_paths:
        p = p.replace("${XDG_DATA_HOME}", xdg_data)
        full_path = Path(os.path.expanduser(p)) / name
        if full_path.exists():
            with open(full_path) as f: return f.read()
    return fallback

# "the target language name is loaded from ./res/languages.json."
def get_target_language(output_path, config):
    langs_json = load_resource("languages.json", config, EMBEDDED_LANGUAGES)
    langs = json.loads(langs_json)
    ext = os.path.splitext(output_path)[1]
    base = os.path.basename(output_path)
    # "if the target language is not found in the map, the key used for the lookup... is used"
    lang_name = langs.get(ext) or langs.get(base) or ext or base
    return lang_name.lstrip('.')

def get_cost(provider, model, usage, pricing_data):
    """costs are accounted by input, output, and thinking costs."""
    key = f"{provider}/{model}"
    data = pricing_data.get(key, {})
    
    in_tokens = usage.get('input', 0)
    out_tokens = usage.get('output', 0)
    think_tokens = usage.get('thinking', 0)
    
    # "if cost per thinking token is not known, they are priced as output tokens."
    in_cost = in_tokens * data.get('input_cost_per_token', 0)
    out_cost = out_tokens * data.get('output_cost_per_token', 0)
    think_cost = think_tokens * data.get('output_cost_per_token', 0)
    
    return {'input': in_cost, 'output': out_cost, 'thinking': think_cost}

class UsageTracker:
    def __init__(self, pricing):
        self.total_input = 0
        self.total_output = 0
        self.total_thinking = 0
        self.total_elapsed = 0.0
        self.pricing = pricing

    def add(self, provider, model, usage, elapsed):
        self.total_input += usage.get('input', 0)
        self.total_output += usage.get('output', 0)
        self.total_thinking += usage.get('thinking', 0)
        self.total_elapsed += elapsed

    def report(self):
        """at the end of the invocation... a final accumulated summary is displayed"""
        print(f"\n--- api request summary ---")
        thinking_str = f", {self.total_thinking} thinking" if self.total_thinking > 0 else ""
        print(f"tokens: {self.total_input} input, {self.total_output} output{thinking_str}")
        
        # pricing check
        costs = {'input': 0.0, 'output': 0.0, 'thinking': 0.0}
        # note: simplified for summary; ideally per-call costs are summed. 
        # in this impl, we sum tokens, then price (assuming same model used).
        # strictly following spec "accumulated totals across all API calls"
        # we'll use a per-call cost tracking instead.
        
    def add_cost(self, cost_dict):
        if not hasattr(self, 'total_costs'): self.total_costs = {'input': 0.0, 'output': 0.0, 'thinking': 0.0}
        for k in cost_dict: self.total_costs[k] += cost_dict[k]

    def finalize(self):
        thinking_str = f", {self.total_thinking} thinking" if self.total_thinking > 0 else ""
        print(f"\n--- api request summary ---")
        print(f"tokens: {self.total_input} input, {self.total_output} output{thinking_str}")
        print("estimated cost:")
        print(f"  - input: ${self.total_costs['input']:0.6f}")
        print(f"  - output: ${self.total_costs['output']:0.6f}")
        if self.total_thinking > 0:
            print(f"  - thinking: ${self.total_costs['thinking']:0.6f}")
        total = sum(self.total_costs.values())
        print(f"total: ${total:0.6f}")
        print(f"elapsed time: {self.total_elapsed:0.2f}s")

# "enc can make use of the Anthropic Claude API backend via the /v1/messages endpoint with streaming support enabled."
def call_anthropic(config, prompt, log_path):
    url = "https://api.anthropic.com/v1/messages"
    headers = {
        "x-api-key": config["ANTHROPIC_API_KEY"],
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }
    
    payload = {
        "model": config["MODEL"] or "claude-sonnet-4-20250514",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": config["MAX_TOKENS"] or 8192,
        "stream": True
    }
    
    if config.get("THINKING_BUDGET"):
        payload["thinking"] = {"type": "enabled", "budget_tokens": config["THINKING_BUDGET"]}
        payload["temperature"] = 1 # "temperature may only be set to 1 when thinking is enabled"
    elif config.get("SEED") is not None:
        payload["temperature"] = 0 # "seed parameter ISN'T supported. instead, temperature is set to 0"

    start_time = time.time()
    response = requests.post(url, headers=headers, json=payload, stream=True, timeout=config["TIMEOUT"])
    log_debug(f"api request-id: {response.headers.get('request-id')}", log_path)
    
    if response.status_code != 200:
        log_debug(f"api error: {response.text}", log_path)
        raise Exception(f"api call failed with code {response.status_code}")

    full_text = ""
    thinking_text = ""
    usage = {"input": 0, "output": 0, "thinking": 0}

    # "the streaming parser uses line buffering... handles text_delta, thinking_delta, signature_delta"
    for line in response.iter_lines():
        if not line: continue
        line_str = line.decode('utf-8')
        if line_str.startswith("data: "):
            event = json.loads(line_str[6:])
            etype = event.get("type")
            if etype == "message_start":
                usage["input"] = event["message"]["usage"]["input_tokens"]
            elif etype == "content_block_delta":
                delta = event["delta"]
                if delta["type"] == "text_delta":
                    full_text += delta["text"]
                elif delta["type"] == "thinking_delta":
                    thinking_text += delta["thinking"]
                    log_debug(f"thinking delta: {delta['thinking']}", log_path)
            elif etype == "message_delta":
                usage["output"] = event["usage"]["output_tokens"]

    # "thinking tokens are NOT included in the usage metadata... estimate by counting tokens... character count divided by 4"
    if thinking_text:
        usage["thinking"] = len(thinking_text) // 4
        
    return full_text, usage, time.time() - start_time, response.status_code

# "enc can make use of the Google Generative AI API backend via the v1beta/models/{model}:generateContent endpoint"
def call_google(config, prompt, log_path):
    model = config["MODEL"] or "gemini-2.5-pro"
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={config['GEMINI_API_KEY']}"
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "maxOutputTokens": config["MAX_TOKENS"] or 65535,
        }
    }
    if config.get("SEED") is not None:
        payload["generationConfig"]["temperature"] = 0

    start_time = time.time()
    response = requests.post(url, json=payload, timeout=config["TIMEOUT"])
    elapsed = time.time() - start_time
    
    if response.status_code != 200:
        log_debug(f"api error: {response.text}", log_path)
        raise Exception(f"api call failed with code {response.status_code}")

    data = response.json()
    # "necessary to concatenate the 'text' parts in candidates/content/parts"
    parts = data['candidates'][0]['content']['parts']
    text = "".join(p['text'] for p in parts)
    
    usage_meta = data.get('usageMetadata', {})
    usage = {
        "input": usage_meta.get('promptTokenCount', 0),
        "output": usage_meta.get('candidatesTokenCount', 0),
        "thinking": usage_meta.get('thinkingTokenCount', 0)
    }
    # "if total_tokens is provided but thinking_tokens is not, calculate thinking tokens using the formula"
    if usage["thinking"] == 0 and 'totalTokenCount' in usage_meta:
        usage["thinking"] = max(0, usage_meta['totalTokenCount'] - usage['input'] - usage['output'])

    return text, usage, elapsed, response.status_code

# "enc can make use of any OpenAI compatible API backend via the /chat/completions endpoint."
def call_openai(config, prompt, log_path):
    url = f"{config['OPENAI_API_BASE']}/chat/completions"
    headers = {"Authorization": f"Bearer {config['OPENAI_API_KEY']}", "Content-Type": "application/json"}
    
    model = config["MODEL"] or "gpt-5-2025-08-07"
    # "when using the official OpenAI API... max_completion_tokens is used. for all other... max_tokens"
    token_param = "max_completion_tokens" if "api.openai.com" in url else "max_tokens"
    
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        token_param: config["MAX_TOKENS"] or 8192
    }
    if config.get("SEED") is not None:
        payload["seed"] = config["SEED"]

    start_time = time.time()
    response = requests.post(url, headers=headers, json=payload, timeout=config["TIMEOUT"])
    elapsed = time.time() - start_time
    
    if response.status_code != 200:
        log_debug(f"api error: {response.text}", log_path)
        raise Exception(f"api call failed with code {response.status_code}")

    data = response.json()
    text = data['choices'][0]['message']['content']
    
    u = data.get('usage', {})
    # "check for thinking tokens in the following order: reasoning_tokens, etc."
    thinking = u.get('completion_tokens_details', {}).get('reasoning_tokens', 0)
    thinking = thinking or u.get('reasoning_tokens', 0)
    thinking = thinking or u.get('thinking_tokens', 0)
    if not thinking and 'total_tokens' in u:
        thinking = max(0, u['total_tokens'] - u.get('prompt_tokens', 0) - u.get('completion_tokens', 0))

    usage = {"input": u.get('prompt_tokens', 0), "output": u.get('completion_tokens', 0), "thinking": thinking}
    return text, usage, elapsed, response.status_code

def expand_prompt(template, variables):
    """template expansion is done ONCE as a SINGLE PASS"""
    res = template
    for k, v in variables.items():
        res = res.replace("{{" + k + "}}", str(v))
    return res

def strip_fences(text):
    """strip ONLY first/last line markdown code fences (```language or ```)"""
    lines = text.strip().splitlines()
    if not lines: return ""
    if lines[0].startswith("```"): lines = lines[1:]
    if lines and lines[-1].startswith("```"): lines = lines[:-1]
    return "\n".join(lines).strip()

def run_test(command, log_path):
    """test command is executed with a restricted environment... PATH, RUSTUP_HOME, CARGO_HOME"""
    env = {k: os.environ[k] for k in ["PATH", "RUSTUP_HOME", "CARGO_HOME"] if k in os.environ}
    try:
        proc = subprocess.run(command, shell=True, capture_output=True, text=True, env=env)
        return proc.returncode, proc.stdout, proc.stderr
    except Exception as e:
        return 1, "", str(e)

def show_config(config):
    """when the --show-config flag is provided, enc FIRST loads all sources of configuration and then writes the redacted config... in JSON format."""
    redacted = redact_api_keys(config)
    output = {}
    for k in sorted(redacted.keys()):
        val = redacted[k]
        # "numeric fields: omit if not explicitly set"
        if k in ["MAX_TOKENS", "SEED", "THINKING_BUDGET"]:
            if val is not None: output[k.lower()] = val
        # "string fields: omit ONLY if the consolidated value equals ''"
        elif isinstance(val, str):
            if val != "" or "API_KEY" in k:
                output[k.lower()] = val if "API_KEY" not in k else "[REDACTED]"
        else:
            output[k.lower()] = val
    print(json.dumps(output, indent=2, sort_keys=True))

def main():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument("input_file", nargs="?")
    parser.add_argument("-o", "--output", dest="output_file")
    parser.add_argument("--show-config", action="store_true")
    # "EVERY configuration setting is also available as command line flag."
    for k in DEFAULT_CONFIG:
        parser.add_argument(f"--{k.lower().replace('_', '-')}", dest=k.lower())

    args, unknown = parser.parse_known_args()
    config = get_consolidated_config(args)

    if args.show_config:
        show_config(config)
        sys.exit(0)

    if not args.input_file or not args.output_file:
        print("usage: enc <INPUT_FILE> -o <OUTPUT_FILE> [OPTIONS]")
        sys.exit(1)

    # "each invocation of enc creates a new log file... YYYYMMDD_HHMMSS.log"
    os.makedirs(config["LOGS_PATH"], exist_ok=True)
    log_name = datetime.datetime.now().strftime("%Y%m%d_%H%M%S.log")
    log_path = os.path.join(config["LOGS_PATH"], log_name)
    print(f"debug log path: {log_path}")

    log_debug(f"consolidated config: {json.dumps(redact_api_keys(config))}", log_path)

    # resources
    pricing_json = load_resource("pricing.json", config, EMBEDDED_PRICING)
    pricing = json.loads(pricing_json)
    tracker = UsageTracker(pricing)
    
    target_lang = get_target_language(args.output_file, config)
    print(f"transpiling '{args.input_file}' to '{args.output_file}' ({target_lang})")

    # template preparation
    prompt_tmpl = load_resource("prompt.tmpl", config, EMBEDDED_PROMPT)
    with open(args.input_file) as f: english_content = f.read()
    
    hacking = ""
    if os.path.exists(config["HACKING_CONVENTIONS"]):
        with open(config["HACKING_CONVENTIONS"]) as f: hacking = f.read()

    ctx_files_str = ""
    if config["CONTEXT_FILES"]:
        for path in config["CONTEXT_FILES"].split(":"):
            if not path: continue
            if not os.path.exists(path):
                print(f"error: context file not found: {path}")
                sys.exit(1)
            with open(path) as f:
                ctx_files_str += f"### {path}\n\n```\n{f.read()}\n```\n\n"

    gen_cmd = " ".join(sys.argv)
    # "if the target format is a derivative of XML (eg. SVG), enc will strip all double-hyphens"
    if args.output_file.endswith((".svg", ".xml", ".html")):
        gen_cmd = gen_cmd.replace("--", "")

    vars_map = {
        "generation_command": gen_cmd,
        "generation_config": json.dumps(redact_api_keys(config), indent=2),
        "target_language": target_lang,
        "output_path": args.output_file,
        "english_content": english_content,
        "hacking_conventions": hacking,
        "context_files": ctx_files_str
    }

    prompt = expand_prompt(prompt_tmpl, vars_map)
    log_debug(f"expanded prompt:\n{prompt}", log_path)

    attempt = 0
    test_failures = ""
    while True:
        attempt += 1
        current_prompt = prompt
        if test_failures:
            current_prompt += f"\n\nPrevious attempt failed tests:\n{test_failures}\nPlease fix the issues."

        print(f"calling api (provider: {config['PROVIDER']}, model: {config['MODEL'] or 'default'})...")
        try:
            if config["PROVIDER"] == "google":
                text, usage, elapsed, code = call_google(config, current_prompt, log_path)
            elif config["PROVIDER"] == "anthropic":
                text, usage, elapsed, code = call_anthropic(config, current_prompt, log_path)
            else:
                text, usage, elapsed, code = call_openai(config, current_prompt, log_path)
        except Exception as e:
            print(f"error: {str(e)}")
            tracker.finalize()
            sys.exit(1)

        print(f"api call completed with response code {code} after {elapsed:0.2f}s")
        log_debug(f"llm response:\n{text}", log_path)
        
        cost_map = get_cost(config["PROVIDER"], config["MODEL"] or "default", usage, pricing)
        tracker.add(config["PROVIDER"], config["MODEL"] or "default", usage, elapsed)
        tracker.add_cost(cost_map)

        # "if the LLM returns an empty response, enc will refuse to overwrite the output file"
        generated_code = strip_fences(text)
        if not generated_code:
            print("error: llm returned empty response")
            tracker.finalize()
            sys.exit(1)

        with open(args.output_file, "w") as f:
            f.write(generated_code)

        if not config["TEST_COMMAND"]:
            print(f"successfully transpiled '{args.output_file}'")
            break

        print(f"executing test command (`{config['TEST_COMMAND']}`), attempt {attempt}/{config['TEST_ITERATIONS']}")
        rc, out, err = run_test(config["TEST_COMMAND"], log_path)
        
        if rc == 0:
            print("test command succeeded!")
            print(f"successfully transpiled '{args.output_file}'")
            break
        else:
            print("test command failed, see log for details")
            log_debug(f"test failed (code {rc})\nstdout: {out}\nstderr: {err}", log_path)
            test_failures = f"Exit Code: {rc}\nSTDOUT:\n{out}\nSTDERR:\n{err}"
            
            if config["TEST_ITERATIONS"] != -1 and attempt >= config["TEST_ITERATIONS"]:
                tracker.finalize()
                sys.exit(1)

    tracker.finalize()

if __name__ == "__main__":
    try:
        main()
    except Exception:
        # "errors always include the file and line number"
        traceback.print_exc()
        sys.exit(1)