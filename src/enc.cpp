// NOTICE
// this file was automatically generated by https://github.com/khimaros/enc
// using the following invocation: ./enc-release src/enc.en -o src/enc.cpp --context-files ./.enc.env.example

#include <iostream>
#include <string>
#include <vector>
#include <fstream>
#include <sstream>
#include <map>
#include <stdexcept>
#include <chrono>
#include <iomanip>
#include <cstdlib>
#include <filesystem>
#include <regex>
#include <numeric>
#include <random>

// third-party dependencies (assumed to be available in the include path)
// nlohmann/json: for robust json parsing
#include <nlohmann/json.hpp>
// cpr: a modern c++ http library (libcurl wrapper)
#include <cpr/cpr.h>

namespace fs = std::filesystem;
using json = nlohmann::json;

// for error logging with file and line
#define THROW_RUNTIME_ERROR(msg) \
    throw std::runtime_error(std::string(msg) + " (" + __FILE__ + ":" + std::to_string(__LINE__) + ")")

// forward declarations
class Logger;
struct Config;
std::string get_current_timestamp();
void load_env_file(const fs::path& path, Config& config, bool overwrite);
std::string read_file_or_throw(const fs::path& path);


// INPUT CONTENT: configuration
struct Config {
    // from command line
    std::string input_file;
    std::string output_file;
    bool show_config = false;
    std::vector<std::string> generation_args;

    // from env/flags
    std::string provider = "google";
    std::string model = "gemini-2.5-pro";
    int max_tokens = 0; // 0 means use model default
    int thinking_budget = 2048;
    std::string seed;
    std::string hacking_conventions_file = "./HACKING.md";
    int timeout = 1800;
    std::vector<std::string> context_files;
    std::string gemini_api_key;
    std::string anthropic_api_key;
    std::string openai_api_key;
    std::string openai_api_base;
    std::string logs_path = "./log/";
    std::string resources_path = "./res/:${XDG_DATA_HOME}/enc/res/";
};

// INPUT CONTENT: logging
class Logger {
public:
    Logger() = default;
    Logger(const Logger&) = delete;
    Logger& operator=(const Logger&) = delete;

    void init(const std::string& log_path) {
        fs::create_directories(log_path);
        std::string timestamp = get_current_timestamp();
        log_file_path = fs::path(log_path) / (timestamp + ".log");
        log_stream.open(log_file_path);
        if (!log_stream.is_open()) {
            throw std::runtime_error("failed to open log file: " + log_file_path.string());
        }
        std::cout << "debug log path: " << log_file_path.string() << std::endl;
    }

    template<typename T>
    void log(const T& message) {
        if (log_stream.is_open()) {
            log_stream << message << std::endl;
        }
    }

    fs::path get_log_path() const { return log_file_path; }

private:
    std::ofstream log_stream;
    fs::path log_file_path;
};

// global logger instance
Logger logger;

// INPUT CONTENT: configuration
// helper to get a redacted version of the configuration
std::string get_redacted_config_string(const Config& config) {
    std::stringstream ss;
    ss << "provider: " << config.provider << "\n"
       << "model: " << config.model << "\n"
       << "max_tokens: " << (config.max_tokens == 0 ? "default" : std::to_string(config.max_tokens)) << "\n"
       << "thinking_budget: " << config.thinking_budget << "\n"
       << "seed: " << (config.seed.empty() ? "random" : config.seed) << "\n"
       << "hacking_conventions: " << config.hacking_conventions_file << "\n"
       << "timeout: " << config.timeout << "\n"
       << "context_files: ";
    for (const auto& f : config.context_files) ss << f << " ";
    ss << "\n"
       << "gemini_api_key: " << (config.gemini_api_key.empty() ? "" : "[REDACTED]") << "\n"
       << "anthropic_api_key: " << (config.anthropic_api_key.empty() ? "" : "[REDACTED]") << "\n"
       << "openai_api_key: " << (config.openai_api_key.empty() ? "" : "[REDACTED]") << "\n"
       << "openai_api_base: " << config.openai_api_base << "\n"
       << "logs_path: " << config.logs_path << "\n"
       << "resources_path: " << config.resources_path;
    return ss.str();
}

// INPUT CONTENT: utilities
std::string get_current_timestamp() {
    auto now = std::chrono::system_clock::now();
    auto in_time_t = std::chrono::system_clock::to_time_t(now);
    std::stringstream ss;
    ss << std::put_time(std::localtime(&in_time_t), "%Y%m%d_%H%M%S");
    return ss.str();
}

std::vector<std::string> split_string(const std::string& s, char delimiter) {
    std::vector<std::string> tokens;
    std::string token;
    std::istringstream token_stream(s);
    while (std::getline(token_stream, token, delimiter)) {
        tokens.push_back(token);
    }
    return tokens;
}

// INPUT CONTENT: library workarounds (dotenv)
void load_env_file(const fs::path& path, Config& config, bool overwrite) {
    if (!fs::exists(path)) return;
    std::ifstream file(path);
    std::string line;
    while (std::getline(file, line)) {
        if (line.empty() || line[0] == '#') continue;
        auto eq_pos = line.find('=');
        if (eq_pos == std::string::npos) continue;
        
        std::string key = line.substr(0, eq_pos);
        std::string value = line.substr(eq_pos + 1);
        // strip potential quotes
        if (value.length() >= 2 && value.front() == '"' && value.back() == '"') {
            value = value.substr(1, value.length() - 2);
        }

        auto set_if_needed = [&](std::string& cfg_field){ if (overwrite || cfg_field.empty() || cfg_field == config.hacking_conventions_file || cfg_field == config.logs_path || cfg_field == config.resources_path) cfg_field = value; };
        auto set_int_if_needed = [&](int& cfg_field, int default_val){ if ((overwrite || cfg_field == default_val) && !value.empty()) cfg_field = std::stoi(value); };

        if (key == "PROVIDER") set_if_needed(config.provider);
        else if (key == "MODEL") set_if_needed(config.model);
        else if (key == "MAX_TOKENS") set_int_if_needed(config.max_tokens, 0);
        else if (key == "THINKING_BUDGET") set_int_if_needed(config.thinking_budget, 2048);
        else if (key == "SEED") set_if_needed(config.seed);
        else if (key == "HACKING_CONVENTIONS") set_if_needed(config.hacking_conventions_file);
        else if (key == "TIMEOUT") set_int_if_needed(config.timeout, 1800);
        else if (key == "CONTEXT_FILES") { if(overwrite || config.context_files.empty()) config.context_files = split_string(value, ':'); }
        else if (key == "GEMINI_API_KEY") set_if_needed(config.gemini_api_key);
        else if (key == "ANTHROPIC_API_KEY") set_if_needed(config.anthropic_api_key);
        else if (key == "OPENAI_API_KEY") set_if_needed(config.openai_api_key);
        else if (key == "OPENAI_API_BASE") set_if_needed(config.openai_api_base);
        else if (key == "LOGS_PATH") set_if_needed(config.logs_path);
        else if (key == "RESOURCES_PATH") set_if_needed(config.resources_path);
    }
}

// INPUT CONTENT: configuration
void load_configuration(int argc, char* argv[], Config& config) {
    // 4. hardcoded fallbacks are the struct defaults
    
    // 3. home config
    const char* home_dir = getenv("HOME");
    if (home_dir) {
        load_env_file(fs::path(home_dir) / ".enc.env", config, false);
    }

    // 2. working dir config
    load_env_file("./.enc.env", config, true);

    // 1. command line flags
    for (int i = 0; i < argc; ++i) {
        config.generation_args.emplace_back(argv[i]);
    }
    
    std::vector<std::string> args(argv + 1, argv + argc);
    bool next_is_output = false;
    for (size_t i = 0; i < args.size(); ++i) {
        const auto& arg = args[i];
        
        auto get_next_arg = [&]() {
            if (i + 1 >= args.size()) THROW_RUNTIME_ERROR("flag " + arg + " requires an argument");
            return args[++i];
        };

        if (arg == "--show-config") config.show_config = true;
        else if (arg == "-o" || arg == "--output") config.output_file = get_next_arg();
        else if (arg == "--provider") config.provider = get_next_arg();
        else if (arg == "--model") config.model = get_next_arg();
        else if (arg == "--max-tokens") config.max_tokens = std::stoi(get_next_arg());
        else if (arg == "--thinking-budget") config.thinking_budget = std::stoi(get_next_arg());
        else if (arg == "--seed") config.seed = get_next_arg();
        else if (arg == "--hacking-conventions") config.hacking_conventions_file = get_next_arg();
        else if (arg == "--timeout") config.timeout = std::stoi(get_next_arg());
        else if (arg == "--context-files") config.context_files = split_string(get_next_arg(), ':');
        else if (arg == "--gemini-api-key") config.gemini_api_key = get_next_arg();
        else if (arg == "--anthropic-api-key") config.anthropic_api_key = get_next_arg();
        else if (arg == "--openai-api-key") config.openai_api_key = get_next_arg();
        else if (arg == "--openai-api-base") config.openai_api_base = get_next_arg();
        else if (arg == "--logs-path") config.logs_path = get_next_arg();
        else if (arg == "--resources-path") config.resources_path = get_next_arg();
        else if (arg.rfind("-", 0) != 0) { // doesn't start with -
            if (config.input_file.empty()) config.input_file = arg;
            else THROW_RUNTIME_ERROR("unrecognized positional argument: " + arg);
        } else {
             THROW_RUNTIME_ERROR("unrecognized flag: " + arg);
        }
    }
    
    // INPUT CONTENT: library workarounds (flag parsing)
    if (!config.show_config) {
        if (config.input_file.empty()) THROW_RUNTIME_ERROR("input file is required");
        if (config.output_file.empty()) THROW_RUNTIME_ERROR("output file is required");
    }
}

// INPUT CONTENT: resources
fs::path find_resource(const Config& config, const std::string& resource_name) {
    // 1. search in RESOURCES_PATH
    std::string expanded_path_str = config.resources_path;
    const char* xdg_data_home = getenv("XDG_DATA_HOME");
    std::string xdg_path_val;
    if(xdg_data_home) {
        xdg_path_val = xdg_data_home;
    } else {
        const char* home_dir = getenv("HOME");
        if(home_dir) xdg_path_val = fs::path(home_dir) / ".local/share";
    }
    size_t pos = expanded_path_str.find("${XDG_DATA_HOME}");
    if (pos != std::string::npos && !xdg_path_val.empty()) {
        expanded_path_str.replace(pos, 16, xdg_path_val);
    }

    auto search_paths = split_string(expanded_path_str, ':');
    for (const auto& p_str : search_paths) {
        fs::path p = p_str;
        if (!p.empty()) {
            auto resource_path = p / resource_name;
            if (fs::exists(resource_path)) {
                return resource_path;
            }
        }
    }
    
    // 2. compiled-in fallback (not implemented, will throw)
    // in a real build system, this would use `if` directives and a pre-build step
    // to generate a header with the resource content.
    THROW_RUNTIME_ERROR("resource not found: " + resource_name);
}

// INPUT CONTENT: code generation
std::string get_target_language(const std::string& output_path, const json& lang_map) {
    fs::path p(output_path);
    std::string key;
    if (p.has_extension()) {
        key = p.extension().string();
    } else {
        key = p.filename().string();
    }
    
    if (lang_map.contains(key)) {
        return lang_map.at(key);
    }
    return key; // fallback to extension or filename
}

// INPUT CONTENT: prompt template
std::string expand_template(std::string tmpl, const std::map<std::string, std::string>& vars) {
    for (const auto& [key, val] : vars) {
        std::string placeholder = "{{" + key + "}}";
        size_t pos = tmpl.find(placeholder);
        while (pos != std::string::npos) {
            tmpl.replace(pos, placeholder.length(), val);
            pos = tmpl.find(placeholder, pos + val.length());
        }
    }
    return tmpl;
}

// INPUT CONTENT: API cost summary
struct TokenUsage {
    int input = 0;
    int output = 0;
    int thinking = 0;
    int total = 0;
};

struct LLMResponse {
    std::string content;
    TokenUsage usage;
};

void print_cost_summary(const std::string& model, const TokenUsage& usage, const json& pricing_data) {
    if (!pricing_data.contains(model)) {
        logger.log("warning: no pricing data for model " + model);
        return;
    }
    const auto& model_pricing = pricing_data.at(model);

    double input_price = model_pricing.value("input_price_per_mille", 0.0);
    double output_price = model_pricing.value("output_price_per_mille", 0.0);
    double thinking_price = model_pricing.value("thinking_price_per_mille", output_price); // fallback

    double input_cost = (static_cast<double>(usage.input) / 1000.0) * input_price;
    double output_cost = (static_cast<double>(usage.output) / 1000.0) * output_price;
    double thinking_cost = (static_cast<double>(usage.thinking) / 1000.0) * thinking_price;
    double total_cost = input_cost + output_cost + thinking_cost;

    std::cout << "\n--- api cost summary ---\n"
              << "tokens: " << usage.input << " input, " << usage.output << " output, " << usage.thinking << " thinking\n"
              << "estimated cost:\n"
              << std::fixed << std::setprecision(6)
              << "  - input   : $" << input_cost << "\n"
              << "  - output  : $" << output_cost << "\n"
              << "  - thinking: $" << thinking_cost << "\n"
              << "total: $" << total_cost << std::endl;
}


// INPUT CONTENT: provider workarounds
std::string strip_markdown_fences(std::string text) {
    // remove optional language name after opening fence
    text = std::regex_replace(text, std::regex("^```[a-zA-Z]*\n"), "");
    if (text.size() >= 3 && text.substr(text.size() - 3) == "```") {
        text = text.substr(0, text.size() - 3);
    }
    // trim trailing newline if any
    if (!text.empty() && text.back() == '\n') {
       text.pop_back();
    }
    return text;
}

// INPUT CONTENT: providers
class LLMProvider {
public:
    virtual ~LLMProvider() = default;
    virtual LLMResponse transpile(const Config& config, const std::string& prompt, const json& pricing_data) = 0;
protected:
    int get_model_max_tokens(const std::string& model, const json& pricing_data) {
        if(pricing_data.contains(model) && pricing_data.at(model).contains("max_tokens")) {
            return pricing_data.at(model).at("max_tokens");
        }
        return 8192; // default
    }
};

// INPUT CONTENT: providers (google)
class GoogleProvider : public LLMProvider {
public:
    LLMResponse transpile(const Config& config, const std::string& prompt, const json& pricing_data) override {
        if (config.gemini_api_key.empty()) THROW_RUNTIME_ERROR("GEMINI_API_KEY is not set");

        std::string url = "https://generativelanguage.googleapis.com/v1beta/models/" + config.model + ":generateContent?key=" + config.gemini_api_key;
        
        json generation_config;
        if (config.max_tokens > 0) {
            // max_tokens is NOT sent to the google api
        }
        if (!config.seed.empty()) {
            generation_config["temperature"] = 0.0f;
        }

        json payload = {
            {"contents", {{{"role", "user"}, {"parts", {{{"text", prompt}}}}}}},
            {"generationConfig", generation_config}
        };

        logger.log("google payload:\n" + payload.dump(2));
        auto res = cpr::Post(cpr::Url{url},
                             cpr::Body{payload.dump()},
                             cpr::Header{{"Content-Type", "application/json"}},
                             cpr::Timeout{config.timeout * 1000});
        
        if (res.status_code != 200) {
            THROW_RUNTIME_ERROR("google api error " + std::to_string(res.status_code) + ": " + res.text);
        }
        
        json j_res = json::parse(res.text);
        logger.log("google response:\n" + j_res.dump(2));
        
        std::string content;
        if (j_res.contains("candidates") && !j_res["candidates"].empty()) {
            const auto& parts = j_res["candidates"][0]["content"]["parts"];
            for (const auto& part : parts) {
                if(part.contains("text")) content += part["text"].get<std::string>();
            }
        }
        
        LLMResponse llm_res;
        llm_res.content = content;
        if (j_res.contains("usageMetadata")) {
            const auto& usage = j_res["usageMetadata"];
            llm_res.usage.input = usage.value("promptTokenCount", 0);
            llm_res.usage.output = usage.value("candidatesTokenCount", 0);
            llm_res.usage.total = usage.value("totalTokenCount", 0);
            llm_res.usage.thinking = llm_res.usage.total - llm_res.usage.input - llm_res.usage.output;
        }

        return llm_res;
    }
};

// INPUT CONTENT: providers (anthropic)
class AnthropicProvider : public LLMProvider {
public:
    LLMResponse transpile(const Config& config, const std::string& prompt, const json& pricing_data) override {
        if (config.anthropic_api_key.empty()) THROW_RUNTIME_ERROR("ANTHROPIC_API_KEY is not set");
        std::string url = "https://api.anthropic.com/v1/messages";
        
        json payload = {
            {"model", config.model},
            {"max_tokens", config.max_tokens > 0 ? config.max_tokens : get_model_max_tokens(config.model, pricing_data)},
            {"messages", {{{"role", "user"}, {"content", prompt}}}},
        };
        if (config.thinking_budget > 0) {
            payload["max_tokens_to_sample_for_chain_of_thought"] = config.thinking_budget;
        }

        if (!config.seed.empty()) {
            payload["temperature"] = 0.0f;
        }

        logger.log("anthropic payload:\n" + payload.dump(2));
        auto res = cpr::Post(cpr::Url{url},
                             cpr::Body{payload.dump()},
                             cpr::Header{
                                 {"x-api-key", config.anthropic_api_key},
                                 {"anthropic-version", "2023-06-01"},
                                 {"content-type", "application/json"}
                             },
                             cpr::Timeout{config.timeout * 1000});

        if (res.status_code != 200) {
            THROW_RUNTIME_ERROR("anthropic api error " + std::to_string(res.status_code) + ": " + res.text);
        }

        json j_res = json::parse(res.text);
        logger.log("anthropic response:\n" + j_res.dump(2));

        std::string content;
        if (j_res.contains("content")) {
            for (const auto& block : j_res["content"]) {
                if (block["type"] == "text") {
                    content += block["text"].get<std::string>();
                }
            }
        }
        
        // workaround: strip <thinking> blocks
        const std::string thinking_start = "<thinking>";
        const std::string thinking_end = "</thinking>";
        if (content.rfind(thinking_start, 0) == 0) {
            size_t end_pos = content.find(thinking_end);
            if(end_pos != std::string::npos) {
                content = content.substr(end_pos + thinking_end.length());
                // also strip leading whitespace/newlines
                content.erase(0, content.find_first_not_of(" \n\r\t"));
            }
        }

        LLMResponse llm_res;
        llm_res.content = content;
        if (j_res.contains("usage")) {
            const auto& usage = j_res["usage"];
            llm_res.usage.input = usage.value("input_tokens", 0);
            llm_res.usage.output = usage.value("output_tokens", 0);
            if (usage.contains("thinking_tokens")) {
                llm_res.usage.thinking = usage.value("thinking_tokens", 0);
            }
        }
        return llm_res;
    }
};

// INPUT CONTENT: providers (openai)
class OpenAIProvider : public LLMProvider {
public:
    LLMResponse transpile(const Config& config, const std::string& prompt, const json& pricing_data) override {
        if (config.openai_api_key.empty()) THROW_RUNTIME_ERROR("OPENAI_API_KEY is not set");

        std::string url = config.openai_api_base.empty() ? "https://api.openai.com/v1/chat/completions" : config.openai_api_base + "/chat/completions";

        json payload = {
            {"model", config.model},
            {"messages", {{{"role", "user"}, {"content", prompt}}}},
        };

        if (config.max_tokens > 0) {
            payload["max_tokens"] = config.max_tokens;
        }
        if (!config.seed.empty()) {
            payload["seed"] = std::stoll(config.seed);
        }

        logger.log("openai payload:\n" + payload.dump(2));
        auto res = cpr::Post(cpr::Url{url},
                             cpr::Body{payload.dump()},
                             cpr::Header{
                                 {"Authorization", "Bearer " + config.openai_api_key},
                                 {"Content-Type", "application/json"}
                             },
                             cpr::Timeout{config.timeout * 1000});
        
        if (res.status_code != 200) {
            THROW_RUNTIME_ERROR("openai api error " + std::to_string(res.status_code) + ": " + res.text);
        }

        json j_res = json::parse(res.text);
        logger.log("openai response:\n" + j_res.dump(2));

        LLMResponse llm_res;
        llm_res.content = j_res["choices"][0]["message"]["content"];
        if (j_res.contains("usage")) {
            const auto& usage = j_res["usage"];
            llm_res.usage.input = usage.value("prompt_tokens", 0);
            llm_res.usage.output = usage.value("completion_tokens", 0);
            llm_res.usage.total = usage.value("total_tokens", 0);
            llm_res.usage.thinking = 0; // openai doesn't report this
        }

        return llm_res;
    }
};


std::unique_ptr<LLMProvider> create_provider(const std::string& provider_name) {
    if (provider_name == "google") return std::make_unique<GoogleProvider>();
    if (provider_name == "anthropic") return std::make_unique<AnthropicProvider>();
    if (provider_name == "openai") return std::make_unique<OpenAIProvider>();
    THROW_RUNTIME_ERROR("unknown provider: " + provider_name);
}

std::string read_file_or_throw(const fs::path& path) {
    if (!fs::exists(path)) {
        THROW_RUNTIME_ERROR("file not found: " + path.string());
    }
    std::ifstream file(path);
    if (!file.is_open()) {
        THROW_RUNTIME_ERROR("could not open file: " + path.string());
    }
    std::stringstream buffer;
    buffer << file.rdbuf();
    return buffer.str();
}

int main(int argc, char* argv[]) {
    try {
        // INPUT CONTENT: configuration
        Config config;
        load_configuration(argc, argv, config);
        
        // INPUT CONTENT: logging
        logger.init(config.logs_path);
        
        // INPUT CONTENT: --show-config
        if (config.show_config) {
            std::cout << get_redacted_config_string(config) << std::endl;
            return 0;
        }

        logger.log("consolidated configuration:\n" + get_redacted_config_string(config));

        // INPUT CONTENT: determinism
        if (config.seed.empty()) {
            std::random_device rd;
            std::mt19937_64 gen(rd());
            std::uniform_int_distribution<long long> distrib;
            config.seed = std::to_string(distrib(gen));
            logger.log("generated random seed: " + config.seed);
        }
        
        // INPUT CONTENT: resources
        auto pricing_json_path = find_resource(config, "pricing.json");
        auto lang_json_path = find_resource(config, "languages.json");
        auto prompt_tmpl_path = find_resource(config, "prompt.tmpl");
        
        json pricing_data = json::parse(read_file_or_throw(pricing_json_path));
        json lang_data = json::parse(read_file_or_throw(lang_json_path));
        std::string prompt_template = read_file_or_throw(prompt_tmpl_path);

        std::cout << "provider: " << config.provider << ", model: " << config.model << std::endl;
        std::cout << "transpiling '" << config.input_file << "' to '" << config.output_file << "' (" << get_target_language(config.output_file, lang_data) << ")" << std::endl;

        // INPUT CONTENT: prompt template
        std::map<std::string, std::string> template_vars;
        
        std::string gen_command = std::accumulate(config.generation_args.begin() + 1, config.generation_args.end(), config.generation_args.front(),
            [](const std::string& a, const std::string& b) { return a + " " + b; });
        
        // workaround for xml-like targets
        if(fs::path(config.output_file).extension() == ".svg") {
             size_t pos = gen_command.find("--");
             while(pos != std::string::npos) {
                 gen_command.replace(pos, 2, " -");
                 pos = gen_command.find("--", pos + 2);
             }
        }
        template_vars["generation_command"] = gen_command;

        template_vars["generation_config"] = get_redacted_config_string(config);
        template_vars["target_language"] = get_target_language(config.output_file, lang_data);
        template_vars["output_path"] = config.output_file;
        template_vars["english_content"] = read_file_or_throw(config.input_file);
        
        if (fs::exists(config.hacking_conventions_file)) {
            template_vars["hacking_conventions"] = read_file_or_throw(config.hacking_conventions_file);
        } else {
            template_vars["hacking_conventions"] = "";
        }
        
        std::stringstream context_ss;
        for (const auto& f_path_str : config.context_files) {
            fs::path f_path(f_path_str);
            if(fs::exists(f_path)) {
                context_ss << "### " << f_path.string() << "\n";
                context_ss << "```\n" << read_file_or_throw(f_path) << "\n```\n";
            }
        }
        template_vars["context_files"] = context_ss.str();

        std::string final_prompt = expand_template(prompt_template, template_vars);
        logger.log("--- expanded prompt ---\n" + final_prompt);
        
        // INPUT CONTENT: providers
        auto provider = create_provider(config.provider);
        LLMResponse response = provider->transpile(config, final_prompt, pricing_data);
        logger.log("--- llm response text ---\n" + response.content);
        
        // INPUT CONTENT: provider workarounds
        std::string final_code = strip_markdown_fences(response.content);
        
        std::ofstream out_file(config.output_file);
        if(!out_file.is_open()) THROW_RUNTIME_ERROR("failed to open output file for writing: " + config.output_file);
        out_file << final_code;
        out_file.close();

        std::cout << "successfully transpiled '" << config.output_file << "'" << std::endl;
        
        // INPUT CONTENT: token budgets
        print_cost_summary(config.model, response.usage, pricing_data);

    } catch (const std::exception& e) {
        std::cerr << "error: " << e.what() << std::endl;
        logger.log(std::string("FATAL ERROR: ") + e.what());
        return 1;
    }

    return 0;
}