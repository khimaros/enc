// notice: this file was automatically generated by https://github.com/khimaros/enc using the following invocation: ./enc-release src/enc.en -o src/enc.cpp --context-files ./.enc.env.example:CMakeLists.txt

// hacking: keep dependencies to a minimum and prefer standard library.
#include <algorithm>
#include <array>
#include <chrono>
#include <cstdlib>
#include <ctime>
#include <filesystem>
#include <fstream>
#include <iostream>
#include <map>
#include <memory>
#include <optional>
#include <regex>
#include <sstream>
#include <string>
#include <string_view>
#include <thread>
#include <vector>

// hacking: keep the code "dry" and organized into small functions.
#include <CLI/CLI.hpp>
#include <cpr/cpr.h>
#include <fmt/chrono.h>
#include <fmt/core.h>
#include <fmt/os.h>
#include <fmt/ranges.h>
#include <nlohmann/json.hpp>
#include <pwd.h>
#include <unistd.h>
#include <sys/types.h>

using json = nlohmann::json;
namespace fs = std::filesystem;

// configuration: hardcoded fallbacks in the application based on those in ./.enc.env.example
namespace defaults {
    const std::string provider = "google";
    const std::string model_google = "gemini-2.5-pro";
    const std::string model_anthropic = "claude-sonnet-4-20250514";
    const std::string model_openai = "gpt-5-2025-08-07";
    const std::string openai_base = "https://api.openai.com/v1";
    const std::string hacking_conventions = "./HACKING.md";
    const std::string logs_path = "./log/";
    // resources: ./res/:${XDG_DATA_HOME}/enc/res/
    const std::string resources_path = "./res/:${XDG_DATA_HOME}/enc/res/"; 
    const int test_iterations = 3;
    const int timeout = 1800;
}

// resources: embedded fallbacks via compile-time macros
const std::string embedded_prompt_tmpl = R"(Context files:
{{context_files}}

Hacking conventions:
{{hacking_conventions}}

Target language: {{target_language}}

Command: {{generation_command}}

Objective:
Transpile the following English description into {{target_language}}.
{{english_content}}
)";

const std::string embedded_languages_json = R"({
  ".c": "c", ".h": "c", ".cpp": "c++", ".hpp": "c++", ".cc": "c++", ".rs": "rust", 
  ".py": "python", ".js": "javascript", ".ts": "typescript", ".go": "go", 
  ".java": "java", ".rb": "ruby", ".sh": "shell", ".md": "markdown"
})";

const std::string embedded_pricing_json = "{}";

struct Config {
    std::string input_file;
    std::string output_file;
    bool show_config = false;
    
    std::string provider;
    std::string model;
    std::optional<int> max_tokens;
    std::optional<int> thinking_budget;
    std::optional<int64_t> seed;
    std::string hacking_conventions;
    int timeout = defaults::timeout;
    std::string context_files;
    
    std::string gemini_api_key;
    std::string anthropic_api_key;
    std::string openai_api_key;
    std::string openai_api_base;
    
    std::string logs_path;
    std::string resources_path;
    
    std::string test_command;
    int test_iterations = defaults::test_iterations;

    // derived
    std::vector<std::string> raw_args;
};

struct Stats {
    int input_tokens = 0;
    int output_tokens = 0;
    int thinking_tokens = 0;
    double input_cost = 0.0;
    double output_cost = 0.0;
    double thinking_cost = 0.0;
    
    void add(int in, int out, int think, double in_p, double out_p) {
        input_tokens += in;
        output_tokens += out;
        thinking_tokens += think;
        input_cost += in * in_p;
        // if thinking token cost is unknown, price as output tokens.
        output_cost += (out) * out_p;
        thinking_cost += think * out_p; 
    }
};

// logging: create timestamped log file, write config
std::string g_log_file_path;
void log_message(const std::string& msg) {
    if (g_log_file_path.empty()) return;
    std::ofstream log(g_log_file_path, std::ios::app);
    if (log.is_open()) {
        log << msg << "\n";
    }
}

// hacking: small functions, dry
std::string get_env(const std::string& key, const std::string& default_val = "") {
    const char* val = std::getenv(key.c_str());
    return val ? std::string(val) : default_val;
}

std::string get_home_dir() {
    std::string home = get_env("HOME");
    if (home.empty()) {
        struct passwd* pw = getpwuid(getuid());
        if (pw) home = pw->pw_dir;
    }
    return home;
}

std::string get_xdg_data_home() {
    std::string xdg = get_env("XDG_DATA_HOME");
    if (xdg.empty()) {
        xdg = get_home_dir() + "/.local/share";
    }
    return xdg;
}

// configuration: .env files are read manually
std::string trim(const std::string& str) {
    size_t first = str.find_first_not_of(" \t\r\n");
    if (std::string::npos == first) return "";
    size_t last = str.find_last_not_of(" \t\r\n");
    return str.substr(first, (last - first + 1));
}

std::map<std::string, std::string> parse_env_file(const std::string& path) {
    std::map<std::string, std::string> env;
    std::ifstream file(path);
    if (!file.is_open()) return env;

    std::string line;
    while (std::getline(file, line)) {
        std::string trimmed_line = trim(line);
        if (trimmed_line.empty() || trimmed_line[0] == '#') continue;

        auto pos = line.find('=');
        if (pos != std::string::npos) {
            std::string key = trim(line.substr(0, pos));
            std::string val = trim(line.substr(pos + 1));
            
            // remove quotes if present
            if (val.size() >= 2 && val.front() == '"' && val.back() == '"') {
                val = val.substr(1, val.size() - 2);
            }
            env[key] = val;
        }
    }
    return env;
}

// configuration: merging rules (CRITICAL)
// empty values MUST NOT override non-empty values from lower-precedence sources
void merge_string(std::string& dest, const std::string& src) {
    if (!src.empty()) dest = src;
}

void merge_int(int& dest, int src, int default_val) {
    if (src != default_val) dest = src; 
}

void merge_optional_int(std::optional<int>& dest, std::optional<int> src) {
    if (src.has_value()) dest = src;
}

void merge_optional_i64(std::optional<int64_t>& dest, std::optional<int64_t> src) {
    if (src.has_value()) dest = src;
}

// configuration: layered configuration
// precedence: flags > env > ./.enc.env > ~/.enc.env > defaults
Config load_configuration(int argc, char** argv) {
    Config cfg;
    
    // 1. Defaults
    cfg.provider = defaults::provider;
    cfg.hacking_conventions = defaults::hacking_conventions;
    cfg.logs_path = defaults::logs_path;
    cfg.resources_path = defaults::resources_path;
    cfg.openai_api_base = defaults::openai_base;

    // Helper to apply a map to config
    auto apply_map = [&](const std::map<std::string, std::string>& m) {
        auto get_val = [&](const std::string& k) {
            auto it = m.find(k);
            return (it != m.end()) ? it->second : "";
        };
        
        merge_string(cfg.provider, get_val("PROVIDER"));
        merge_string(cfg.model, get_val("MODEL"));
        
        std::string max_t = get_val("MAX_TOKENS");
        if (!max_t.empty()) cfg.max_tokens = std::stoi(max_t);
        
        std::string think_b = get_val("THINKING_BUDGET");
        if (!think_b.empty()) cfg.thinking_budget = std::stoi(think_b);

        std::string seed_s = get_val("SEED");
        if (!seed_s.empty()) cfg.seed = std::stoll(seed_s);

        merge_string(cfg.hacking_conventions, get_val("HACKING_CONVENTIONS"));
        
        std::string timeout_s = get_val("TIMEOUT");
        if (!timeout_s.empty()) cfg.timeout = std::stoi(timeout_s);

        merge_string(cfg.context_files, get_val("CONTEXT_FILES"));
        merge_string(cfg.gemini_api_key, get_val("GEMINI_API_KEY"));
        merge_string(cfg.anthropic_api_key, get_val("ANTHROPIC_API_KEY"));
        merge_string(cfg.openai_api_key, get_val("OPENAI_API_KEY"));
        merge_string(cfg.openai_api_base, get_val("OPENAI_API_BASE"));
        merge_string(cfg.logs_path, get_val("LOGS_PATH"));
        merge_string(cfg.resources_path, get_val("RESOURCES_PATH"));
        merge_string(cfg.test_command, get_val("TEST_COMMAND"));
        
        std::string iter_s = get_val("TEST_ITERATIONS");
        if (!iter_s.empty()) cfg.test_iterations = std::stoi(iter_s);
    };

    // 2. Home config
    apply_map(parse_env_file(get_home_dir() + "/.enc.env"));

    // 3. Working dir config
    apply_map(parse_env_file("./.enc.env"));

    // 4. Env vars
    std::map<std::string, std::string> env_vars;
    auto check_env = [&](const std::string& k) {
        std::string v = get_env(k);
        if(!v.empty()) env_vars[k] = v;
    };
    for(const auto& k : {"PROVIDER", "MODEL", "MAX_TOKENS", "THINKING_BUDGET", "SEED", "HACKING_CONVENTIONS", "TIMEOUT", "CONTEXT_FILES", "GEMINI_API_KEY", "ANTHROPIC_API_KEY", "OPENAI_API_KEY", "OPENAI_API_BASE", "LOGS_PATH", "RESOURCES_PATH", "TEST_COMMAND", "TEST_ITERATIONS"}) {
        check_env(k);
    }
    apply_map(env_vars);

    // 5. Command line flags
    // Capture flags into temporary structure, then merge
    Config cli_cfg;
    CLI::App app{"enc - transpiler"};
    
    // command line: enc <INPUT_FILE> -o <OUTPUT_FILE> [OPTIONS]
    app.add_option("input_file", cli_cfg.input_file, "Input file");
    app.add_option("-o,--output", cli_cfg.output_file, "Output file");
    app.add_flag("--show-config", cli_cfg.show_config, "Show configuration");
    
    // config flags
    auto* f_prov = app.add_option("--provider", cli_cfg.provider);
    auto* f_mod = app.add_option("--model", cli_cfg.model);
    auto* f_tok = app.add_option("--max-tokens", cli_cfg.max_tokens);
    auto* f_think = app.add_option("--thinking-budget", cli_cfg.thinking_budget);
    auto* f_seed = app.add_option("--seed", cli_cfg.seed);
    auto* f_hack = app.add_option("--hacking-conventions", cli_cfg.hacking_conventions);
    auto* f_to = app.add_option("--timeout", cli_cfg.timeout);
    auto* f_ctx = app.add_option("--context-files", cli_cfg.context_files);
    auto* f_gem = app.add_option("--gemini-api-key", cli_cfg.gemini_api_key);
    auto* f_ant = app.add_option("--anthropic-api-key", cli_cfg.anthropic_api_key);
    auto* f_oai = app.add_option("--openai-api-key", cli_cfg.openai_api_key);
    auto* f_base = app.add_option("--openai-api-base", cli_cfg.openai_api_base);
    auto* f_log = app.add_option("--logs-path", cli_cfg.logs_path);
    auto* f_res = app.add_option("--resources-path", cli_cfg.resources_path);
    auto* f_test = app.add_option("--test-command", cli_cfg.test_command);
    auto* f_iter = app.add_option("--test-iterations", cli_cfg.test_iterations);

    try {
        app.parse(argc, argv);
    } catch (const CLI::ParseError& e) {
        std::exit(app.exit(e));
    }
    
    // Merge CLI if set
    if (!cli_cfg.input_file.empty()) cfg.input_file = cli_cfg.input_file;
    if (app.count("--output")) cfg.output_file = cli_cfg.output_file; // can be empty if not provided, check later
    if (cli_cfg.show_config) cfg.show_config = true;

    if (f_prov->count()) merge_string(cfg.provider, cli_cfg.provider);
    if (f_mod->count()) merge_string(cfg.model, cli_cfg.model);
    if (f_tok->count()) merge_optional_int(cfg.max_tokens, cli_cfg.max_tokens);
    if (f_think->count()) merge_optional_int(cfg.thinking_budget, cli_cfg.thinking_budget);
    if (f_seed->count()) merge_optional_i64(cfg.seed, cli_cfg.seed);
    if (f_hack->count()) merge_string(cfg.hacking_conventions, cli_cfg.hacking_conventions);
    if (f_to->count()) cfg.timeout = cli_cfg.timeout; // int defaults match
    if (f_ctx->count()) merge_string(cfg.context_files, cli_cfg.context_files);
    if (f_gem->count()) merge_string(cfg.gemini_api_key, cli_cfg.gemini_api_key);
    if (f_ant->count()) merge_string(cfg.anthropic_api_key, cli_cfg.anthropic_api_key);
    if (f_oai->count()) merge_string(cfg.openai_api_key, cli_cfg.openai_api_key);
    if (f_base->count()) merge_string(cfg.openai_api_base, cli_cfg.openai_api_base);
    if (f_log->count()) merge_string(cfg.logs_path, cli_cfg.logs_path);
    if (f_res->count()) merge_string(cfg.resources_path, cli_cfg.resources_path);
    if (f_test->count()) merge_string(cfg.test_command, cli_cfg.test_command);
    if (f_iter->count()) cfg.test_iterations = cli_cfg.test_iterations;

    // validation: check required API key for selected provider exists
    if (!cfg.show_config) {
        if (cfg.input_file.empty()) {
             std::cerr << "error: input file is required\n";
             std::exit(1);
        }
        if (cfg.output_file.empty()) {
             std::cerr << "error: output file is required (-o)\n";
             std::exit(1);
        }
    }

    return cfg;
}

// configuration: consolidated configuration to JSON (redacted)
json config_to_json(const Config& cfg) {
    json j;
    j["provider"] = cfg.provider;
    if (!cfg.model.empty()) j["model"] = cfg.model;
    if (cfg.max_tokens.has_value()) j["max_tokens"] = cfg.max_tokens.value();
    if (cfg.thinking_budget.has_value()) j["thinking_budget"] = cfg.thinking_budget.value();
    if (cfg.seed.has_value()) j["seed"] = cfg.seed.value();
    if (!cfg.hacking_conventions.empty()) j["hacking_conventions"] = cfg.hacking_conventions;
    j["timeout"] = cfg.timeout;
    if (!cfg.context_files.empty()) j["context_files"] = cfg.context_files;
    
    // security: API keys are ALWAYS redacted to "[REDACTED]"
    j["gemini_api_key"] = "[REDACTED]";
    j["anthropic_api_key"] = "[REDACTED]";
    j["openai_api_key"] = "[REDACTED]";
    
    if (!cfg.openai_api_base.empty()) j["openai_api_base"] = cfg.openai_api_base;
    if (!cfg.logs_path.empty()) j["logs_path"] = cfg.logs_path;
    if (!cfg.resources_path.empty()) j["resources_path"] = cfg.resources_path;
    if (!cfg.test_command.empty()) j["test_command"] = cfg.test_command;
    j["test_iterations"] = cfg.test_iterations;
    
    // sorting: output keys lexically sorted (nlohmann::json does this by default for objects)
    return j;
}

std::string read_file(const std::string& path) {
    std::ifstream t(path);
    if (!t.is_open()) return "";
    std::stringstream buffer;
    buffer << t.rdbuf();
    return buffer.str();
}

std::string string_replace(std::string str, const std::string& from, const std::string& to) {
    size_t start_pos = 0;
    while((start_pos = str.find(from, start_pos)) != std::string::npos) {
        str.replace(start_pos, from.length(), to);
        start_pos += to.length();
    }
    return str;
}

// resources: find prompt.tmpl, languages.json, pricing.json
std::string load_resource(const Config& cfg, const std::string& filename, const std::string& fallback) {
    std::string res_path_expanded = string_replace(cfg.resources_path, "${XDG_DATA_HOME}", get_xdg_data_home());
    
    std::stringstream ss(res_path_expanded);
    std::string segment;
    while(std::getline(ss, segment, ':')) {
        if(segment.empty()) continue;
        fs::path p = fs::path(segment) / filename;
        if(fs::exists(p)) {
            return read_file(p.string());
        }
    }
    return fallback;
}

// providers: enc supports three LLM providers
struct ProviderSpec {
    std::string name;
    std::string model;
    double in_cost = 0;
    double out_cost = 0;
};

ProviderSpec get_provider_spec(const Config& cfg, const json& pricing) {
    ProviderSpec spec;
    spec.name = cfg.provider;
    
    // default models
    if (cfg.model.empty()) {
        if (cfg.provider == "google") spec.model = defaults::model_google;
        else if (cfg.provider == "anthropic") spec.model = defaults::model_anthropic;
        else if (cfg.provider == "openai") spec.model = defaults::model_openai;
    } else {
        spec.model = cfg.model;
    }

    std::string key = cfg.provider + "/" + spec.model;
    // pricing data
    if (pricing.contains(key)) {
        spec.in_cost = pricing[key].value("input_cost_per_token", 0.0);
        spec.out_cost = pricing[key].value("output_cost_per_token", 0.0);
    }
    
    return spec;
}

// output: strip markdown code fences from first/last lines only
std::string clean_response(std::string resp) {
    std::stringstream ss(resp);
    std::string line;
    std::vector<std::string> lines;
    while(std::getline(ss, line)) {
        lines.push_back(line);
    }
    
    if (!lines.empty()) {
        if (lines.front().rfind("```", 0) == 0) lines.erase(lines.begin());
    }
    if (!lines.empty()) {
        if (lines.back().rfind("```", 0) == 0) lines.pop_back();
    }
    
    std::string out;
    for (size_t i = 0; i < lines.size(); ++i) {
        out += lines[i] + "\n";
    }
    return out;
}

// execution flow: call API
// Handle specific provider logic
void call_api(const Config& cfg, const std::string& prompt, Stats& stats, std::string& output_code) {
    json pricing_json = json::parse(load_resource(cfg, "pricing.json", embedded_pricing_json));
    ProviderSpec spec = get_provider_spec(cfg, pricing_json);
    
    std::cout << fmt::format("calling api (provider: {}, model: {})...", spec.name, spec.model) << std::endl;
    
    auto start = std::chrono::high_resolution_clock::now();
    int http_code = 0;
    std::string response_body;
    int in_tok = 0, out_tok = 0, think_tok = 0;
    
    if (spec.name == "google") {
        // google request format
        json req;
        req["contents"] = json::array({ {{"parts", json::array({ {{"text", prompt}} })}} });
        
        // google-specific behavior: seed via temp 0
        json generation_config;
        if (cfg.seed.has_value()) {
            generation_config["temperature"] = 0;
        }
        if (cfg.max_tokens.has_value()) {
            generation_config["maxOutputTokens"] = cfg.max_tokens.value();
        }
        if (!generation_config.empty()) req["generationConfig"] = generation_config;

        std::string url = fmt::format("https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent?key={}", spec.model, cfg.gemini_api_key);
        
        cpr::Response r = cpr::Post(cpr::Url{url}, cpr::Body{req.dump()}, cpr::Header{{"Content-Type", "application/json"}}, cpr::Timeout{cfg.timeout * 1000});
        
        http_code = r.status_code;
        response_body = r.text;
        
        if (http_code == 200) {
            json resp = json::parse(response_body);
            // responses may contain multiple text parts
            std::string content;
            if (resp.contains("candidates") && !resp["candidates"].empty()) {
                auto cand = resp["candidates"][0];
                if (cand.contains("content") && cand["content"].contains("parts")) {
                    for (const auto& part : cand["content"]["parts"]) {
                        if (part.contains("text")) content += part["text"];
                    }
                }
                
                // thinking tokens: usageMetadata.thinkingTokenCount
                if (resp.contains("usageMetadata")) {
                    in_tok = resp["usageMetadata"].value("promptTokenCount", 0);
                    out_tok = resp["usageMetadata"].value("candidatesTokenCount", 0); // Total candidates
                    int total_tok = resp["usageMetadata"].value("totalTokenCount", 0);

                    // If thinkingTokenCount exists, use it. Else derive from total - prompt - candidates
                    if (resp["usageMetadata"].contains("thinkingTokenCount")) {
                        think_tok = resp["usageMetadata"]["thinkingTokenCount"];
                    } else {
                        think_tok = total_tok - in_tok - out_tok;
                    }
                    if (think_tok < 0) think_tok = 0;
                    
                    // Note: For Google, if fallback is (total - prompt - candidates), then candidates excludes thinking.
                    // So we do NOT subtract think_tok from out_tok.
                }
            }
            output_code = content;
        }

    } else if (spec.name == "anthropic") {
        // anthropic request format
        json req;
        req["model"] = spec.model;
        req["messages"] = json::array({ {{"role", "user"}, {"content", prompt}} });
        req["stream"] = true;
        req["max_tokens"] = cfg.max_tokens.value_or(8192);

        if (cfg.thinking_budget.has_value() && cfg.thinking_budget.value() > 0) {
            req["thinking"] = {{"type", "enabled"}, {"budget_tokens", cfg.thinking_budget.value()}};
            req["temperature"] = 1;
            // max_tokens MUST be greater than thinking_budget
            if (req["max_tokens"].get<int>() <= cfg.thinking_budget.value()) {
                 req["max_tokens"] = cfg.thinking_budget.value() + 4096; // bump it
            }
        } else if (cfg.seed.has_value()) {
            req["temperature"] = 0;
        }

        std::string full_content;
        std::string thinking_content;
        
        // streaming handling
        cpr::Response r = cpr::Post(
            cpr::Url{"https://api.anthropic.com/v1/messages"},
            cpr::Header{
                {"x-api-key", cfg.anthropic_api_key},
                {"anthropic-version", "2023-06-01"},
                {"Content-Type", "application/json"}
            },
            cpr::Body{req.dump()},
            cpr::Timeout{cfg.timeout * 1000},
            cpr::WriteCallback([&](const std::string_view& data, intptr_t) {
                // Parse SSE
                std::stringstream ss((std::string)data);
                std::string line;
                while(std::getline(ss, line)) {
                    if (line.find("data: ") == 0) {
                        std::string json_str = line.substr(6);
                        if (json_str == "[DONE]") continue;
                        try {
                            json event = json::parse(json_str);
                            std::string type = event.value("type", "");
                            if (type == "message_start") {
                                if (event.contains("message") && event["message"].contains("usage")) {
                                    in_tok = event["message"]["usage"].value("input_tokens", 0);
                                }
                            } else if (type == "content_block_delta") {
                                if (event.contains("delta")) {
                                    if (event["delta"].value("type", "") == "text_delta") {
                                        full_content += event["delta"].value("text", "");
                                    } else if (event["delta"].value("type", "") == "thinking_delta") {
                                        thinking_content += event["delta"].value("thinking", "");
                                    }
                                }
                            } else if (type == "message_delta") {
                                if (event.contains("usage")) {
                                    out_tok = event["usage"].value("output_tokens", 0);
                                }
                            }
                        } catch (...) {}
                    }
                    response_body += line + "\n"; // capture raw for error reporting
                }
                return true;
            })
        );
        
        http_code = r.status_code;
        if (http_code == 200) {
            output_code = full_content;
            // thinking tokens: estimate
            think_tok = thinking_content.length() / 4;
            // Subtract estimated thinking from total output tokens
            if (out_tok >= think_tok) out_tok -= think_tok;
            else out_tok = 0;
        } else {
             if (response_body.empty()) response_body = r.text;
        }

    } else if (spec.name == "openai") {
        json req;
        req["model"] = spec.model;
        req["messages"] = json::array({ {{"role", "user"}, {"content", prompt}} });
        if (cfg.seed.has_value()) req["seed"] = cfg.seed.value();
        
        // openai-specific behavior
        bool is_official = (cfg.openai_api_base.find("api.openai.com") != std::string::npos);
        if (is_official) {
             req["max_completion_tokens"] = cfg.max_tokens.value_or(8192);
        } else {
             req["max_tokens"] = cfg.max_tokens.value_or(8192);
        }

        std::string url = cfg.openai_api_base + "/chat/completions";
        cpr::Response r = cpr::Post(
            cpr::Url{url},
            cpr::Header{{"Authorization", "Bearer " + cfg.openai_api_key}, {"Content-Type", "application/json"}},
            cpr::Body{req.dump()},
            cpr::Timeout{cfg.timeout * 1000}
        );
        
        http_code = r.status_code;
        response_body = r.text;
        
        if (http_code == 200) {
            json resp = json::parse(response_body);
            if (resp.contains("choices") && !resp["choices"].empty()) {
                output_code = resp["choices"][0]["message"].value("content", "");
            }
            if (resp.contains("usage")) {
                in_tok = resp["usage"].value("prompt_tokens", 0);
                out_tok = resp["usage"].value("completion_tokens", 0);
                
                // thinking tokens logic
                if (resp["usage"].contains("completion_tokens_details") && resp["usage"]["completion_tokens_details"].contains("reasoning_tokens")) {
                    think_tok = resp["usage"]["completion_tokens_details"]["reasoning_tokens"];
                } else if (resp["usage"].contains("reasoning_tokens")) {
                    think_tok = resp["usage"]["reasoning_tokens"];
                } else if (resp["usage"].contains("thinking_tokens")) {
                    think_tok = resp["usage"]["thinking_tokens"];
                }
                
                // Subtract thinking from completion tokens (since OpenAI completion includes reasoning)
                if (think_tok > 0 && out_tok >= think_tok) {
                    out_tok -= think_tok;
                }
            }
        }
    }

    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> elapsed = end - start;
    std::cout << fmt::format("api call completed with response code {} after {:.2f}s", http_code, elapsed.count()) << std::endl;
    
    // logging
    log_message(fmt::format("Response Body:\n{}", response_body));

    // error handling: API errors
    if (http_code < 200 || http_code >= 300) {
        std::cerr << fmt::format("error: {} error: {}", spec.name, response_body) << std::endl;
        stats.add(in_tok, out_tok, think_tok, spec.in_cost, spec.out_cost);
        // summary and exit
        std::cout << "\n--- api request summary ---\n";
        if (stats.thinking_tokens > 0) {
            std::cout << fmt::format("tokens: {} input, {} output, {} thinking", stats.input_tokens, stats.output_tokens, stats.thinking_tokens) << std::endl;
        } else {
            std::cout << fmt::format("tokens: {} input, {} output", stats.input_tokens, stats.output_tokens) << std::endl;
        }
        std::cout << "estimated cost:\n";
        std::cout << fmt::format("  - input: ${:.6f}", stats.input_cost) << std::endl;
        std::cout << fmt::format("  - output: ${:.6f}", stats.output_cost) << std::endl;
        if (stats.thinking_tokens > 0) {
            std::cout << fmt::format("  - thinking: ${:.6f}", stats.thinking_cost) << std::endl;
        }
        std::cout << fmt::format("total: ${:.6f}", stats.input_cost + stats.output_cost + stats.thinking_cost) << std::endl;
        std::cout << fmt::format("elapsed time: {:.2f}s", elapsed.count()) << std::endl;
        exit(1); 
    }
    
    stats.add(in_tok, out_tok, think_tok, spec.in_cost, spec.out_cost);
}

// test iteration mode: run command, capture output
struct ExecResult {
    int code;
    std::string out;
    std::string err;
};

// restricted environment: PATH, RUSTUP_HOME, CARGO_HOME
ExecResult run_command(const std::string& cmd) {
    std::string env_prefix = "env -i";
    for (const auto& var : {"PATH", "RUSTUP_HOME", "CARGO_HOME"}) {
        const char* val = std::getenv(var);
        if (val) {
            env_prefix += fmt::format(" {}='{}'", var, val);
        }
    }
    
    // use sh -c manually since env -i takes command
    // "env -i VAR=VAL sh -c 'original command 2>&1'"
    std::string full_cmd = fmt::format("{} sh -c \"{} 2>&1\"", env_prefix, cmd);
    
    std::array<char, 128> buffer;
    std::string result;
    FILE* pipe = popen(full_cmd.c_str(), "r");
    if (!pipe) return {1, "", "popen failed"};
    while (fgets(buffer.data(), buffer.size(), pipe) != nullptr) {
        result += buffer.data();
    }
    int rc = pclose(pipe);
    // WEXITSTATUS for posix
    int exit_code = WEXITSTATUS(rc);
    return {exit_code, result, ""}; // stderr merged
}

int main(int argc, char** argv) {
    // 1. parse arguments & 2. load configuration
    Config cfg = load_configuration(argc, argv);
    
    // --show-config
    if (cfg.show_config) {
        std::cout << config_to_json(cfg).dump(2) << std::endl;
        return 0;
    }

    // 3. validate configuration (already partially done, check key for selected provider)
    if ((cfg.provider == "google" && cfg.gemini_api_key.empty()) ||
        (cfg.provider == "anthropic" && cfg.anthropic_api_key.empty()) ||
        (cfg.provider == "openai" && cfg.openai_api_key.empty())) {
        std::cerr << "error: api key for provider '" << cfg.provider << "' is missing" << std::endl;
        return 1;
    }

    // 4. initialize logging
    {
        auto now = std::chrono::system_clock::now();
        auto in_time_t = std::chrono::system_clock::to_time_t(now);
        std::stringstream ss;
        ss << std::put_time(std::localtime(&in_time_t), "%Y%m%d_%H%M%S");
        fs::create_directories(cfg.logs_path);
        g_log_file_path = (fs::path(cfg.logs_path) / (ss.str() + ".log")).string();
        std::cout << "debug log path: " << g_log_file_path << std::endl;
        log_message(config_to_json(cfg).dump(2));
    }

    Stats stats;
    auto start_total = std::chrono::high_resolution_clock::now();

    // 5. load resources
    std::string prompt_tmpl = load_resource(cfg, "prompt.tmpl", embedded_prompt_tmpl);
    json lang_map = json::parse(load_resource(cfg, "languages.json", embedded_languages_json));
    
    // 6. read input
    std::string input_content = read_file(cfg.input_file);
    if (input_content.empty()) {
         std::cerr << "error: input file is empty or cannot be read" << std::endl;
         return 1;
    }

    // code generation: language detection
    std::string target_lang = "unknown";
    std::string ext = fs::path(cfg.output_file).extension().string();
    std::string fname = fs::path(cfg.output_file).filename().string();
    if (lang_map.contains(ext)) target_lang = lang_map[ext];
    else if (lang_map.contains(fname)) target_lang = lang_map[fname];
    else if (!ext.empty()) target_lang = ext; // fallback to extension

    // 7. expand prompt
    // Prepare variables
    std::string generation_cmd;
    for (int i = 0; i < argc; ++i) generation_cmd += std::string(argv[i]) + " ";
    
    std::string hacking_content;
    if (!cfg.hacking_conventions.empty()) hacking_content = read_file(cfg.hacking_conventions);
    
    std::string ctx_content;
    std::stringstream ctx_ss(cfg.context_files);
    std::string ctx_path;
    while(std::getline(ctx_ss, ctx_path, ':')) {
        if (ctx_path.empty()) continue;
        std::string content = read_file(ctx_path);
        if (content.empty()) {
            std::cerr << "error: context file '" << ctx_path << "' cannot be read" << std::endl;
            return 1;
        }
        ctx_content += "### " + ctx_path + "\n\n```\n" + content + "\n```\n\n";
    }
    
    // Simple substitution
    std::string prompt = prompt_tmpl;
    prompt = string_replace(prompt, "{{generation_command}}", generation_cmd);
    prompt = string_replace(prompt, "{{generation_config}}", config_to_json(cfg).dump());
    prompt = string_replace(prompt, "{{target_language}}", target_lang);
    prompt = string_replace(prompt, "{{output_path}}", cfg.output_file);
    prompt = string_replace(prompt, "{{english_content}}", input_content);
    prompt = string_replace(prompt, "{{hacking_conventions}}", hacking_content);
    prompt = string_replace(prompt, "{{context_files}}", ctx_content);
    
    log_message("Prompt:\n" + prompt);

    std::cout << fmt::format("transpiling '{}' to '{}' ({})", cfg.input_file, cfg.output_file, target_lang) << std::endl;

    // Test Loop
    int iteration = 0;
    int max_attempts = (cfg.test_command.empty()) ? 1 : cfg.test_iterations;
    if (max_attempts == -1) max_attempts = 999999;

    bool success = false;
    std::string current_prompt = prompt;

    while (iteration < max_attempts) {
        iteration++;
        
        // 8. call API
        std::string output_code;
        call_api(cfg, current_prompt, stats, output_code);
        
        // 9. process response
        output_code = clean_response(output_code);
        if (output_code.empty()) {
            std::cerr << "error: received empty response" << std::endl;
            if (iteration < max_attempts) continue;
            else break;
        }

        // 10. write output
        std::ofstream out(cfg.output_file);
        out << output_code;
        out.close();

        // 11. run tests
        if (!cfg.test_command.empty()) {
            std::cout << fmt::format("executing test command (`{}`), attempt {}/{}", cfg.test_command, iteration, max_attempts) << std::endl;
            ExecResult res = run_command(cfg.test_command);
            if (res.code == 0) {
                std::cout << "test command succeeded!" << std::endl;
                success = true;
                break;
            } else {
                std::cout << "test command failed, see log for details" << std::endl;
                log_message(fmt::format("Test Failed:\nStdout/Stderr: {}", res.out));
                
                // update prompt for retry
                current_prompt = "The previous generation failed the test.\n\nCode:\n```\n" + output_code + "\n```\n\nTest Command: " + cfg.test_command + "\nExit Code: " + std::to_string(res.code) + "\nOutput:\n" + res.out + "\n\nPlease fix the code.";
            }
        } else {
            success = true;
            break;
        }
    }

    if (success) {
        std::cout << fmt::format("successfully transpiled '{}'", cfg.output_file) << std::endl;
    } else {
        std::cerr << "failed to generate correct code after " << iteration << " attempts" << std::endl;
    }

    // 12. display summary
    auto end_total = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> elapsed_total = end_total - start_total;

    std::cout << "\n--- api request summary ---\n";
    if (stats.thinking_tokens > 0) {
        std::cout << fmt::format("tokens: {} input, {} output, {} thinking", stats.input_tokens, stats.output_tokens, stats.thinking_tokens) << std::endl;
    } else {
        std::cout << fmt::format("tokens: {} input, {} output", stats.input_tokens, stats.output_tokens) << std::endl;
    }
    
    std::cout << "estimated cost:\n";
    std::cout << fmt::format("  - input: ${:.6f}", stats.input_cost) << std::endl;
    std::cout << fmt::format("  - output: ${:.6f}", stats.output_cost) << std::endl;
    if (stats.thinking_tokens > 0) {
        std::cout << fmt::format("  - thinking: ${:.6f}", stats.thinking_cost) << std::endl;
    }
    std::cout << fmt::format("total: ${:.6f}", stats.input_cost + stats.output_cost + stats.thinking_cost) << std::endl;
    std::cout << fmt::format("elapsed time: {:.2f}s", elapsed_total.count()) << std::endl;

    return success ? 0 : 1;
}