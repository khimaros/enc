this script downloads and converts pricing data from litellm

the source url is: https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json

with python, json5 should be used to parse the source data as exponent forms are used for some of the floating point values.

the base JSON is an object and the keys are strings and each value is another object.

those objects have many keys, but will be filtered to have only the following ones:

- max_tokens
- max_input_tokens
- input_cost_per_token
- input_cost_per_token_above_200k_tokens
- output_cost_per_token
- output_cost_per_token_above_200k_tokens
- source

items which are missing `input_cost_per_token` or `output_cost_per_token` will be skipped.

the key named "sample_spec" should be skipped.

the source data looks like:

```json
{
    "gemini/gemini-2.5-pro": {
        "max_tokens": 65535,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535,
        "max_images_per_prompt": 3000,
        "max_videos_per_prompt": 10,
        "max_video_length": 1,
        "max_audio_length_hours": 8.4,
        "max_audio_per_prompt": 1,
        "max_pdf_size_mb": 30,
        "input_cost_per_token": 1.25e-06,
        "input_cost_per_token_above_200k_tokens": 2.5e-06,
        "output_cost_per_token": 1e-05,
        "output_cost_per_token_above_200k_tokens": 1.5e-05,
        "litellm_provider": "gemini",
        "mode": "chat",
        "rpm": 2000,
        "tpm": 800000,
        "supports_system_messages": true,
        "supports_function_calling": true,
        "supports_vision": true,
        "supports_audio_input": true,
        "supports_video_input": true,
        "supports_pdf_input": true,
        "supports_response_schema": true,
        "supports_tool_choice": true,
        "supports_reasoning": true,
        "supported_endpoints": [
            "/v1/chat/completions",
            "/v1/completions"
        ],
        "supported_modalities": [
            "text",
            "image",
            "audio",
            "video"
        ],
        "supported_output_modalities": [
            "text"
        ],
        "source": "https://cloud.google.com/vertex-ai/generative-ai/pricing",
        "supports_web_search": true,
        "cache_read_input_token_cost": 3.125e-07,
        "supports_prompt_caching": true
    },
    // ... more models follow ...
}
```

not all objects contain all of the keys listed above.

the output should look like:

```json
{
    "google/gemini-2.5-pro": {
        "max_tokens": 65536,
        "max_input_tokens": 1048576,
        "input_cost_per_token": 0.00000125,
        "input_cost_per_token_above_200k_tokens": 0.0000025,
        "output_cost_per_token": 0.00001,
        "output_cost_per_token_above_200k_tokens": 0.000015,
        "source": "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro"
    }
}
```

all keys will be sorted for deterministic output.

only items with key `litellm_provider` and value of "gemini", "anthropic", or "openai" will be included. all other items will be skipped.

when creating the key for the output, the model name will be prefixed with the value of the `litellm_provider` key and separated by `/`. where provider is "gemini", it will be prefixed with "google" meaning the "gemini/" prefix will be removed and "google/" will be added.

for "claude-3-7-sonnet-20250219" and "claude-3-7-sonnet-latest", the max_tokens will be written as 64000, regardless of what it says in the input JSON.

in general, the JSON parsing is extremely permissive to well formedness issues.

the resulting data should be written to ./res/pricing.json
