#!/usr/bin/env python3
# NOTICE: this file was automatically generated by https://github.com/khimaros/enc using the following invocation:
# ./enc-release scripts/pricing.en -o scripts/pricing.py --context-files ./res/pricing.json:requirements.txt

import json
import json5
import os
import urllib.request

# the source url is: https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
SOURCE_URL = "https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json"

# the resulting data should be written to ./res/pricing.json
OUTPUT_PATH = "./res/pricing.json"

# those objects have many keys, but will be filtered to have only the following ones:
KEYS_TO_KEEP = {
    "max_tokens",
    "max_input_tokens",
    "input_cost_per_token",
    "input_cost_per_token_above_200k_tokens",
    "output_cost_per_token",
    "output_cost_per_token_above_200k_tokens",
    "source",
}

# only items with key `litellm_provider` and value of "gemini", "anthropic", or "openai" will be included.
ALLOWED_PROVIDERS = {"gemini", "anthropic", "openai"}

# where provider is "gemini", it will be prefixed with "google"
PROVIDER_MAP = {"gemini": "google"}

# for "claude-3.7-sonnet-20250219" and "claude-3.7-sonnet-latest", the max_tokens will be written as 64000
SPECIAL_MAX_TOKENS = {
    "claude-3-7-sonnet-20250219": 64000,
    "claude-3-7-sonnet-latest": 64000,
}

# this script downloads and converts pricing data from litellm
def download_data(url):
    """downloads data from a url and returns it as a string."""
    print(f"downloading from {url}...")
    with urllib.request.urlopen(url) as response:
        if response.status != 200:
            raise RuntimeError(f"failed to download data: http {response.status}")
        return response.read().decode('utf-8')

def should_include_model(model_key, model_data):
    """returns true if a model should be included in the output."""
    # the key named "sample_spec" should be skipped.
    if model_key == "sample_spec":
        return False

    # only items with key `litellm_provider` and value of "gemini", "anthropic", or "openai" will be included.
    if model_data.get("litellm_provider") not in ALLOWED_PROVIDERS:
        return False

    # items which are missing `input_cost_per_token` or `output_cost_per_token` will be skipped.
    if "input_cost_per_token" not in model_data or "output_cost_per_token" not in model_data:
        return False

    return True

def process_data(source_data):
    """filters and transforms the raw pricing data."""
    processed = {}
    for model_key, model_data in source_data.items():
        if not should_include_model(model_key, model_data):
            continue

        provider = model_data["litellm_provider"]
        model_name = model_key.split("/")[-1]

        # when creating the key for the output, the model name will be prefixed with the value of the `litellm_provider` key
        output_provider = PROVIDER_MAP.get(provider, provider)
        output_key = f"{output_provider}/{model_name}"

        # those objects have many keys, but will be filtered
        output_model_data = {
            key: model_data[key] for key in KEYS_TO_KEEP if key in model_data
        }

        # for "claude-3.7-sonnet-20250219" and "claude-3.7-sonnet-latest", the max_tokens will be written as 64000
        if model_name in SPECIAL_MAX_TOKENS:
            output_model_data["max_tokens"] = SPECIAL_MAX_TOKENS[model_name]

        # all keys will be sorted for deterministic output.
        processed[output_key] = dict(sorted(output_model_data.items()))

    # all keys will be sorted for deterministic output.
    return dict(sorted(processed.items()))

def write_json(path, data):
    """writes data to a json file, creating directories if necessary."""
    print(f"writing to {path}...")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
        f.write('\n')

def main():
    """main entry point for the script."""
    content = download_data(SOURCE_URL)
    # with python, json5 should be used to parse the source data
    source_data = json5.loads(content)
    processed_data = process_data(source_data)
    write_json(OUTPUT_PATH, processed_data)
    print("done.")

if __name__ == "__main__":
    main()
